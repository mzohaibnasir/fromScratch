{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Causal masks in vanilla transformer for text .. makes model autogressive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# vision model:\n",
    "models that extract data from images.\n",
    "\n",
    "input1 : image\n",
    "input2 : prompt -\"Where is the photographer resting?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Causal masks in vanilla transformer for text .. makes model autogressive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3227613456.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[2], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    image -> contrastive vision encoder -> linear projection ->embeddingImage\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "image -> contrastive vision encoder -> linear projection ->embeddingImage\n",
    "\n",
    "prompt -> tokenizer( sentencePiece) ->embedddingPrompt\n",
    "\n",
    "=> concatenated embedding (embeddingImage + embedddingPrompt) -> transformer decoder -> response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each input image will be split into  blocks  of pixels creating a grid; then each block will be converted into an embedding. This embedding is a vector of fixed size.. that will be concenated with tokens' embeddings of  input prompt text i.e. each token will have an embedding just as each block of pixels will have an embedding. then this concatenated vetor will be sent to transformer decoder.\n",
    "\n",
    "![alt text](vision_lang_model_01.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Contrastive learning\n",
    "\n",
    "![alt text](vision_lang_model_02.png)\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "**Image encoder is vision transformer**\n",
    "\n",
    "each I1 is in the embedding of image 1\n",
    "\n",
    "\n",
    "**so for n image In with n descriptions Tn, we will get n*n matrix**\n",
    "\n",
    "\n",
    "\n",
    "**we want dot prduct of image It with corresponding description text Tt to give higher value**\n",
    "**and dot product of non corrspnding image and description should be low**\n",
    "***so, we want diagonal to have high values and all non-diagonal low.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### what is contrastive learninig?\n",
    "so with contrastive learning we take list of images and a list of corresponding texts; we encoder them and then we want to train text encoer and image encoder to produce embeddings in such a way that dot product of image with its corresponding text is done, it should produce high value and the dot product of image with non-corresponding text should retunr lower value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "\n",
    "**What is contrastive learning?**  \n",
    "Contrastive learning is a machine learning technique where we work with pairs of related data, such as images and their corresponding text descriptions. The goal is to train two encoders—a text encoder and an image encoder—so that they generate embeddings (numerical representations) with specific properties:  \n",
    "- The dot product of an image embedding and its corresponding text embedding should be high, indicating strong similarity.  \n",
    "- Conversely, the dot product of an image embedding with a non-corresponding text embedding should be low, indicating dissimilarity.  \n",
    "\n",
    "This approach helps the model learn to associate related data while distinguishing unrelated pairs effectively.  \n",
    "\n",
    "**all non corresponding images are -ve samples**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here’s a more technical and detailed explanation:  \n",
    "\n",
    "**What is Contrastive Learning?**  \n",
    "Contrastive learning is a representation learning paradigm that trains models by distinguishing between similar and dissimilar data pairs. It is widely used in tasks where the goal is to map data from different modalities (e.g., images and text) into a shared embedding space.  \n",
    "\n",
    "In the context of image-text contrastive learning, the process involves:  \n",
    "\n",
    "1. **Input Data**:  \n",
    "   - A dataset consisting of pairs of images \\( I \\) and their corresponding textual descriptions \\( T \\).  \n",
    "   - The dataset also implicitly includes negative pairs, where an image \\( I \\) is matched with a non-corresponding text \\( T' \\).  \n",
    "\n",
    "2. **Encoders**:  \n",
    "   - An **image encoder** \\( f_I(I) \\): Maps images into a high-dimensional embedding space. This is often a convolutional neural network (e.g., ResNet, Vision Transformer).  \n",
    "   - A **text encoder** \\( f_T(T) \\): Maps text descriptions into the same embedding space. This is often a Transformer-based model (e.g., BERT, RoBERTa).  \n",
    "\n",
    "3. **Objective**:  \n",
    "   - The goal is to learn embeddings \\( \\mathbf{z}_I = f_I(I) \\) for images and \\( \\mathbf{z}_T = f_T(T) \\) for text such that:  \n",
    "     - The **similarity score** (e.g., dot product or cosine similarity) between embeddings of corresponding pairs \\( (\\mathbf{z}_I, \\mathbf{z}_T) \\) is maximized.  \n",
    "     - The similarity score between embeddings of non-corresponding pairs \\( (\\mathbf{z}_I, \\mathbf{z}_{T'}) \\) is minimized.  \n",
    "\n",
    "4. **Loss Function**:  \n",
    "   - A popular loss for contrastive learning is the **InfoNCE loss** (based on Noise Contrastive Estimation):  \n",
    "     \\[\n",
    "     \\mathcal{L} = - \\frac{1}{N} \\sum_{i=1}^{N} \\left[ \\log \\frac{\\exp(\\text{sim}(\\mathbf{z}_{I_i}, \\mathbf{z}_{T_i}) / \\tau)}{\\sum_{j=1}^{N} \\exp(\\text{sim}(\\mathbf{z}_{I_i}, \\mathbf{z}_{T_j}) / \\tau)} \\right]\n",
    "     \\]  \n",
    "     Here:  \n",
    "     - \\( \\text{sim}(\\mathbf{z}_{I}, \\mathbf{z}_{T}) \\) is the similarity measure (e.g., dot product or cosine similarity).  \n",
    "     - \\( \\tau \\) is a temperature hyperparameter that controls the sharpness of the distribution.  \n",
    "     - \\( N \\) is the batch size.  \n",
    "     - The numerator represents the similarity of the positive pair, while the denominator sums over similarities for all pairs in the batch (positive and negative).  \n",
    "\n",
    "5. **Training Dynamics**:  \n",
    "   - The encoders are trained jointly to minimize the contrastive loss. This ensures that embeddings of positive pairs are pulled closer together in the shared embedding space, while embeddings of negative pairs are pushed farther apart.  \n",
    "\n",
    "6. **Applications**:  \n",
    "   - Contrastive learning is foundational in models like **CLIP** (Contrastive Language-Image Pretraining), where it is used to align visual and textual modalities.  \n",
    "   - It is also used in self-supervised learning frameworks (e.g., SimCLR, MoCo) to learn representations without explicit labels by treating augmentations of the same image as positive pairs and different images as negative pairs.  \n",
    "\n",
    "This approach is highly effective for multimodal tasks, enabling downstream applications like image-text retrieval, zero-shot classification, and multimodal embedding alignment.  \n",
    "\n",
    "Let me know if you’d like even more depth on any specific part!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WE do this using cross entropy loss in contrastive laerning \n",
    "\n",
    "\n",
    "Using CEloss we can force a number(true label ) to have larger value. we will be cosidering vertical for text and horizontql for images\n",
    "\n",
    "isn't CEloss just like a look up table where only true label value is considered? Remember\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Entropy Loss in Contrastive Learning  \n",
    "\n",
    "In contrastive learning, **Cross-Entropy Loss (CE Loss)** is often employed to enforce alignment between corresponding pairs (e.g., images and texts) and separation between non-corresponding pairs. Here's a detailed explanation:\n",
    "\n",
    "---\n",
    "\n",
    "### **How Cross-Entropy Loss Works in Contrastive Learning**  \n",
    "\n",
    "1. **Embedding Alignment**:  \n",
    "   - We have embeddings for **images** and **texts**. For simplicity:  \n",
    "     - Let \\( \\mathbf{z}_I \\) represent the embedding of an image.  \n",
    "     - Let \\( \\mathbf{z}_T \\) represent the embedding of a text.  \n",
    "   - The similarity between an image-text pair is computed, often using the **dot product** or **cosine similarity**.\n",
    "\n",
    "2. **Similarity Matrix**:  \n",
    "   - For a batch of \\( N \\) image-text pairs, we calculate the similarity scores for all pairs, forming a similarity matrix \\( S \\in \\mathbb{R}^{N \\times N} \\):  \n",
    "     \\[\n",
    "     S[i, j] = \\text{sim}(\\mathbf{z}_{I_i}, \\mathbf{z}_{T_j})\n",
    "     \\]  \n",
    "     - The \\( i^{th} \\) row corresponds to similarities between the \\( i^{th} \\) image and all texts in the batch.  \n",
    "     - The \\( j^{th} \\) column corresponds to similarities between the \\( j^{th} \\) text and all images in the batch.\n",
    "\n",
    "3. **Cross-Entropy Loss Objective**:  \n",
    "   - Cross-Entropy Loss forces the model to focus on the **true labels** by maximizing the similarity score of the corresponding pair (positive pair) while minimizing the similarity scores for non-corresponding pairs (negative pairs).  \n",
    "   - The CE Loss for the image-to-text direction can be written as:  \n",
    "     \\[\n",
    "     \\mathcal{L}_{I \\to T} = - \\frac{1}{N} \\sum_{i=1}^N \\log \\frac{\\exp(S[i, i] / \\tau)}{\\sum_{j=1}^N \\exp(S[i, j] / \\tau)}\n",
    "     \\]  \n",
    "     Similarly, for the text-to-image direction:  \n",
    "     \\[\n",
    "     \\mathcal{L}_{T \\to I} = - \\frac{1}{N} \\sum_{j=1}^N \\log \\frac{\\exp(S[j, j] / \\tau)}{\\sum_{i=1}^N \\exp(S[i, j] / \\tau)}\n",
    "     \\]  \n",
    "     - \\( S[i, i] \\): Similarity of the positive pair (image \\( i \\) with text \\( i \\)).  \n",
    "     - \\( \\tau \\): Temperature parameter controlling sharpness.  \n",
    "     - The numerator focuses on the true label (positive pair), while the denominator aggregates over all possible labels (negative pairs included).\n",
    "\n",
    "4. **Loss Aggregation**:  \n",
    "   - The total loss is a combination of both directions:  \n",
    "     \\[\n",
    "     \\mathcal{L} = \\mathcal{L}_{I \\to T} + \\mathcal{L}_{T \\to I}\n",
    "     \\]  \n",
    "\n",
    "---\n",
    "\n",
    "### **Is Cross-Entropy Loss Like a Lookup Table?**  \n",
    "\n",
    "In a way, **Cross-Entropy Loss acts like a lookup table** because it explicitly focuses on the **true label's score** (the positive pair) while treating other scores as negatives. However, it's more sophisticated than a simple lookup table:  \n",
    "\n",
    "1. **Focus on the True Label**:  \n",
    "   - CE Loss forces the score of the true label (positive pair) to dominate the similarity matrix, making it larger than all other scores.  \n",
    "\n",
    "2. **Gradient-Based Optimization**:  \n",
    "   - Unlike a static lookup table, CE Loss generates gradients for both the positive pair (to increase its similarity) and all negative pairs (to decrease their similarity).  \n",
    "   - This dynamic adjustment ensures that the embeddings are optimized in a shared space.  \n",
    "\n",
    "3. **Batch-Wide Comparison**:  \n",
    "   - CE Loss considers the entire batch for computing the loss, encouraging embeddings to learn relationships relative to all samples in the batch.  \n",
    "\n",
    "---\n",
    "\n",
    "### **\"Vertical for Text and Horizontal for Images\"**  \n",
    "\n",
    "This refers to how we interpret the similarity matrix \\( S \\):  \n",
    "- **Rows (horizontal)**: Each row corresponds to one image and its similarity with all texts.  \n",
    "- **Columns (vertical)**: Each column corresponds to one text and its similarity with all images.  \n",
    "\n",
    "For contrastive learning:  \n",
    "- Each **row** is used to compute the image-to-text loss (\\( \\mathcal{L}_{I \\to T} \\)).  \n",
    "- Each **column** is used to compute the text-to-image loss (\\( \\mathcal{L}_{T \\to I} \\)).  \n",
    "\n",
    "---\n",
    "\n",
    "### Summary  \n",
    "\n",
    "Cross-Entropy Loss in contrastive learning isn't just a lookup table—it’s a dynamic mechanism that enforces alignment for positive pairs and separation for negative pairs across the entire batch. It uses gradients to optimize the embeddings iteratively, ensuring the model learns meaningful relationships between modalities (e.g., images and text)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](vision_lang_model_03_code.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**labels = np.arrange(n) beccause of diagonal sequence i.e. 0,1,2,3...**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ISSUES WITH CLIP \n",
    "we are using cross entropy loss and CE loss basically does comparsion between two distributions so we are converting each colunm/row into a distribution in n*n matrix. Conversion to distribution is done by softmax functon. Softwamx is numerically unstable.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "![alt text](vision_lang_model_04_softmax.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***EXPONENTIAL fUNCTION GROWS TOO FAST AND MAY NOT FIT IN 32-BIT FLOATING POIN PRECISION***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Issues with CLIP /softamax\n",
    "\n",
    "CLIP (Contrastive Language–Image Pretraining) is a powerful multimodal model, but like any machine learning approach, it has limitations. A significant factor contributing to these issues stems from the use of **Cross-Entropy Loss (CE Loss)** in its contrastive learning framework. Here’s a breakdown of the challenges:\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Cross-Entropy Loss and Distribution Comparison**  \n",
    "\n",
    "- **Nature of CE Loss**:  \n",
    "  Cross-Entropy Loss compares two probability distributions:  \n",
    "  - One is the true label distribution (usually one-hot encoded).  \n",
    "  - The other is the predicted distribution, obtained by normalizing similarity scores (e.g., using softmax).  \n",
    "  This means CE Loss emphasizes maximizing the similarity of the true pair relative to the batch but may overlook absolute similarity.  \n",
    "\n",
    "- **Impact on CLIP**:  \n",
    "  - **Relative Comparisons**: CE Loss only ensures that positive pairs are more similar than negative pairs *within the batch*. It doesn’t guarantee high absolute similarity for the positive pairs.  \n",
    "  - **Batch Dependence**: The performance of CLIP depends on the quality and diversity of negative samples in the batch. Poorly chosen negatives can lead to suboptimal training.  \n",
    "\n",
    "---\n",
    "\n",
    "### **2. Sensitivity to Batch Size**  \n",
    "\n",
    "- Contrastive learning frameworks like CLIP are highly sensitive to batch size because the denominator in CE Loss involves all negative samples in the batch.  \n",
    "- **Small Batch Size**:  \n",
    "  - Reduces the diversity of negative samples.  \n",
    "  - Leads to overfitting, where the model struggles to generalize beyond the batch.  \n",
    "- **Large Batch Size**:  \n",
    "  - Requires significant memory and computational resources.  \n",
    "  - Makes training more expensive, especially for high-dimensional embeddings like those in CLIP.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Modality Gaps**  \n",
    "\n",
    "- **Embedding Misalignment**:  \n",
    "  CLIP aligns embeddings from two modalities (image and text) in a shared space. However, the distributions of embeddings for images and texts may not align perfectly due to differences in their inherent structures.  \n",
    "  - Images have spatial and visual patterns.  \n",
    "  - Text has sequential and semantic patterns.  \n",
    "  This mismatch can lead to suboptimal performance in downstream tasks.  \n",
    "\n",
    "- **Bias in Pretraining**:  \n",
    "  The pretraining dataset and loss may inadvertently favor one modality (e.g., text) over the other, leading to less robust representations for the disadvantaged modality.  \n",
    "\n",
    "---\n",
    "\n",
    "### **4. Lack of Fine-Grained Supervision**  \n",
    "\n",
    "- CLIP relies on global alignment between image and text embeddings. However, it does not explicitly enforce fine-grained relationships (e.g., parts of an image corresponding to specific words in the text).  \n",
    "- This limitation can cause issues in tasks requiring precise alignment, such as object localization or detailed image-caption matching.\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Dependence on Temperature Parameter (\\( \\tau \\))**  \n",
    "\n",
    "- The temperature parameter \\( \\tau \\) in the softmax function controls the sharpness of the predicted probability distribution.  \n",
    "  - A **small \\( \\tau \\)** makes the model focus heavily on the most similar pair, potentially ignoring other relevant information.  \n",
    "  - A **large \\( \\tau \\)** spreads the focus, which can reduce contrast between positive and negative pairs.  \n",
    "- Choosing the optimal \\( \\tau \\) is non-trivial and can significantly affect performance.\n",
    "\n",
    "---\n",
    "\n",
    "### **6. Bias in Pretraining Data**  \n",
    "\n",
    "- **Imbalanced Dataset**:  \n",
    "  CLIP is trained on large-scale datasets scraped from the internet. These datasets may have biases (e.g., cultural, geographic, or demographic) that are inadvertently learned by the model.  \n",
    "- **Noise in Data**:  \n",
    "  The image-text pairs in web-scraped datasets can be noisy or irrelevant, leading to suboptimal learning.\n",
    "\n",
    "---\n",
    "\n",
    "### **7. Over-Reliance on Negative Samples**  \n",
    "\n",
    "- CE Loss in contrastive learning depends heavily on negative samples to push embeddings apart. However:  \n",
    "  - **Hard Negatives** (difficult non-corresponding pairs) are rare but crucial for training. Without them, the model might converge to a suboptimal solution.  \n",
    "  - Overemphasis on hard negatives can lead to instability during training.\n",
    "\n",
    "---\n",
    "\n",
    "### **Summary of Key Issues**  \n",
    "\n",
    "| **Issue**                  | **Impact**                                                                                     |  \n",
    "|----------------------------|-----------------------------------------------------------------------------------------------|  \n",
    "| CE Loss (relative comparisons) | Focuses on relative similarities, ignoring absolute similarity for positive pairs.          |  \n",
    "| Sensitivity to Batch Size   | Small batches reduce diversity; large batches are computationally expensive.                 |  \n",
    "| Modality Gaps               | Misalignment between image and text embeddings.                                              |  \n",
    "| Lack of Fine-Grained Supervision | Cannot capture detailed relationships between image regions and text tokens.                |  \n",
    "| Dependence on Temperature   | Suboptimal \\( \\tau \\) can lead to poor alignment of embeddings.                               |  \n",
    "| Dataset Bias and Noise      | Biases in web-scraped data can limit the generalizability of the model.                       |  \n",
    "| Negative Sample Dependence  | Limited hard negatives can reduce training effectiveness; overemphasis causes instability.   |  \n",
    "\n",
    "Addressing these issues often involves designing better loss functions (e.g., debiased contrastive loss), improving data quality, and incorporating additional supervision (e.g., fine-grained attention mechanisms). Let me know if you'd like elaboration on potential solutions!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ISSUES WITH CLIP  \n",
    "\n",
    "CLIP (Contrastive Language–Image Pretraining) faces certain challenges, one of which stems from its use of **Cross-Entropy Loss (CE Loss)** in its contrastive learning framework. Let’s delve into these issues:  \n",
    "\n",
    "---\n",
    "\n",
    "### **1. Cross-Entropy Loss and Distribution Comparison**  \n",
    "\n",
    "- **Nature of CE Loss**:  \n",
    "  Cross-Entropy Loss is designed to compare two probability distributions. In CLIP, it works on the **n × n similarity matrix**, where:  \n",
    "  - Each row corresponds to a specific image or text.  \n",
    "  - Each column corresponds to a distribution over all potential matches in the batch.  \n",
    "\n",
    "- **How CE Loss Works in CLIP**:  \n",
    "  - Each similarity score in the matrix is converted into a probability distribution using the **softmax function**.  \n",
    "  - The model is trained to maximize the probability of correct (positive) pairs while minimizing the probability of incorrect (negative) pairs.  \n",
    "\n",
    "---\n",
    "\n",
    "### **2. Softmax Conversion Issues**  \n",
    "\n",
    "The **softmax function** is a key component in converting raw similarity scores into probabilities, but it introduces several challenges:  \n",
    "\n",
    "- **Numerical Instability**:  \n",
    "  - Softmax involves exponentiating similarity scores, which can cause overflow or underflow when the values are very large or very small.  \n",
    "  - This instability can lead to unreliable gradients, especially when the similarity scores in the matrix vary significantly.  \n",
    "\n",
    "- **Exaggeration of Differences**:  \n",
    "  - Softmax amplifies differences between similarity scores.  \n",
    "  - This can cause the model to over-focus on the highest similarity score, potentially ignoring meaningful relationships between other pairs.  \n",
    "\n",
    "---\n",
    "\n",
    "### **3. Conversion to Row/Column Distributions**  \n",
    "\n",
    "- In the **n × n similarity matrix**, rows represent images and columns represent texts (or vice versa).  \n",
    "- Softmax is applied to each row (for images) or column (for texts) to convert raw scores into distributions.  \n",
    "- **Limitations**:  \n",
    "  - The process forces each row/column to sum to 1, but this does not inherently ensure meaningful alignment across modalities.  \n",
    "  - It creates a dependency on the relative differences within the batch, which can degrade performance if the batch contains poor-quality negatives.\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Dependence on Batch Quality**  \n",
    "\n",
    "- **Small Batches**:  \n",
    "  - Reduce the diversity of negative samples.  \n",
    "  - Make the softmax normalization less effective because of limited contrast in similarity scores.  \n",
    "\n",
    "- **Noisy Negatives**:  \n",
    "  - In real-world datasets, some negative samples may not be truly irrelevant (e.g., an image and text might share subtle semantic similarities).  \n",
    "  - These noisy negatives can confuse the model, reducing the effectiveness of CE Loss.  \n",
    "\n",
    "---\n",
    "\n",
    "### **5. Temperature Scaling in Softmax**  \n",
    "\n",
    "- The softmax function in CLIP uses a **temperature parameter (\\( \\tau \\))** to control the sharpness of the probability distribution:  \n",
    "  - **Small \\( \\tau \\)**: Focuses heavily on the highest similarity score, ignoring other scores.  \n",
    "  - **Large \\( \\tau \\)**: Produces a more uniform distribution, reducing contrast between positive and negative pairs.  \n",
    "- Finding the optimal \\( \\tau \\) is critical but challenging. Suboptimal temperature scaling can degrade model performance.\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Observations**  \n",
    "\n",
    "- The use of **softmax** in CE Loss enables contrastive learning but comes with trade-offs:  \n",
    "  - It introduces **numerical instability**, especially with high-dimensional embeddings and diverse datasets.  \n",
    "  - The focus on relative differences (via softmax normalization) may not capture absolute alignment effectively.  \n",
    "- Addressing these issues may involve alternative loss functions (e.g., debiased contrastive loss) or improved numerical techniques (e.g., log-sum-exp trick to stabilize softmax).  \n",
    "\n",
    "Let me know if you'd like further technical elaboration or examples!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ISSUES WITH CLIP\n",
    "\n",
    "CLIP (Contrastive Language–Image Pretraining) uses **Cross-Entropy Loss (CE Loss)** for contrastive learning, which involves comparing two distributions. The core issue here lies in the conversion of similarity scores into distributions using the **softmax function**, which can lead to **numerical instability** and precision issues, especially when dealing with large datasets and high-dimensional embeddings. Let's dive deeper into the specifics of this issue:\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Cross-Entropy Loss and Distribution Comparison**\n",
    "\n",
    "- **CE Loss Overview**:  \n",
    "  Cross-Entropy Loss compares two probability distributions:  \n",
    "  - One distribution is the true label (often one-hot encoded).  \n",
    "  - The other is the predicted distribution, which is generated by applying the **softmax function** to similarity scores between images and text in the **n × n matrix**.\n",
    "\n",
    "- **n × n Matrix**:  \n",
    "  - Each row corresponds to a specific image or text (depending on whether you're comparing image-to-text or text-to-image).  \n",
    "  - Each column represents a distribution over all potential matches in the batch (i.e., similarity scores with other images/texts).  \n",
    "\n",
    "---\n",
    "\n",
    "### **2. Numerical Instability with Softmax**\n",
    "\n",
    "- **Softmax Function**:  \n",
    "  The **softmax function** converts raw similarity scores (which can range from negative to positive) into probabilities by applying the exponential function to each similarity score, followed by normalization:\n",
    "  \n",
    "  \\[\n",
    "  P(x_i) = \\frac{e^{x_i}}{\\sum_{j} e^{x_j}}\n",
    "  \\]\n",
    "\n",
    "  Where \\(x_i\\) is the similarity score for a specific image-text pair, and the denominator is the sum of exponentiated scores across all items in the batch.\n",
    "\n",
    "- **Exponential Growth Issue**:  \n",
    "  The **exponential function** grows very rapidly. When the similarity scores \\(x_i\\) are large (either positive or negative), applying the exponential function causes them to become very large or very small, which can lead to **overflow** or **underflow** during computation. This is especially problematic when the model works with high-dimensional data, such as image and text embeddings.\n",
    "\n",
    "- **Precision Problems**:  \n",
    "  In practice, floating-point precision (e.g., 32-bit floating-point) cannot handle extremely large or small numbers without loss of precision. This issue becomes particularly noticeable when:  \n",
    "  - **Large values** (e.g., similarity scores of 100 or higher) are exponentiated, resulting in values too large to fit within the available precision.  \n",
    "  - **Small values** (e.g., negative similarity scores leading to exponentiation of very small numbers) may cause underflow, resulting in values that are effectively zero.  \n",
    "\n",
    "  This instability can cause incorrect gradients during backpropagation, leading to poor convergence or divergence in training.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Softmax Sensitivity and Precision Loss**\n",
    "\n",
    "- **Effect of Exponential Growth**:  \n",
    "  - The **exponential function** makes large similarity values (whether positive or negative) disproportionately dominant.  \n",
    "  - As a result, even if a positive image-text pair has a moderate similarity, it may be overshadowed by a large negative or positive value, distorting the distribution.\n",
    "\n",
    "- **Precision in 32-bit Floats**:  \n",
    "  - **32-bit floating point** numbers have a limited range (approximately \\(\\pm 3.4 \\times 10^{38}\\)).  \n",
    "  - Exponentiating large numbers can easily result in values that exceed this range, causing an overflow. Similarly, small negative values may underflow to zero, making them indistinguishable from each other.\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Potential Solutions and Mitigations**\n",
    "\n",
    "- **Log-Sum-Exp Trick**:  \n",
    "  One way to mitigate this issue is to use the **log-sum-exp trick**, which stabilizes the computation of the softmax function by factoring out the largest value in the similarity scores before exponentiation. This reduces the range of values being exponentiated, preventing overflow or underflow:\n",
    "\n",
    "  \\[\n",
    "  \\text{softmax}(x) = \\frac{e^{x - \\max(x)}}{\\sum_j e^{x_j - \\max(x)}}\n",
    "  \\]\n",
    "\n",
    "  This trick ensures that the exponential values are within a numerically stable range.\n",
    "\n",
    "- **Clipping Large Values**:  \n",
    "  Another approach is to clip extremely large similarity scores before applying softmax. By setting a maximum threshold for the similarity scores, we can avoid values that are too large to fit in the precision range.\n",
    "\n",
    "- **Switch to 64-bit Precision**:  \n",
    "  While this comes at the cost of increased memory usage, using **64-bit floating-point precision** (double precision) can alleviate the overflow/underflow problem by providing a much larger numerical range. However, this solution may not be feasible for large-scale models due to the computational overhead.\n",
    "\n",
    "- **Use Alternative Loss Functions**:  \n",
    "  Exploring alternative loss functions, such as **BCE (Binary Cross-Entropy)** or **triplet loss**, might offer more stable training without relying heavily on softmax normalization.\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Summary of Key Issues**\n",
    "\n",
    "| **Issue**                     | **Impact**                                                                                         |  \n",
    "|-------------------------------|---------------------------------------------------------------------------------------------------|  \n",
    "| **Exponential Growth in Softmax** | The exponential function grows too fast, causing numerical instability.                          |  \n",
    "| **Precision Loss (32-bit Floats)** | Large or small values can lead to overflow or underflow, leading to poor gradient propagation.    |  \n",
    "| **Effect on Training**         | Numerical instability can cause incorrect gradients and slow or unstable convergence during training. |\n",
    "\n",
    "---\n",
    "\n",
    "Addressing these issues requires careful handling of the numerical properties of the model. Using techniques like the **log-sum-exp trick** or switching to higher precision floating points can help mitigate these problems. Let me know if you'd like further details or clarification!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You've highlighted some important numerical stability issues with the cross-entropy loss implementation in CLIP (Contrastive Language-Image Pre-training). Let me break down the key points and discuss potential solutions:\n",
    "\n",
    "1. Numerical Instability of Softmax\n",
    "   - The softmax function involves exponentiating values, which can quickly lead to extremely large numbers\n",
    "   - In 32-bit floating-point precision, this can cause overflow, leading to computational errors\n",
    "\n",
    "2. Exponential Function Limitations\n",
    "   - Exponential function grows extremely rapidly\n",
    "   - When input values are large, exp(x) can exceed the maximum representable float32 value\n",
    "   - This can cause:\n",
    "     - Infinity values\n",
    "     - Loss of precision\n",
    "     - Potential model training instabilities\n",
    "\n",
    "Potential Solutions:\n",
    "\n",
    "1. Log-Sum-Exp Trick\n",
    "```python\n",
    "   def stable_softmax(x):\n",
    "      # Subtract max value to prevent overflow\n",
    "      max_x = np.max(x, axis=-1, keepdims=True)\n",
    "      exp_x = np.exp(x - max_x)\n",
    "      return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n",
    "```\n",
    "\n",
    "2. Numerical Stabilization Techniques\n",
    "   - Clip input values to a reasonable range\n",
    "   - Use log-space computations\n",
    "   - Implement careful scaling of input values\n",
    "\n",
    "3. Consider using float64 precision if computational resources allow\n",
    "\n",
    "4. Alternative Loss Formulations\n",
    "   - Investigate modified contrastive loss approaches that avoid direct softmax computation\n",
    "\n",
    "Would you like me to elaborate on any of these approaches or discuss implementation strategies for improving numerical stability?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# That's an excellent question. The scaling techniques I mentioned actually don't fundamentally change the model's performance when implemented correctly. Here's why:\n",
    "\n",
    "1. Normalization, Not Alteration\n",
    "   - The log-sum-exp trick and similar stabilization methods don't change the mathematical properties of the softmax\n",
    "   - They simply prevent numerical overflow while maintaining the same relative probability distributions\n",
    "   - The core computational goal remains unchanged\n",
    "\n",
    "2. Preservation of Relative Relationships\n",
    "   - By subtracting the max value before exponentation, you're preserving the relative differences between values\n",
    "   - The softmax still produces the same ranking and proportional probabilities\n",
    "   - Only the raw magnitude is adjusted to fit within computational limits\n",
    "\n",
    "Example to illustrate:\n",
    "\n",
    "```python\n",
    "# Original problematic input\n",
    "x = np.array([1000, 2000, 3000])\n",
    "\n",
    "# Unstable softmax might produce inf or nan\n",
    "unstable_softmax = np.exp(x) / np.sum(np.exp(x))\n",
    "\n",
    "# Stabilized version\n",
    "max_x = np.max(x)\n",
    "stable_softmax = np.exp(x - max_x) / np.sum(np.exp(x - max_x))\n",
    "```\n",
    "\n",
    "In this example, the stabilized version produces a valid probability distribution without changing the fundamental comparative relationships between the original values.\n",
    "\n",
    "Key Benefits:\n",
    "- Prevents computational errors\n",
    "- Maintains model logic\n",
    "- Ensures consistent training behavior\n",
    "- Allows precise gradient computation\n",
    "\n",
    "The technique is essentially a mathematical \"normalization\" that keeps the core computational logic intact while preventing numerical instability.\n",
    "\n",
    "Would you like me to elaborate on how this preservation of relative relationships works mathematically?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **larger x resulting in infinity, so we are subtracting by max(x)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_117518/2405016546.py:6: RuntimeWarning: overflow encountered in exp\n",
      "  unstable_softmax = np.exp(x) / np.sum(np.exp(x))\n",
      "/tmp/ipykernel_117518/2405016546.py:6: RuntimeWarning: invalid value encountered in divide\n",
      "  unstable_softmax = np.exp(x) / np.sum(np.exp(x))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([ 0., nan, nan]), array([0., 0., 1.]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Original problematic input\n",
    "x = np.array([10, 2000, 3000])\n",
    "\n",
    "# Unstable softmax might produce inf or nan\n",
    "unstable_softmax = np.exp(x) / np.sum(np.exp(x))\n",
    "print(f\" ----\")\n",
    "# Stabilized version\n",
    "max_x = np.max(x)\n",
    "stable_softmax = np.exp(x - max_x) / np.sum(\n",
    "    np.exp(x - max_x)\n",
    ")  # just subtract max(x) from x before np.exp()\n",
    "unstable_softmax, stable_softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### range of logits\n",
    "\n",
    "In general, **logits** in a **Cross-Entropy Loss** function can take any real value, and their range is theoretically **\\((- \\infty, + \\infty)\\)**.  \n",
    "\n",
    "### Why Logits Can Be Unbounded:\n",
    "- **Logits** are the raw, unnormalized scores produced by a model before applying the **softmax function**.  \n",
    "- The **softmax** converts these logits into a probability distribution, but the logits themselves are not constrained.  \n",
    "\n",
    "---\n",
    "\n",
    "### Mathematical Context\n",
    "\n",
    "For a classification problem:\n",
    "1. **Logits**: \\( z_i \\) (output of the model for class \\( i \\)) can be any real number:  \n",
    "   \\[\n",
    "   z_i \\in (-\\infty, +\\infty)\n",
    "   \\]\n",
    "2. **Softmax**: Converts the logits into probabilities:  \n",
    "   \\[\n",
    "   p_i = \\frac{e^{z_i}}{\\sum_{j} e^{z_j}}\n",
    "   \\]\n",
    "   - If \\( z_i \\) is very large, \\( e^{z_i} \\) dominates the numerator.  \n",
    "   - If \\( z_i \\) is very small (negative), \\( e^{z_i} \\) approaches zero.\n",
    "\n",
    "3. **Cross-Entropy Loss**:  \n",
    "   Cross-Entropy Loss compares the predicted probabilities \\( p_i \\) with the true labels \\( y_i \\):  \n",
    "   \\[\n",
    "   L = - \\sum_{i=1}^C y_i \\log(p_i)\n",
    "   \\]\n",
    "   Here, the softmax ensures \\( p_i \\) is in the range \\([0, 1]\\), but the raw logits \\( z_i \\) are unconstrained.\n",
    "\n",
    "---\n",
    "\n",
    "### Range of Logits in Practice:\n",
    "1. **Neural Networks**: Logits depend on the output of the last layer of the network. For fully connected layers:\n",
    "   - No activation function is applied after the last layer.\n",
    "   - Therefore, logits can be very large (positive or negative), especially if weights or inputs have large magnitudes.\n",
    "\n",
    "2. **Stability of Softmax**:\n",
    "   - Large positive logits (\\( z_i \\to +\\infty \\)) lead to probabilities close to \\( 1 \\).  \n",
    "   - Large negative logits (\\( z_i \\to -\\infty \\)) lead to probabilities close to \\( 0 \\).  \n",
    "   - This causes numerical instability due to the exponential growth of \\( e^{z_i} \\).\n",
    "\n",
    "---\n",
    "\n",
    "### Key Points:\n",
    "- **Range of Logits**: \\( (-\\infty, +\\infty) \\).  \n",
    "- **Range After Softmax**: \\( (0, 1) \\) (probabilities).  \n",
    "- Large logits can cause **numerical instability** when exponentiated in the softmax function, particularly in low-precision floating-point formats (e.g., 32-bit).\n",
    "\n",
    "---\n",
    "\n",
    "### Mitigation Techniques:\n",
    "1. **Logits Normalization**: Normalize logits before applying softmax.  \n",
    "2. **Log-Sum-Exp Trick**: Stabilizes softmax computation by subtracting the maximum logit.  \n",
    "   \\[\n",
    "   p_i = \\frac{e^{z_i - \\max(z)}}{\\sum_{j} e^{z_j - \\max(z)}}\n",
    "   \\]\n",
    "3. **Regularization**: Apply techniques like weight decay to prevent very large weights, which could produce large logits.\n",
    "\n",
    "In summary, logits are unbounded by design and can take any real value \\((- \\infty, + \\infty)\\)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The uploaded image provides a clear explanation of the **numerical instability of the softmax function** and a solution to mitigate it. Here’s a detailed breakdown of the content:\n",
    "\n",
    "---\n",
    "\n",
    "### **Softmax Function**  \n",
    "The softmax function converts a vector of logits \\( a_i \\) into probabilities \\( S_i \\) such that:\n",
    "\\[\n",
    "S_i = \\frac{e^{a_i}}{\\sum_{k=1}^N e^{a_k}}\n",
    "\\]\n",
    "- \\( a_i \\) are the logits (raw scores) from the model.\n",
    "- \\( S_i \\) is the probability for the \\( i \\)-th class.\n",
    "- The softmax ensures \\( S_i \\in [0, 1] \\) and \\( \\sum_{i} S_i = 1 \\).\n",
    "\n",
    "---\n",
    "\n",
    "### **Problem: Numerical Instability**  \n",
    "The softmax function involves the **exponential** \\( e^{a_i} \\), which grows very quickly for large \\( a_i \\):\n",
    "- If \\( a_i \\) is very large, \\( e^{a_i} \\) can **overflow** and exceed the limits of 32-bit floating-point numbers.  \n",
    "- If \\( a_i \\) is very small (negative), \\( e^{a_i} \\) becomes very close to zero, which can cause **underflow**.\n",
    "\n",
    "This instability can cause the softmax computation to fail or produce inaccurate results.\n",
    "\n",
    "---\n",
    "\n",
    "### **Solution: Log-Sum-Exp Trick**  \n",
    "To stabilize the softmax computation, we subtract the **maximum logit** \\( \\max_i (a_i) \\) from all logits before applying the exponential:\n",
    "\\[\n",
    "S_i = \\frac{e^{a_i - \\max_i (a_i)}}{\\sum_{k=1}^N e^{a_k - \\max_i (a_i)}}\n",
    "\\]\n",
    "- By subtracting \\( \\max_i (a_i) \\), the largest logit becomes \\( 0 \\), and all other logits are shifted to negative values.  \n",
    "- This avoids numerical overflow because \\( e^0 = 1 \\) and the remaining terms \\( e^{a_i - \\max_i (a_i)} \\) are in a manageable range.\n",
    "\n",
    "---\n",
    "\n",
    "### **Explanation in the Image**  \n",
    "1. **Problem** (Red Text):  \n",
    "   The exponential function grows too fast and may not fit in 32-bit floating-point precision.  \n",
    "\n",
    "2. **Solution** (Green Text):  \n",
    "   By subtracting the maximum logit, the arguments to the exponential function are pushed towards **negative values**, making the exponential outputs smaller and stable.\n",
    "\n",
    "3. **Mathematical Derivation**:\n",
    "   - The image derives the stabilized softmax step-by-step using a constant \\( c \\) where \\( \\log(c) = -\\max_i (a_i) \\).\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Takeaways**:\n",
    "- Softmax is numerically unstable because of the exponential growth of \\( e^{a_i} \\).  \n",
    "- Stabilization is achieved using the **log-sum-exp trick** by subtracting the maximum logit.  \n",
    "- This ensures that the computation is stable and avoids overflow/underflow issues.\n",
    "\n",
    "Let me know if you'd like further clarification or examples! 🚀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SIGNLIP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In siglip paper due to asymmetry of softmax loss, the normalization is independently performed two times; across images and across texts and matrix n*n is not symmetric because (1,2) is not same as (2,1)..***SO CLIP IS VERY COMPUTATIONALLY EXPENSIVE***    \n",
    "\n",
    "\n",
    "So in SIGLIP, they have proposed to replace CE loss with Sigmoid, after computing n*n matrix(dot products), rather that treating loss as distribution over row or column, we treat it as a binary classification task using sigmoid. In  which each of these dot product is traeated independenly i.e. only diagoal values should be 1 we can do this using sigoid. it will take dot product as input, and sigmoid will return 1 for corresponding text and image, hence all of these dot products become indepedent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](vision_lang_model_05_siglip_softmax.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **there are n labels(y_ij) and remember softmax is just normalizing probailities ... other than that we are only cindering value against label for both normalizations...so we are not actually conisdering rows/colums for loss calculations but just normalization.**\n",
    "\n",
    "and then we apply log for each softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](vision_lang_model_02_CEloss.webp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the **SigLIP** paper (Scaling the Learning of Image-Text Pretraining), the authors address the **asymmetry** of the standard softmax loss in contrastive learning setups, particularly in methods like CLIP. Here’s an elaboration:\n",
    "\n",
    "---\n",
    "\n",
    "### **Issue with Softmax Loss in Contrastive Learning**  \n",
    "In a typical **contrastive learning setup** (e.g., CLIP), the loss uses a single softmax normalization across either:\n",
    "1. **Rows** (image-to-text matching) or  \n",
    "2. **Columns** (text-to-image matching)  \n",
    "\n",
    "This creates an **asymmetry** because the softmax loss is only applied in one direction at a time:\n",
    "- If the loss normalizes across rows, it aligns each **image embedding** to the corresponding **text embedding**.\n",
    "- If the loss normalizes across columns, it aligns each **text embedding** to the corresponding **image embedding**.\n",
    "\n",
    "However, this **single softmax normalization** does not treat images and texts symmetrically, leading to **imbalanced training dynamics**.\n",
    "\n",
    "---\n",
    "\n",
    "### **SigLIP's Solution: Dual Softmax Normalization**  \n",
    "To address this asymmetry, **SigLIP independently normalizes across both images and texts**. The loss is computed **twice**, once for images and once for texts:\n",
    "1. **Image-to-Text Loss**:  \n",
    "   Normalize the logits (dot products) **row-wise** to match each image embedding with its corresponding text embedding.  \n",
    "2. **Text-to-Image Loss**:  \n",
    "   Normalize the logits **column-wise** to match each text embedding with its corresponding image embedding.\n",
    "\n",
    "The total loss is then the **average of both losses**.\n",
    "\n",
    "---\n",
    "\n",
    "### **Mathematical Formulation**  \n",
    "Let \\( z_i \\) be the embedding of image \\( i \\), \\( t_j \\) be the embedding of text \\( j \\), and \\( \\tau \\) be the temperature parameter.\n",
    "\n",
    "The **dual softmax contrastive loss** is:\n",
    "\n",
    "\\[\n",
    "\\mathcal{L} = \\frac{1}{2} \\left( \\mathcal{L}_{\\text{img-to-text}} + \\mathcal{L}_{\\text{text-to-img}} \\right)\n",
    "\\]\n",
    "\n",
    "Where:  \n",
    "- **Image-to-Text Loss**:\n",
    "\\[\n",
    "\\mathcal{L}_{\\text{img-to-text}} = -\\frac{1}{N} \\sum_{i=1}^N \\log \\frac{\\exp(z_i \\cdot t_i / \\tau)}{\\sum_{j=1}^N \\exp(z_i \\cdot t_j / \\tau)}\n",
    "\\]\n",
    "\n",
    "- **Text-to-Image Loss**:\n",
    "\\[\n",
    "\\mathcal{L}_{\\text{text-to-img}} = -\\frac{1}{N} \\sum_{j=1}^N \\log \\frac{\\exp(t_j \\cdot z_j / \\tau)}{\\sum_{i=1}^N \\exp(t_j \\cdot z_i / \\tau)}\n",
    "\\]\n",
    "\n",
    "Here:  \n",
    "- \\( z_i \\cdot t_j \\) is the dot product (similarity) between image \\( i \\) and text \\( j \\).  \n",
    "- The **row-wise softmax** applies to the image embeddings, and the **column-wise softmax** applies to the text embeddings.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Benefits of Dual Normalization**  \n",
    "1. **Symmetry**: Treats images and texts equally during training, ensuring balanced learning.  \n",
    "2. **Stability**: Independent normalization avoids potential instability caused by combining image and text distributions into a single softmax.  \n",
    "3. **Improved Performance**: Experiments in SigLIP show better alignment of embeddings and improved downstream task performance.\n",
    "\n",
    "---\n",
    "\n",
    "### **Summary**  \n",
    "- In **SigLIP**, the asymmetry of the softmax loss is resolved by performing **dual normalization**.  \n",
    "- Softmax is applied **independently** across rows (images) and columns (texts).  \n",
    "- This results in a symmetric, balanced loss that improves the training dynamics of image-text contrastive learning models.\n",
    "\n",
    "Let me know if you’d like me to expand on any part! 🚀"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SIGLIP REPLACES SOFTMAX with SIGMOID\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "So in SIGLIP, they have proposed to replace CE loss with Sigmoid, after computing n*n matrix(dot products), rather that treating loss as distribution over row or column, we treat it as a binary classification task using sigmoid. In  which each of these dot product is traeated independenly i.e. only diagoal values should be 1 we can do this using sigoid. it will take dot product as input, and sigmoid will return 1 for corresponding text and image, hence all of these dot products become indepedent\n",
    "\n",
    "\n",
    "\n",
    "INDEPENDENT because as their is no normalization, we can spit whole matrics into block and each block can be compiuted on different devices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](vision_lang_model_06_siglip_sigmoid.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are correct! In **SigLIP**, the authors propose to replace the **Cross-Entropy (CE) loss** with a **Sigmoid-based Binary Cross-Entropy (BCE) loss** for contrastive learning. This modification simplifies the loss calculation by treating the problem as a **binary classification task** rather than a multi-class distribution task. Let’s break this down:\n",
    "\n",
    "---\n",
    "\n",
    "### **Problem with Cross-Entropy (CE) Loss**\n",
    "1. In standard contrastive learning (e.g., CLIP), CE loss works with a **softmax normalization** over rows or columns of the **N × N similarity matrix** (where \\( N \\) is the batch size).  \n",
    "2. This normalization forces the logits (dot products) to behave like **probability distributions**:\n",
    "   - Row-wise softmax aligns **image-to-text** pairs.\n",
    "   - Column-wise softmax aligns **text-to-image** pairs.  \n",
    "3. However, softmax introduces issues like:  \n",
    "   - **Numerical instability** due to the exponential function (softmax).  \n",
    "   - **Asymmetry** in loss calculation (softmax over rows vs. columns).  \n",
    "   - Tight coupling between dot products in the matrix (non-diagonal values influence the normalization).\n",
    "\n",
    "---\n",
    "\n",
    "### **Sigmoid-based Binary Cross-Entropy (BCE) Loss**\n",
    "Instead of treating the dot products as part of a single probability distribution, SigLIP treats each dot product **independently** as a **binary classification task**.\n",
    "\n",
    "#### Key Idea:\n",
    "- Each entry \\( s_{ij} \\) in the **N × N similarity matrix** (dot product between image \\( i \\) and text \\( j \\)) is treated as an **independent prediction**.\n",
    "- The goal is to classify:\n",
    "  - **Diagonal entries** (\\( i = j \\)) as **positive pairs** (label = 1).  \n",
    "  - **Off-diagonal entries** (\\( i \\neq j \\)) as **negative pairs** (label = 0).  \n",
    "\n",
    "#### **Sigmoid Function**:\n",
    "The sigmoid function maps each dot product \\( s_{ij} \\) into the range \\( (0, 1) \\), where:\n",
    "\\[\n",
    "\\text{Sigmoid}(s_{ij}) = \\frac{1}{1 + e^{-s_{ij}}}\n",
    "\\]\n",
    "- \\( \\text{Sigmoid}(s_{ij}) \\approx 1 \\) when \\( s_{ij} \\) is large (high similarity for positive pairs).  \n",
    "- \\( \\text{Sigmoid}(s_{ij}) \\approx 0 \\) when \\( s_{ij} \\) is small (low similarity for negative pairs).  \n",
    "\n",
    "#### **Binary Cross-Entropy (BCE) Loss**:\n",
    "The BCE loss for the \\( N \\times N \\) similarity matrix can be written as:\n",
    "\\[\n",
    "\\mathcal{L} = - \\frac{1}{N^2} \\sum_{i=1}^N \\sum_{j=1}^N \\left[ y_{ij} \\log(\\sigma(s_{ij})) + (1 - y_{ij}) \\log(1 - \\sigma(s_{ij})) \\right]\n",
    "\\]\n",
    "Where:\n",
    "- \\( \\sigma(s_{ij}) \\) is the sigmoid of the dot product \\( s_{ij} \\).  \n",
    "- \\( y_{ij} = 1 \\) for diagonal entries (positive pairs).  \n",
    "- \\( y_{ij} = 0 \\) for off-diagonal entries (negative pairs).  \n",
    "\n",
    "---\n",
    "\n",
    "### **Why is This Better?**\n",
    "1. **Independence of Dot Products**:  \n",
    "   Each dot product \\( s_{ij} \\) is treated **independently** of others. There is no normalization over rows or columns, removing interdependencies caused by softmax.  \n",
    "\n",
    "2. **Numerical Stability**:  \n",
    "   Sigmoid is more numerically stable than softmax because it avoids the exponential growth caused by softmax normalization.  \n",
    "\n",
    "3. **Simpler Loss**:  \n",
    "   The loss directly focuses on ensuring that diagonal entries (correct image-text pairs) are **highly similar** and off-diagonal entries (incorrect pairs) are **dissimilar**.  \n",
    "\n",
    "4. **Symmetry**:  \n",
    "   The BCE loss inherently treats images and texts symmetrically, unlike the asymmetric softmax loss.\n",
    "\n",
    "---\n",
    "\n",
    "### **Summary**  \n",
    "In SigLIP:  \n",
    "1. The **similarity matrix** (dot products) is computed as \\( N \\times N \\).  \n",
    "2. The task is reframed as a **binary classification problem**:  \n",
    "   - Diagonal values (positive pairs) should be **1**.  \n",
    "   - Off-diagonal values (negative pairs) should be **0**.  \n",
    "3. **Sigmoid** is applied to each dot product independently, followed by the **Binary Cross-Entropy loss**.  \n",
    "4. This eliminates softmax’s numerical instability and asymmetry, making the training more stable and robust.\n",
    "\n",
    "This change leads to better numerical behavior, improved performance, and a cleaner formulation of the loss function.\n",
    "\n",
    "Let me know if you’d like further clarifications or examples! 🚀"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Can we use CE for two classes \n",
    "Yes, you can use **cross-entropy loss** instead of **binary cross-entropy (BCE)** for 2 classes, but it depends on how you format your labels and outputs.\n",
    "\n",
    "### Understanding the Difference:\n",
    "1. **Binary Cross-Entropy (BCE)**:\n",
    "   - Used for **binary classification** where labels are **0 or 1**.\n",
    "   - The model typically outputs a **single probability** (from the sigmoid function) for the positive class, and the negative class probability is \\( 1 - \\text{probability} \\).\n",
    "\n",
    "2. **Cross-Entropy Loss** (Categorical Cross-Entropy):\n",
    "   - Used for **multi-class classification**, where labels are one-hot encoded, and the model outputs probabilities for each class (via **softmax**).\n",
    "   - For 2 classes, the softmax output is still valid, but it produces two probabilities, one for each class.\n",
    "\n",
    "---\n",
    "\n",
    "### Can We Use Cross-Entropy Loss for 2 Classes?\n",
    "**Yes**, cross-entropy loss can be used for binary classification **if you treat the problem as a 2-class multi-class classification**. Here's how it works:\n",
    "\n",
    "- Instead of using **BCE with sigmoid**, you:\n",
    "   - Use a **softmax function** at the output layer (which produces probabilities for 2 classes).\n",
    "   - Use **cross-entropy loss** with the 2-class outputs.\n",
    "\n",
    "---\n",
    "\n",
    "### Key Differences Between BCE and Cross-Entropy for 2 Classes:\n",
    "| Aspect                     | BCE (Binary Cross-Entropy)               | Cross-Entropy for 2 Classes         |\n",
    "|----------------------------|-----------------------------------------|-------------------------------------|\n",
    "| **Output Layer**           | Sigmoid (single output, 0–1)            | Softmax (two outputs summing to 1)  |\n",
    "| **Labels**                 | Single value (0 or 1)                   | One-hot encoded vector [1,0] or [0,1] |\n",
    "| **Loss Function**          | BCE (binary)                            | Cross-Entropy (categorical)         |\n",
    "| **Model Output**           | Single probability (positive class)     | Two probabilities (for both classes) |\n",
    "\n",
    "---\n",
    "\n",
    "### When to Use Which:\n",
    "- **Use BCE**: If your model outputs a **single probability** (via sigmoid) for the positive class.\n",
    "- **Use Cross-Entropy**: If your model outputs **two probabilities** (via softmax) for the two classes.\n",
    "\n",
    "---\n",
    "\n",
    "### Practical Note:\n",
    "- Mathematically, **binary cross-entropy** and **cross-entropy for 2 classes** produce the **same loss value** when implemented correctly. The difference is how you represent the outputs and labels.\n",
    "\n",
    "For example:\n",
    "- **BCE**:\n",
    "   - Output: \\( p \\) (probability of class 1).\n",
    "   - Label: \\( y \\) (0 or 1).\n",
    "   - Loss: \\( - [ y \\log(p) + (1-y) \\log(1-p) ] \\).\n",
    "\n",
    "- **Cross-Entropy**:\n",
    "   - Output: \\( [p_1, p_2] \\) where \\( p_1 + p_2 = 1 \\).\n",
    "   - Label: \\( [1,0] \\) or \\( [0,1] \\).\n",
    "   - Loss: \\( - [ y_1 \\log(p_1) + y_2 \\log(p_2) ] \\).\n",
    "\n",
    "Both are equivalent for 2 classes, just reformulated.\n",
    "\n",
    "---\n",
    "\n",
    "### Why Libraries Combine Them:\n",
    "Many frameworks like **PyTorch** provide a single loss function, such as:\n",
    "- `BCEWithLogitsLoss`: Combines sigmoid + BCE for binary classification.\n",
    "- `CrossEntropyLoss`: Combines softmax + cross-entropy for multi-class classification, including the 2-class case.\n",
    "\n",
    "In summary, while BCE is more efficient for binary classification, cross-entropy loss can still be used for 2 classes with a softmax output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    BCE returns single value for 2 claasses so sigmoid gives label 0 or 1\n",
    "\n",
    "    for two claaes, CE will return 2 values [0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***our vision encoder is contrastive but not our vision language model***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONTRASTIVE VISION ENCODER -- VISION TRANSFORMER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WHY CONTRASTIVE VISION ENCODER? why not ordinary vision encoder?\n",
    "In Sigclip, we want this embedding to not only capture infomration about image but can be contrasted or can be used along with text embedings and this is exactly we do in vision embedding model.\n",
    "\n",
    "and contrasted vision encoder is easy to train because its just crawl it from internet.\n",
    "\n",
    "### ONLY OUR VISION ENCODER model IS CONTRaASTED\n",
    "![alt text](vision_lang_model_01.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exactly! In **SigLIP** (and other contrastive learning frameworks like CLIP), the goal is to ensure that the **vision embeddings** and **text embeddings** share a common **representation space**. This shared space allows embeddings from the two modalities (images and text) to be **comparable** and **contrasted** effectively.\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Idea**: Shared Representation Space\n",
    "The embeddings are trained so that:\n",
    "- An **image embedding** (from the vision model) aligns closely with its **corresponding text embedding** (from the text model).\n",
    "- Non-matching pairs (e.g., unrelated images and texts) are pushed apart in the embedding space.\n",
    "\n",
    "---\n",
    "\n",
    "### **Vision Embedding Model**  \n",
    "In the vision model:\n",
    "1. The input image is passed through a backbone neural network (e.g., ResNet, ViT - Vision Transformer) to produce a **fixed-dimensional embedding**.  \n",
    "2. This embedding is **rich** in visual information and represents the semantic content of the image.  \n",
    "3. The vision embeddings are not just generic visual features—they are explicitly trained to **align** with the corresponding text embeddings.\n",
    "\n",
    "---\n",
    "\n",
    "### **Why Is Contrast Important?**  \n",
    "The ability to **contrast** embeddings across modalities (image and text) is critical because:\n",
    "- It allows the model to **discriminate** between correct (positive) and incorrect (negative) image-text pairs.  \n",
    "- This contrastive behavior is achieved by ensuring:\n",
    "   - Positive pairs (correct image-text pairs) have **high similarity** in the embedding space.\n",
    "   - Negative pairs (incorrect image-text pairs) have **low similarity**.\n",
    "\n",
    "---\n",
    "\n",
    "### **Embedding Alignment Process**  \n",
    "The training process enforces this alignment using a **contrastive loss**:\n",
    "1. **Dot Products**: The similarity between an image embedding \\( v_i \\) and a text embedding \\( t_j \\) is computed as their **dot product**:  \n",
    "   \\[\n",
    "   s_{ij} = v_i \\cdot t_j\n",
    "   \\]\n",
    "2. **Sigmoid-based Binary Classification**: In **SigLIP**, each dot product \\( s_{ij} \\) is treated as a binary classification task:\n",
    "   - Diagonal entries \\( s_{ii} \\) (positive pairs) are pushed toward 1 (high similarity).  \n",
    "   - Off-diagonal entries \\( s_{ij} \\) (negative pairs) are pushed toward 0 (low similarity).  \n",
    "\n",
    "This way, the embedding space becomes **jointly optimized** for both image and text inputs.\n",
    "\n",
    "---\n",
    "\n",
    "### **Outcome**: Joint Embedding Space\n",
    "By training the vision embedding model to align with the text embeddings:\n",
    "- The vision embeddings capture **not only visual features** but also **semantic information** that can be interpreted in relation to text.  \n",
    "- This allows for tasks like **image-text retrieval**, **zero-shot classification**, and other multimodal applications.\n",
    "\n",
    "---\n",
    "\n",
    "### **Summary**  \n",
    "In SigLIP (and CLIP-like models):\n",
    "- The **vision embeddings** are designed to **align** with text embeddings in a **shared representation space**.  \n",
    "- This alignment is achieved through **contrastive learning** with a loss function (e.g., sigmoid-based BCE loss).  \n",
    "- The embeddings can then be **compared** and **contrasted** effectively across modalities, enabling multimodal tasks.\n",
    "\n",
    "This makes the vision model not just a generic feature extractor but a model that understands images in a way that is **semantically compatible** with natural language. 🚀Exactly! In **SigLIP** (and other contrastive learning frameworks like CLIP), the goal is to ensure that the **vision embeddings** and **text embeddings** share a common **representation space**. This shared space allows embeddings from the two modalities (images and text) to be **comparable** and **contrasted** effectively.\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Idea**: Shared Representation Space\n",
    "The embeddings are trained so that:\n",
    "- An **image embedding** (from the vision model) aligns closely with its **corresponding text embedding** (from the text model).\n",
    "- Non-matching pairs (e.g., unrelated images and texts) are pushed apart in the embedding space.\n",
    "\n",
    "---\n",
    "\n",
    "### **Vision Embedding Model**  \n",
    "In the vision model:\n",
    "1. The input image is passed through a backbone neural network (e.g., ResNet, ViT - Vision Transformer) to produce a **fixed-dimensional embedding**.  \n",
    "2. This embedding is **rich** in visual information and represents the semantic content of the image.  \n",
    "3. The vision embeddings are not just generic visual features—they are explicitly trained to **align** with the corresponding text embeddings.\n",
    "\n",
    "---\n",
    "\n",
    "### **Why Is Contrast Important?**  \n",
    "The ability to **contrast** embeddings across modalities (image and text) is critical because:\n",
    "- It allows the model to **discriminate** between correct (positive) and incorrect (negative) image-text pairs.  \n",
    "- This contrastive behavior is achieved by ensuring:\n",
    "   - Positive pairs (correct image-text pairs) have **high similarity** in the embedding space.\n",
    "   - Negative pairs (incorrect image-text pairs) have **low similarity**.\n",
    "\n",
    "---\n",
    "\n",
    "### **Embedding Alignment Process**  \n",
    "The training process enforces this alignment using a **contrastive loss**:\n",
    "1. **Dot Products**: The similarity between an image embedding \\( v_i \\) and a text embedding \\( t_j \\) is computed as their **dot product**:  \n",
    "   \\[\n",
    "   s_{ij} = v_i \\cdot t_j\n",
    "   \\]\n",
    "2. **Sigmoid-based Binary Classification**: In **SigLIP**, each dot product \\( s_{ij} \\) is treated as a binary classification task:\n",
    "   - Diagonal entries \\( s_{ii} \\) (positive pairs) are pushed toward 1 (high similarity).  \n",
    "   - Off-diagonal entries \\( s_{ij} \\) (negative pairs) are pushed toward 0 (low similarity).  \n",
    "\n",
    "This way, the embedding space becomes **jointly optimized** for both image and text inputs.\n",
    "\n",
    "---\n",
    "\n",
    "### **Outcome**: Joint Embedding Space\n",
    "By training the vision embedding model to align with the text embeddings:\n",
    "- The vision embeddings capture **not only visual features** but also **semantic information** that can be interpreted in relation to text.  \n",
    "- This allows for tasks like **image-text retrieval**, **zero-shot classification**, and other multimodal applications.\n",
    "\n",
    "---\n",
    "\n",
    "### **Summary**  \n",
    "In SigLIP (and CLIP-like models):\n",
    "- The **vision embeddings** are designed to **align** with text embeddings in a **shared representation space**.  \n",
    "- This alignment is achieved through **contrastive learning** with a loss function (e.g., sigmoid-based BCE loss).  \n",
    "- The embeddings can then be **compared** and **contrasted** effectively across modalities, enabling multimodal tasks.\n",
    "\n",
    "This makes the vision model not just a generic feature extractor but a model that understands images in a way that is **semantically compatible** with natural language. 🚀"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vision transformer\n",
    "takes in an image as split into pacthes i.e. 16 patches. then we extract information from each patch using convolution producing an embedding for each patch and then flatten all embeddings rsulting in loss of positional information, order doesn't matter here; we just concatenate them. basically we are loosing two dimensionality here.\n",
    "Then we add positional encoding to this embedding vector to contains positional information. UNlike vanilla transformer, These positional encoding are not calculated using sinisidual function but they are learned so that position one in positional encoding vector always get added to top left patch:1 ; and 4 at top right.. so that model still has acces to 2d infomration even though data itelf is flattened. Model will run positional encdoing. Then we feed it to transformer. Transformer does contextualization of this embedding. so transformer intakes a series of embeddings each representing one single patch; the output of transformer through attention mechanism is a series of embeddings but each of these embeedings only not capturing infomration about itself but also about other patches.\n",
    "\n",
    "In language models we use causal masks because text models contains infomration about previous words(autoregressive models) but we don't need that here in vision transformer.Becasue in image there is no auto regressiveness(no sequential order). SO these contextualized embddings not only capture information about themselves but also all other images. So we use these embeedings to capture info about each patch butalso how it is present in image. SO we want each patch to contain infomratoin about its position which is given y positional encoding but we are also concerned about patch's sorroundings in image by cointextualizing.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "![alt text](vision_lang_model_08_vit_contextualization.png)\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "![alt text](vision_lang_model_07_vitp_input.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vision Transformer (ViT) Explained\n",
    "\n",
    "The **Vision Transformer (ViT)** adapts the **Transformer architecture**, originally designed for NLP, to process images. Here’s a detailed breakdown of the explanation you provided:\n",
    "\n",
    "---\n",
    "\n",
    "### 1. **Input Image as Patches**\n",
    "- The input image is divided into **non-overlapping patches** (e.g., 16x16 pixels each).\n",
    "- For a standard image of size \\( 224 \\times 224 \\), splitting into 16x16 patches results in \\( 14 \\times 14 = 196 \\) patches.\n",
    "- Each patch is treated like a \"token\" in a Transformer, similar to how words are tokens in NLP models.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Patch Embedding with Convolution**\n",
    "- To extract features from each patch, we use a **convolution operation** or a **linear projection**.\n",
    "- This produces a **vector embedding** for each patch, representing the features extracted from that patch.\n",
    "- **Flattening**: These patch embeddings are flattened into a **1D sequence** (order doesn't matter yet, and we lose 2D spatial structure at this point).\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Loss of Positional Information**\n",
    "- Flattening the embeddings removes the **2D positional information** (the spatial relationship between patches, like top-left or bottom-right).\n",
    "- Without positional information, the Transformer would not \"know\" where each patch came from, which is crucial for images.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **Positional Encoding**\n",
    "To restore the positional information:\n",
    "- We add **positional encodings** to each patch embedding.  \n",
    "- **Learned Positional Encoding**: Unlike vanilla Transformers (which use fixed sinusoidal functions), Vision Transformers **learn the positional encodings** during training.\n",
    "    - For example, the position \\( P_1 \\) in the positional encoding vector will always get added to the **top-left patch**.\n",
    "    - Position \\( P_4 \\) will correspond to the **top-right patch**, and so on.\n",
    "- This ensures the model retains **2D spatial awareness** even though the embeddings are flattened into a sequence.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. **Feeding Embeddings to the Transformer**\n",
    "- The positional encodings are added to the patch embeddings to create **position-aware embeddings**.\n",
    "- These embeddings are then fed into the **Transformer**.\n",
    "- The Transformer operates on this sequence of embeddings and performs **self-attention**.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. **Contextualization via Attention**\n",
    "- In a Transformer, **self-attention** allows each patch embedding to interact with all other patch embeddings.\n",
    "- The output of the Transformer is a new set of embeddings where:\n",
    "  - Each embedding contains **information about itself** (features of the patch).\n",
    "  - It also captures information about **other patches** (contextual information).\n",
    "\n",
    "This is similar to how words in NLP models \"attend\" to each other to understand context, but here, patches attend to each other to understand the overall structure of the image.\n",
    "\n",
    "---\n",
    "\n",
    "### 7. **No Causal Masks in Vision Transformers**\n",
    "- In language models, we often use **causal masks** for autoregressive training (e.g., predicting the next word while only looking at previous words).\n",
    "- In Vision Transformers, there is **no sequential dependency** between patches like in text.  \n",
    "- Images are not sequential, so we do **not** use causal masks.\n",
    "\n",
    "Instead:\n",
    "- Each patch embedding can \"see\" all other patches simultaneously through self-attention.\n",
    "- This helps the model capture both **local** (features of a single patch) and **global** (relationship with other patches) information.\n",
    "\n",
    "---\n",
    "\n",
    "### 8. **Summary**\n",
    "- The Vision Transformer splits an image into patches and converts each patch into an embedding.\n",
    "- **Positional encodings** are added to restore spatial relationships (learned during training).\n",
    "- The Transformer contextualizes the embeddings using self-attention, enabling each patch to capture:\n",
    "   - Its **own features**.\n",
    "   - Context from **surrounding patches**.\n",
    "- This approach removes the need for convolutional layers or causal masks and allows the Transformer to process the image holistically.\n",
    "\n",
    "---\n",
    "\n",
    "### Why is This Effective?\n",
    "- By using self-attention, the model can **globally model relationships** between patches, which is harder for traditional CNNs that operate with local receptive fields.\n",
    "- Positional encodings ensure that spatial relationships are retained, even when the image is represented as a sequence.\n",
    "\n",
    "Let me know if you’d like a visual representation or further clarifications on any part! 😊"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vision Transformer (ViT): In-Depth Explanation\n",
    "\n",
    "The **Vision Transformer (ViT)** is a model that applies the Transformer architecture, originally designed for NLP tasks, to computer vision. Instead of using convolutional neural networks (CNNs) to process images, ViT uses **self-attention** to capture both local and global features of an image. Let’s break it down in more depth:\n",
    "\n",
    "---\n",
    "\n",
    "## 1. **Image as Patches**\n",
    "\n",
    "- **Traditional CNNs** process images as 2D grids and use convolutional filters to extract spatial features.  \n",
    "- **ViT**, on the other hand, treats an image as a **sequence of patches**, much like words in a sentence for NLP.  \n",
    "- The image \\( I \\) of size \\( H \\times W \\times C \\) (Height × Width × Channels) is divided into **non-overlapping patches** of size \\( P \\times P \\), where \\( P \\) is the patch size.\n",
    "\n",
    "### Example:\n",
    "For an image of size \\( 224 \\times 224 \\times 3 \\) (standard ImageNet input):\n",
    "- If \\( P = 16 \\), the image is divided into \\( 14 \\times 14 = 196 \\) patches.\n",
    "- Each patch has dimensions \\( 16 \\times 16 \\times 3 \\), which are **flattened** into a vector of size \\( 16 \\times 16 \\times 3 = 768 \\).\n",
    "\n",
    "---\n",
    "\n",
    "## 2. **Patch Embeddings**\n",
    "\n",
    "Each patch is treated as a **token**. To convert these patches into a form usable by the Transformer:\n",
    "- **Linear Projection**: A linear layer (or convolution) is applied to **flatten each patch** into a 1D embedding vector.  \n",
    "- This linear layer maps each patch \\( P \\) (size \\( P \\times P \\times C \\)) into a **D-dimensional embedding vector**:\n",
    "  \\[\n",
    "  x_i = \\text{Linear}(\\text{Flatten}(P_i))\n",
    "  \\]\n",
    "  where \\( x_i \\) is the embedding for patch \\( i \\).\n",
    "\n",
    "- This gives a sequence of embeddings:\n",
    "  \\[\n",
    "  X = [x_1, x_2, ..., x_N] \\quad \\text{where} \\quad N = \\frac{H \\times W}{P^2}\n",
    "  \\]\n",
    "  \\( N \\) is the total number of patches.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. **Positional Encodings**\n",
    "\n",
    "The Transformer does not inherently understand the **order** or **position** of the patches.  \n",
    "To provide positional information, **positional encodings** are added to the patch embeddings.\n",
    "\n",
    "### Why is Positional Encoding Needed?\n",
    "- After flattening the patches into a sequence, the **2D spatial structure** of the image is lost.\n",
    "- Without positional information, the Transformer treats the patches as unordered tokens.\n",
    "\n",
    "### Learned Positional Encoding:\n",
    "- Unlike NLP Transformers that often use **sinusoidal positional encodings** (fixed), ViT **learns positional encodings** during training.  \n",
    "- Each positional encoding \\( PE_i \\) is a learnable vector added to the patch embedding \\( x_i \\):\n",
    "  \\[\n",
    "  z_i = x_i + PE_i\n",
    "  \\]\n",
    "  where \\( z_i \\) is the position-aware embedding for patch \\( i \\).\n",
    "\n",
    "### 2D Spatial Awareness:\n",
    "- The positional encodings are learned such that:\n",
    "  - The first positional encoding corresponds to the **top-left patch**.\n",
    "  - The last positional encoding corresponds to the **bottom-right patch**.\n",
    "- This ensures the model retains **2D spatial relationships** even though the patches are flattened.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. **Class Token (Optional)**\n",
    "\n",
    "In addition to the patch embeddings, ViT introduces a special **[CLS] token** (classification token) similar to BERT:\n",
    "- A **learnable vector** \\( x_{cls} \\) is prepended to the sequence of patch embeddings.\n",
    "- The output corresponding to this token at the final layer is used for **classification**.\n",
    "\n",
    "### Input to the Transformer:\n",
    "The input to the Transformer is the sequence:\n",
    "\\[\n",
    "Z = [x_{cls}, z_1, z_2, ..., z_N]\n",
    "\\]\n",
    "where \\( z_i \\) are the position-aware patch embeddings.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. **Transformer Encoder**\n",
    "\n",
    "The Transformer encoder processes the sequence of embeddings. It consists of:\n",
    "1. **Multi-Head Self-Attention (MHSA)**:  \n",
    "   - Allows each patch embedding to attend to all other patches.\n",
    "   - Each patch captures **global context** by interacting with all other patches.\n",
    "\n",
    "2. **Feed-Forward Network (FFN)**:  \n",
    "   - A position-wise MLP (Multi-Layer Perceptron) applied to each embedding.\n",
    "\n",
    "3. **Layer Normalization** and **Residual Connections**:  \n",
    "   - Ensure stable training and efficient gradient flow.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. **Self-Attention in ViT**\n",
    "\n",
    "The self-attention mechanism is key to ViT. It allows patches to interact with each other:\n",
    "- Each patch embedding \\( z_i \\) queries the other embeddings \\( z_j \\) using **Query, Key, and Value** projections:\n",
    "  \\[\n",
    "  \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right) V\n",
    "  \\]\n",
    "  where:\n",
    "  - \\( Q = z_i W_Q \\) (Query),\n",
    "  - \\( K = z_j W_K \\) (Key),\n",
    "  - \\( V = z_j W_V \\) (Value),\n",
    "  - \\( d_k \\) is the dimensionality of the keys.\n",
    "\n",
    "- The result is that each patch embedding is updated to include:\n",
    "  - Its own information.\n",
    "  - Contextual information from all other patches.\n",
    "\n",
    "---\n",
    "\n",
    "## 7. **No Causal Masking**\n",
    "\n",
    "In NLP tasks, causal masks ensure that a token can only attend to previous tokens (autoregressive property).  \n",
    "In ViT:\n",
    "- **No causal masks** are used because patches do not have a sequential order like words in a sentence.\n",
    "- Each patch can attend to all other patches **simultaneously**.\n",
    "\n",
    "This allows the model to capture **global context** across the entire image.\n",
    "\n",
    "---\n",
    "\n",
    "## 8. **Output of the Transformer**\n",
    "\n",
    "The output of the Transformer is a **sequence of contextualized embeddings**:\n",
    "- Each embedding \\( z_i' \\) corresponds to a patch, but it now contains:\n",
    "  - Information about the patch itself.\n",
    "  - Information about the **relationships** between the patch and all other patches.\n",
    "\n",
    "For classification:\n",
    "- The output corresponding to the **[CLS] token** is used as the image representation.\n",
    "\n",
    "For segmentation or reconstruction tasks:\n",
    "- The outputs corresponding to each patch are used.\n",
    "\n",
    "---\n",
    "\n",
    "## 9. **Advantages of Vision Transformers**\n",
    "\n",
    "1. **Global Context**:\n",
    "   - Unlike CNNs, which rely on local receptive fields, ViT uses self-attention to model **global relationships** between patches.\n",
    "\n",
    "2. **Flexibility**:\n",
    "   - ViT is more flexible than CNNs because it does not impose a **fixed inductive bias** (like locality or translation invariance).\n",
    "\n",
    "3. **Scalability**:\n",
    "   - When trained on large datasets (e.g., ImageNet-21k or JFT-300M), ViTs outperform CNNs.\n",
    "\n",
    "---\n",
    "\n",
    "## 10. **Summary Workflow**\n",
    "\n",
    "1. **Split the Image into Patches**: Convert \\( H \\times W \\) image into \\( N \\) patches.\n",
    "2. **Linear Projection**: Embed each patch into a \\( D \\)-dimensional vector.\n",
    "3. **Add Positional Encodings**: Restore spatial information lost during flattening.\n",
    "4. **Transformer Encoder**:\n",
    "   - Process the sequence of embeddings using **self-attention**.\n",
    "   - Capture global and contextual information.\n",
    "5. **Output**:\n",
    "   - Use the **[CLS] token** for classification or patch embeddings for other tasks.\n",
    "\n",
    "---\n",
    "\n",
    "### Key Insights:\n",
    "- Vision Transformers treat images as sequences of patches and use self-attention to model relationships between patches.\n",
    "- Positional encodings ensure that spatial information is preserved.\n",
    "- The Transformer encoder captures both **local features** and **global context** without relying on convolution.\n",
    "\n",
    "Would you like to dive deeper into specific parts, such as self-attention math or training strategies for ViT? 😊"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01_modelling_siglip.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Tuple\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# poligemma\n",
    "\n",
    "\n",
    "class SiglipVisionConfig:\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_size=768,  # embedding size\n",
    "        intermediate_size=3072,  # size of linear layer\n",
    "        num_hidden_layers=12,  # number of layers of vision transformer\n",
    "        num_attention_heads=12,  # number of heads in multihead attention\n",
    "        num_channels=3,  # RGB\n",
    "        image_size=224,\n",
    "        patch_size=14,\n",
    "        layer_norm_eps=1e-6,\n",
    "        attention_dropout=0.0,\n",
    "        num_image_tokens: int = None,  # how many output embedding we will have for each image; each of these contextualized embedding will be considered as a tokens of image.It wont ba a one single embrding that represents whole imagebut list of embeddings that represesnt a patch of each image and als info about other patches throigh the attention mechanismo\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.intermediate_size = (intermediate_size,)\n",
    "        self.num_hidden_layers = (num_hidden_layers,)\n",
    "        self.num_attention_heads = (num_attention_heads,)\n",
    "        self.num_channels = (num_channels,)\n",
    "        self.image_size = (image_size,)\n",
    "        self.patch_size = (patch_size,)\n",
    "        self.layer_norm_eps = (layer_norm_eps,)\n",
    "        self.attention_dropout = (attention_dropout,)\n",
    "        self.num_image_tokens = num_image_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Python code defines a **configuration class** for a custom Vision Transformer (ViT)-based architecture, called `SiglipVisionConfig`. This configuration class is intended to hold hyperparameters and settings that control the structure and behavior of a Vision Transformer model. Let’s break it down step by step:\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Imports**\n",
    "```python\n",
    "from typing import Optional, Tuple\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "```\n",
    "- **`typing`**: Provides support for type hints like `Optional` and `Tuple`.\n",
    "- **`torch` and `torch.nn`**: Used for defining and implementing neural network layers and operations in PyTorch.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. `SiglipVisionConfig` Class**\n",
    "This class serves as a configuration container for the Vision Transformer model. The hyperparameters defined here control various aspects of the architecture, such as the number of layers, attention heads, and embedding sizes.\n",
    "\n",
    "#### **Constructor (`__init__` Method)**\n",
    "```python\n",
    "class SiglipVisionConfig:\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_size=768,  # Size of the embedding vector for each patch\n",
    "        intermediate_size=3072,  # Dimensionality of the MLP layers in the transformer\n",
    "        num_hidden_layers=12,  # Number of transformer encoder layers\n",
    "        num_attention_heads=12,  # Number of attention heads in multi-head self-attention\n",
    "        num_channels=3,  # Number of image channels (e.g., 3 for RGB)\n",
    "        image_size=224,  # Height/Width of the input image\n",
    "        patch_size=14,  # Size of each patch (14x14 pixels)\n",
    "        layer_norm_eps=1e-6,  # Epsilon value for LayerNorm (numerical stability)\n",
    "        attention_dropout=0.0,  # Dropout rate for attention\n",
    "        num_image_tokens: int = None,  # Number of contextualized embeddings (tokens) for the image\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__()\n",
    "```\n",
    "\n",
    "The **parameters** define the key components of the Vision Transformer. Let’s dive into each:\n",
    "\n",
    "#### **Key Parameters**\n",
    "1. **`hidden_size=768`**:\n",
    "   - The dimensionality of the embedding for each patch after projection.  \n",
    "   - Each patch of the image will be represented by a vector of size \\( 768 \\).\n",
    "\n",
    "2. **`intermediate_size=3072`**:\n",
    "   - The size of the intermediate layer in the **feed-forward network (FFN)** inside each Transformer encoder block.  \n",
    "   - Typically, \\( \\text{intermediate\\_size} = 4 \\times \\text{hidden\\_size} \\).\n",
    "\n",
    "3. **`num_hidden_layers=12`**:\n",
    "   - Number of **Transformer encoder layers** (or blocks) in the model.\n",
    "\n",
    "4. **`num_attention_heads=12`**:\n",
    "   - Number of heads in **multi-head self-attention**.\n",
    "   - Each attention head works independently and captures relationships between patches.\n",
    "\n",
    "5. **`num_channels=3`**:\n",
    "   - The number of input channels in the image (e.g., 3 for RGB, 1 for grayscale).\n",
    "\n",
    "6. **`image_size=224`**:\n",
    "   - The height and width of the input image (e.g., \\( 224 \\times 224 \\)).\n",
    "\n",
    "7. **`patch_size=14`**:\n",
    "   - The size of each non-overlapping patch.  \n",
    "   - If \\( \\text{image\\_size} = 224 \\) and \\( \\text{patch\\_size} = 14 \\), the image is divided into \\( (224 / 14) \\times (224 / 14) = 16 \\times 16 = 256 \\) patches.\n",
    "\n",
    "8. **`layer_norm_eps=1e-6`**:\n",
    "   - A small constant added to the denominator in **Layer Normalization** for numerical stability.\n",
    "\n",
    "9. **`attention_dropout=0.0`**:\n",
    "   - Dropout rate applied to the attention weights during training to prevent overfitting.\n",
    "\n",
    "10. **`num_image_tokens`**:\n",
    "    - Specifies the number of contextualized embeddings (tokens) for the image after processing by the Transformer.\n",
    "    - Each embedding represents information about a specific patch while incorporating relationships with other patches through the attention mechanism.\n",
    "\n",
    "    #### Key Insight:\n",
    "    - Instead of representing the whole image as a single embedding, the model produces **multiple embeddings** (one for each patch). This allows for richer representations that preserve **spatial and contextual information**.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Additional Attributes**\n",
    "After the parameters are passed into the constructor, they are stored as attributes of the class:\n",
    "```python\n",
    "self.hidden_size = hidden_size\n",
    "self.intermediate_size = (intermediate_size,)\n",
    "self.num_hidden_layers = (num_hidden_layers,)\n",
    "self.num_attention_heads = (num_attention_heads,)\n",
    "self.num_channels = (num_channels,)\n",
    "self.image_size = (image_size,)\n",
    "self.patch_size = (patch_size,)\n",
    "self.layer_norm_eps = (layer_norm_eps,)\n",
    "self.attention_dropout = (attention_dropout,)\n",
    "self.num_image_tokens = num_image_tokens\n",
    "```\n",
    "\n",
    "- Each hyperparameter is assigned to an attribute (e.g., `self.hidden_size`), making it accessible later when defining or using the model.\n",
    "- Some attributes are stored as **tuples** (e.g., `self.intermediate_size = (intermediate_size,)`), possibly for compatibility with other parts of the code.\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Comments in the Code**\n",
    "The comment under `num_image_tokens` provides important clarification:\n",
    "```python\n",
    "# how many output embeddings we will have for each image; \n",
    "# each of these contextualized embeddings will be considered as tokens of the image.\n",
    "# It won’t be a single embedding that represents the whole image, \n",
    "# but a list of embeddings that represent a patch of the image and also info \n",
    "# about other patches through the attention mechanism.\n",
    "```\n",
    "\n",
    "This explains that:\n",
    "- **`num_image_tokens`** determines how many embeddings (tokens) are produced for each image.  \n",
    "- Instead of summarizing the entire image into one embedding (as in CNNs), ViT outputs a **sequence of embeddings**, one for each patch.  \n",
    "- These embeddings are enriched by the **self-attention mechanism**, which allows each patch to encode information about other patches.\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Purpose of the Class**\n",
    "This class provides a centralized way to:\n",
    "1. **Store hyperparameters** for the Vision Transformer model.\n",
    "2. **Configure the architecture** dynamically by modifying the parameters.\n",
    "\n",
    "When designing the model, these attributes (e.g., `hidden_size`, `num_attention_heads`) will be used to construct the layers (e.g., multi-head attention, feed-forward networks).\n",
    "\n",
    "---\n",
    "\n",
    "### **6. Next Steps**\n",
    "- This class is only the configuration container. To implement the Vision Transformer:\n",
    "  - Use these attributes to define layers like **patch embeddings**, **multi-head self-attention**, and **MLPs**.\n",
    "  - Add training logic (e.g., classification head) on top of the transformer outputs.\n",
    "\n",
    "Would you like to see how to use this configuration to build the actual Vision Transformer? 😊"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiglipVisionModel(nn.Module):\n",
    "\n",
    "    def __init__(self, config: SiglipVisionConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.visio_model = SiglipVisionTransformer(config)\n",
    "\n",
    "    def forward(self, pixel_values) -> Tuple:\n",
    "        \"\"\"\n",
    "        [batch_size, num_channels, height, width] => [batch_size, num_image_tokens/num_patches, hidden_size/embedding_dim]\n",
    "        takes in batch of images and returns list of embeddigs for each image in batch\n",
    "        \"\"\"\n",
    "\n",
    "        return self.vision_model(pixel_values=pixel_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not necessarily, but they are closely related. Let’s clarify the difference:\n",
    "\n",
    "---\n",
    "\n",
    "### **Number of Patches**\n",
    "The **number of patches** refers to how the input image is divided into smaller non-overlapping segments. It is calculated as:\n",
    "\n",
    "\\[\n",
    "\\text{Number of patches} = \\left(\\frac{\\text{image\\_size}}{\\text{patch\\_size}}\\right)^2\n",
    "\\]\n",
    "\n",
    "For example:\n",
    "- If the image size is \\( 224 \\times 224 \\) and the patch size is \\( 14 \\times 14 \\), then:\n",
    "  \\[\n",
    "  \\text{Number of patches} = \\left(\\frac{224}{14}\\right)^2 = 16 \\times 16 = 256\n",
    "  \\]\n",
    "\n",
    "Each patch will then be converted into a **patch embedding** of size `hidden_size` (e.g., 768).\n",
    "\n",
    "---\n",
    "\n",
    "### **num_image_tokens**\n",
    "The **`num_image_tokens`** refers to the number of output tokens produced by the Vision Transformer. Typically:\n",
    "- If there are no additional tokens (like a class token), **`num_image_tokens` is equal to the number of patches**.\n",
    "- However, if additional tokens (e.g., a classification token or other special tokens) are added, then:\n",
    "  \\[\n",
    "  \\text{num\\_image\\_tokens} = \\text{number of patches} + \\text{number of special tokens}\n",
    "  \\]\n",
    "\n",
    "For example:\n",
    "1. If there are **256 patches** and no extra tokens, then:\n",
    "   \\[\n",
    "   \\text{num\\_image\\_tokens} = 256\n",
    "   \\]\n",
    "\n",
    "2. If there are **256 patches** and **1 class token** (as in the original Vision Transformer), then:\n",
    "   \\[\n",
    "   \\text{num\\_image\\_tokens} = 256 + 1 = 257\n",
    "   \\]\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Differences**\n",
    "| **Aspect**                | **Number of Patches**                      | **num_image_tokens**                            |\n",
    "|---------------------------|--------------------------------------------|------------------------------------------------|\n",
    "| **Definition**            | Number of patches extracted from the image | Number of output tokens after processing       |\n",
    "| **Includes Class Token?** | No                                         | Sometimes (if class tokens are used)           |\n",
    "| **Typical Usage**         | Input size for the Transformer             | Output size of the Transformer (per sequence) |\n",
    "\n",
    "---\n",
    "\n",
    "### **Conclusion**\n",
    "- In most cases, **`num_image_tokens` is the same as the number of patches**, unless extra tokens (e.g., class tokens) are added to the input sequence.  \n",
    "- If your Vision Transformer does not use a class token, then:\n",
    "  \\[\n",
    "  \\text{num\\_image\\_tokens} = \\text{number of patches}\n",
    "  \\]\n",
    "Would you like more details on how class tokens or special tokens are used in Vision Transformers? 😊"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiglipVisionTransformer(nn.Module):\n",
    "    def __init__(self, config: SiglipVisionConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        embed_dim = config.hidden_size\n",
    "\n",
    "        self.embeddings = SiglipVisionEmbeddings(\n",
    "            config\n",
    "        )  # pacthes will be converted to embeddings\n",
    "        self.encoder = SiglipEncoder(config)\n",
    "        self.post_layernorm = nn.LayerNorm(embed_dim, eps=config.layer_norm_eps)\n",
    "\n",
    "    def forward(self, pixel_values: torch.Tensor) -> torch.Tensor:\n",
    "        # pixel values: [batch_size, num_channels, height, width] => [batch_size, num_image_tokens/num_patches, hidden_size/embedding_dim]\n",
    "        hidden_states = self.embeddings(pixel_values)\n",
    "        last_hidden_state = self.encoder(input_embds=hidden_states)\n",
    "        last_hidden_state = self.post_layernorm(last_hidden_state)\n",
    "        return last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s break this code down step by step:\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Class Definition**\n",
    "The `SiglipVisionTransformer` class is a custom implementation of a Vision Transformer (ViT). It uses two main components:\n",
    "- **Embeddings**: Converts input image patches into embeddings.\n",
    "- **Encoder**: Applies the Transformer architecture (multi-head self-attention and feedforward layers) to process these embeddings.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Constructor (`__init__`)**\n",
    "```python\n",
    "def __init__(self, config: SiglipVisionConfig):\n",
    "    super().__init__()\n",
    "    self.config = config\n",
    "    embed_dim = config.hidden_size\n",
    "\n",
    "    self.embeddings = SiglipVisionEmbeddings(config) # Patches will be converted to embeddings\n",
    "    self.encoder = SiglipEncoder(config)\n",
    "    self.post_layernorm = nn.LayerNorm(embed_dim, eps=config.layer_norm_eps)\n",
    "```\n",
    "\n",
    "- **`config`**: An instance of `SiglipVisionConfig` that holds all the hyperparameters (e.g., `hidden_size`, `num_attention_heads`, etc.).\n",
    "- **`embed_dim`**: Equal to `hidden_size` (the size of the patch embeddings).\n",
    "- **`self.embeddings`**: Responsible for:\n",
    "  - Dividing the input image into patches.\n",
    "  - Converting each patch into a fixed-size embedding.\n",
    "  - Adding positional encodings to retain spatial information.\n",
    "- **`self.encoder`**: The Transformer encoder, which applies self-attention and feedforward layers to contextualize the embeddings.\n",
    "- **`self.post_layernorm`**: A Layer Normalization applied after the encoder to stabilize the output.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Forward Method (`forward`)**\n",
    "The `forward` method defines how the input data flows through the model.\n",
    "\n",
    "```python\n",
    "def forward(self, pixel_values: torch.Tensor) -> torch.Tensor:\n",
    "    # pixel_values: [batch_size, num_channels, height, width] => [batch_size, num_image_tokens/num_patches, hidden_size/embedding_dim]\n",
    "    hidden_states = self.embeddings(pixel_values)\n",
    "    last_hidden_state = self.encoder(input_embds = hidden_states)\n",
    "    last_hidden_state = self.post_layernorm(last_hidden_state)\n",
    "    return last_hidden_state\n",
    "```\n",
    "\n",
    "#### **Input**\n",
    "- **`pixel_values`**: A batch of input images with shape:\n",
    "  \\[\n",
    "  [\\text{batch\\_size}, \\text{num\\_channels}, \\text{height}, \\text{width}]\n",
    "  \\]\n",
    "\n",
    "#### **Steps**\n",
    "1. **Convert Image to Embeddings (`self.embeddings`)**:\n",
    "   - The image is divided into patches.\n",
    "   - Each patch is flattened and projected into an embedding of size `hidden_size`.\n",
    "   - Positional encodings are added to these embeddings.\n",
    "   - The output shape is:\n",
    "     \\[\n",
    "     [\\text{batch\\_size}, \\text{num\\_patches}, \\text{hidden\\_size}]\n",
    "     \\]\n",
    "\n",
    "2. **Contextualize Embeddings (`self.encoder`)**:\n",
    "   - The embeddings are passed through the Transformer encoder.\n",
    "   - Self-attention allows each patch to attend to information from all other patches.\n",
    "   - The output shape remains:\n",
    "     \\[\n",
    "     [\\text{batch\\_size}, \\text{num\\_patches}, \\text{hidden\\_size}]\n",
    "     \\]\n",
    "\n",
    "3. **Post Layer Normalization (`self.post_layernorm`)**:\n",
    "   - Layer normalization is applied to stabilize the outputs.\n",
    "\n",
    "#### **Output**\n",
    "- **`last_hidden_state`**:\n",
    "  - A tensor containing the final contextualized embeddings for all patches.\n",
    "  - Shape:\n",
    "    \\[\n",
    "    [\\text{batch\\_size}, \\text{num\\_patches}, \\text{hidden\\_size}]\n",
    "    \\]\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Key Points**\n",
    "- **`self.embeddings`** handles patch extraction and positional encoding.\n",
    "- **`self.encoder`** processes embeddings through multi-head attention and feedforward layers, capturing relationships between patches.\n",
    "- **`self.post_layernorm`** ensures numerical stability and normalization after encoding.\n",
    "\n",
    "---\n",
    "\n",
    "### **High-Level Flow**\n",
    "1. **Input**: Raw pixel values \\([B, C, H, W]\\).\n",
    "2. **Embeddings**:\n",
    "   - Divide the image into patches.\n",
    "   - Convert each patch into a fixed-size embedding.\n",
    "   - Add positional encodings.\n",
    "   - Output shape: \\([B, N, D]\\), where \\(N\\) is the number of patches, and \\(D\\) is `hidden_size`.\n",
    "3. **Transformer Encoder**:\n",
    "   - Contextualize the embeddings using self-attention.\n",
    "   - Output shape: \\([B, N, D]\\).\n",
    "4. **Normalization**:\n",
    "   - Apply LayerNorm to the final embeddings.\n",
    "   - Output shape: \\([B, N, D]\\).\n",
    "\n",
    "---\n",
    "\n",
    "Would you like a detailed explanation of any specific part (e.g., embeddings, encoder, or attention mechanism)? 😊"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### encoder \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "afer converting to embedding, we'll run it  through list of transformwe encoder layers\n",
    "In case of vision transformer.. normalization is done before FEED FORWARD and MULTI-HEAD ATTENTION\n",
    "![alt text](vision_lang_model_09_vit_encoder_layers.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***In convolution , we convolve each kernel on all channels of rgb image***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In convolution, we convolve a **single kernel** with **all the channels of the input image** (e.g., the RGB channels) simultaneously. Here's how it works:\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Single Kernel Across All Channels**\n",
    "When performing a convolution operation on an RGB image:\n",
    "- Each **kernel** has a depth equal to the number of channels in the input image (e.g., 3 for RGB).\n",
    "- The kernel is not a 2D matrix but a **3D tensor** with shape \\((k_h, k_w, c_{in})\\), where:\n",
    "  - \\(k_h\\) and \\(k_w\\): Kernel height and width.\n",
    "  - \\(c_{in}\\): Number of input channels (e.g., 3 for RGB).\n",
    "\n",
    "The convolution operation is applied to **all the channels of the image simultaneously** by summing the results across the channels.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. How Convolution Works**\n",
    "- **Input Image Shape**: \\((H, W, C_{in})\\), where \\(H\\) = height, \\(W\\) = width, and \\(C_{in}\\) = number of channels (e.g., 3 for RGB).\n",
    "- **Kernel Shape**: \\((k_h, k_w, C_{in})\\), where the depth matches the number of input channels.\n",
    "- **Output Pixel Calculation**:\n",
    "  For each spatial position in the output, the kernel slides over the corresponding position in the input image, performs an element-wise multiplication across all channels, and sums the result:\n",
    "  \\[\n",
    "  \\text{Output}[i, j] = \\sum_{c=1}^{C_{in}} \\sum_{x=1}^{k_h} \\sum_{y=1}^{k_w} \\text{Kernel}[x, y, c] \\cdot \\text{Input}[i+x, j+y, c]\n",
    "  \\]\n",
    "\n",
    "- The result of this operation is a **single-channel feature map** (i.e., the output of one kernel).\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Multiple Kernels for Multiple Feature Maps**\n",
    "- In practice, convolution layers typically use multiple kernels (e.g., 32 or 64) to capture different features of the input image.\n",
    "- Each kernel produces its own **output feature map**.\n",
    "- If there are \\(C_{out}\\) kernels, the output of the convolution layer has shape:\n",
    "  \\[\n",
    "  (H_{\\text{out}}, W_{\\text{out}}, C_{\\text{out}})\n",
    "  \\]\n",
    "  where \\(C_{\\text{out}}\\) is the number of kernels.\n",
    "\n",
    "---\n",
    "\n",
    "### **Example: RGB Image Convolution**\n",
    "- Input image: \\((224, 224, 3)\\) (height, width, RGB channels).\n",
    "- Kernel: \\((3, 3, 3)\\) (height, width, and depth matching the input channels).\n",
    "- Output from **one kernel**: A single feature map of shape \\((222, 222)\\).\n",
    "- If there are 64 kernels, the output has shape \\((222, 222, 64)\\).\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Takeaways**\n",
    "1. Each kernel operates across **all input channels simultaneously**.\n",
    "2. The depth of the kernel matches the number of input channels.\n",
    "3. The final output is a stack of feature maps, one for each kernel.\n",
    "\n",
    "Would you like to dive deeper into multi-channel convolution or its implementation in PyTorch? 😊"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for //: 'tuple' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m image_size \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m224\u001b[39m,)\n\u001b[1;32m      2\u001b[0m patch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m14\u001b[39m\n\u001b[0;32m----> 3\u001b[0m num_patches \u001b[38;5;241m=\u001b[39m (\u001b[43mimage_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpatch_size\u001b[49m) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m  \u001b[38;5;66;03m# **2 because of 2D image i.e. 16 * 16\u001b[39;00m\n\u001b[1;32m      4\u001b[0m num_positions \u001b[38;5;241m=\u001b[39m num_patches  \u001b[38;5;66;03m# positional encidings are equal to number of patches becasue we need the inforrmation about where each patch is in the image.\u001b[39;00m\n\u001b[1;32m      5\u001b[0m position_embedding \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mEmbedding(\n\u001b[1;32m      6\u001b[0m     num_positions, embed_dim\n\u001b[1;32m      7\u001b[0m )  \u001b[38;5;66;03m# this vector is same size of partch embedding vector  # each of this will be added to patvh_embedding vector\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for //: 'tuple' and 'int'"
     ]
    }
   ],
   "source": [
    "image_size = (224,)\n",
    "patch_size = 14\n",
    "num_patches = (image_size // patch_size) ** 2  # **2 because of 2D image i.e. 16 * 16\n",
    "num_positions = num_patches  # positional encidings are equal to number of patches becasue we need the inforrmation about where each patch is in the image.\n",
    "position_embedding = nn.Embedding(\n",
    "    num_positions, embed_dim\n",
    ")  # this vector is same size of partch embedding vector  # each of this will be added to patvh_embedding vector\n",
    "register_buffer(\n",
    "    \"position_ids\",\n",
    "    torch.arange(num_positions).expand((1, -1)),\n",
    "    persistent=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here’s the detailed breakdown with **initial shapes** and **shape transformations** for each line in the forward method, including example values.\n",
    "\n",
    "---\n",
    "\n",
    "### Initial Input Shape:\n",
    "```python\n",
    "pixel_values: torch.FloatTensor  # Shape: [Batch_Size, Num_Channels, Height, Width]\n",
    "```\n",
    "Example:\n",
    "- **Input Shape:** [8, 3, 224, 224]  \n",
    "  (Batch size = 8, RGB image with height = 224, width = 224)\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Extract dimensions:\n",
    "```python\n",
    "_, _, height, width = pixel_values.shape\n",
    "```\n",
    "- **Shape:** [8, 3, 224, 224] (No change)\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Apply the patch embedding convolution:\n",
    "```python\n",
    "patch_embeds = self.patch_embedding(pixel_values)\n",
    "```\n",
    "- Convolution operation:\n",
    "  - Kernel size = `Patch_Size x Patch_Size`\n",
    "  - Stride = `Patch_Size` (non-overlapping patches)\n",
    "  - **Input Shape:** [8, 3, 224, 224]\n",
    "  - **Output Shape:** [Batch_Size, Embed_Dim, Num_Patches_H, Num_Patches_W]\n",
    "    - `Num_Patches_H = Height // Patch_Size = 224 // 16 = 14`\n",
    "    - `Num_Patches_W = Width // Patch_Size = 224 // 16 = 14`\n",
    "  - **Resulting Shape:** [8, 768, 14, 14]  \n",
    "    (Embed_Dim = 768 for each patch)\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Flatten the patches:\n",
    "```python\n",
    "embeddings = patch_embeds.flatten(2)\n",
    "```\n",
    "- Flatten the last two dimensions (`Num_Patches_H` and `Num_Patches_W`):\n",
    "  - **Input Shape:** [8, 768, 14, 14]\n",
    "  - **Output Shape:** [8, 768, Num_Patches]\n",
    "    - `Num_Patches = Num_Patches_H * Num_Patches_W = 14 * 14 = 196`\n",
    "  - **Resulting Shape:** [8, 768, 196]\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Transpose embeddings:\n",
    "```python\n",
    "embeddings = embeddings.transpose(1, 2)\n",
    "```\n",
    "- Swap the second and third dimensions:\n",
    "  - **Input Shape:** [8, 768, 196]\n",
    "  - **Output Shape:** [8, 196, 768]\n",
    "    - `196` (Num_Patches) becomes the sequence length for the transformer.\n",
    "  - **Resulting Shape:** [8, 196, 768]\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Add positional embeddings:\n",
    "```python\n",
    "embeddings = embeddings + self.position_embedding(self.position_ids)\n",
    "```\n",
    "- Positional embeddings are added element-wise:\n",
    "  - **Input Shape (embeddings):** [8, 196, 768]\n",
    "  - **Positional Embedding Shape:** [1, 196, 768] (broadcasted across the batch dimension)\n",
    "  - **Output Shape:** [8, 196, 768]  \n",
    "    (Spatial information is encoded for each patch)\n",
    "\n",
    "---\n",
    "\n",
    "### Final Output:\n",
    "```python\n",
    "return embeddings\n",
    "```\n",
    "- **Final Shape:** [8, 196, 768]  \n",
    "  (Batch of 8, sequence of 196 patches, embedding size 768)\n",
    "\n",
    "---\n",
    "\n",
    "### Summary Table:\n",
    "\n",
    "| Step                          | Input Shape        | Output Shape       | Example Shape           |\n",
    "|-------------------------------|--------------------|--------------------|-------------------------|\n",
    "| **Input Tensor**              | [Batch_Size, Num_Channels, Height, Width] | [Batch_Size, Num_Channels, Height, Width] | [8, 3, 224, 224] |\n",
    "| **Patch Embedding (Conv2D)**  | [Batch_Size, Num_Channels, Height, Width] | [Batch_Size, Embed_Dim, Num_Patches_H, Num_Patches_W] | [8, 768, 14, 14] |\n",
    "| **Flatten**                   | [Batch_Size, Embed_Dim, Num_Patches_H, Num_Patches_W] | [Batch_Size, Embed_Dim, Num_Patches] | [8, 768, 196] |\n",
    "| **Transpose**                 | [Batch_Size, Embed_Dim, Num_Patches]     | [Batch_Size, Num_Patches, Embed_Dim] | [8, 196, 768] |\n",
    "| **Add Positional Embeddings** | [Batch_Size, Num_Patches, Embed_Dim]     | [Batch_Size, Num_Patches, Embed_Dim] | [8, 196, 768] |\n",
    "\n",
    "Let me know if you need further clarifications!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's break down the shape transformation of `self.position_embedding(self.position_ids)` step by step:\n",
    "\n",
    "---\n",
    "\n",
    "### 1. **Initialization of `position_ids`**\n",
    "In the `__init__` method, `position_ids` is registered as a buffer:\n",
    "```python\n",
    "self.register_buffer(\n",
    "    \"position_ids\",\n",
    "    torch.arange(self.num_positions).expand((1, -1)),\n",
    "    persistent=False,\n",
    ")\n",
    "```\n",
    "- `torch.arange(self.num_positions)` generates a 1D tensor of integers from `0` to `self.num_positions - 1`.\n",
    "  - Shape: `[self.num_positions]`  \n",
    "    Example: `[0, 1, 2, ..., 195]` if `self.num_positions = 196`.\n",
    "  \n",
    "- `.expand((1, -1))` adds a batch dimension and expands it without allocating new memory:\n",
    "  - Shape: `[1, self.num_positions]`  \n",
    "    Example: `[1, 196]`.\n",
    "\n",
    "So, `self.position_ids` has shape **[1, self.num_positions]**.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Positional Embedding Lookup**\n",
    "In the forward method, `self.position_embedding` is called:\n",
    "```python\n",
    "self.position_embedding(self.position_ids)\n",
    "```\n",
    "- `self.position_embedding` is an instance of `nn.Embedding`:\n",
    "  ```python\n",
    "  self.position_embedding = nn.Embedding(self.num_positions, self.embed_dim)\n",
    "  ```\n",
    "  - **Input:** A tensor of indices (positions).\n",
    "  - **Output:** A tensor where each index is mapped to a learnable embedding vector of size `self.embed_dim`.\n",
    "\n",
    "#### Input Shape to `self.position_embedding`:\n",
    "- `self.position_ids`: **[1, self.num_positions]**  \n",
    "  Example: **[1, 196]**\n",
    "\n",
    "#### Output Shape from `self.position_embedding`:\n",
    "- The embedding layer maps each position index to a vector of size `self.embed_dim`.\n",
    "- **Output Shape:** [1, self.num_positions, self.embed_dim]  \n",
    "  Example: **[1, 196, 768]**\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Broadcasting Positional Embeddings**\n",
    "The positional embeddings are added to the patch embeddings:\n",
    "```python\n",
    "embeddings = embeddings + self.position_embedding(self.position_ids)\n",
    "```\n",
    "- `embeddings` has shape **[Batch_Size, self.num_patches, self.embed_dim]**.  \n",
    "  Example: **[8, 196, 768]**\n",
    "\n",
    "- `self.position_embedding(self.position_ids)` has shape **[1, self.num_positions, self.embed_dim]**.  \n",
    "  Example: **[1, 196, 768]**\n",
    "\n",
    "- **Broadcasting:** The positional embeddings are broadcasted along the batch dimension:\n",
    "  - **Broadcasted Shape:** [Batch_Size, self.num_positions, self.embed_dim]  \n",
    "    Example: **[8, 196, 768]**\n",
    "\n",
    "---\n",
    "\n",
    "### Final Output After Addition:\n",
    "The addition combines the patch embeddings and positional embeddings:\n",
    "- **Output Shape:** [Batch_Size, self.num_positions, self.embed_dim]  \n",
    "  Example: **[8, 196, 768]**\n",
    "\n",
    "---\n",
    "\n",
    "### Summary Table of Shape Changes:\n",
    "| Step                                       | Shape Transformation                | Example Shape       |\n",
    "|--------------------------------------------|--------------------------------------|---------------------|\n",
    "| **`self.position_ids` Initialization**     | `[self.num_positions] → [1, self.num_positions]` | `[196] → [1, 196]` |\n",
    "| **Positional Embedding Lookup**            | `[1, self.num_positions] → [1, self.num_positions, self.embed_dim]` | `[1, 196] → [1, 196, 768]` |\n",
    "| **Broadcasting with `embeddings`**         | `[1, self.num_positions, self.embed_dim] → [Batch_Size, self.num_positions, self.embed_dim]` | `[1, 196, 768] → [8, 196, 768]` |\n",
    "| **Final Addition**                         | `[Batch_Size, self.num_positions, self.embed_dim] + [Batch_Size, self.num_positions, self.embed_dim]` | `[8, 196, 768]` |\n",
    "\n",
    "Let me know if further clarification is needed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Normalization will happen on activations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# layer normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "problem: covariate shift.\n",
    "\n",
    "if the input of layer changes, the ouput will change too; if input chhanges alot thaen output will chnage drastically too and as loss during training is dependednt on output so the loss will chnage drasticallly too; then gradients will change alot; resuliting in darastic update in weights too... resulting oscillations meaning slow traoning..\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Covariate Shift Problem in Neural Networks: Detailed Breakdown\n",
    "\n",
    "#### **What Happens During Covariate Shift?**\n",
    "1. **Input Distribution Changes:**  \n",
    "   When the distribution of inputs to a layer shifts significantly during training (due to parameter updates in previous layers), the outputs of the layer will also shift.\n",
    "\n",
    "2. **Impact on Output and Loss:**  \n",
    "   Since the loss function depends on the final output of the network, drastic changes in layer outputs will cause the loss to fluctuate significantly. This can destabilize training.\n",
    "\n",
    "3. **Effect on Gradients:**  \n",
    "   - The gradient of the loss with respect to weights (\\(\\nabla W\\)) is computed using the chain rule.  \n",
    "   - Large changes in loss result in large gradients.\n",
    "   - Large gradients lead to drastic weight updates during backpropagation.\n",
    "\n",
    "4. **Resulting Oscillations:**  \n",
    "   - Drastic weight updates cause oscillations in the loss landscape.\n",
    "   - Oscillations prevent the model from settling into an optimal point, slowing down convergence and potentially leading to divergence.\n",
    "\n",
    "---\n",
    "\n",
    "#### **How Normalization Helps Mitigate Covariate Shift**\n",
    "\n",
    "1. **Stabilizes Input Distributions:**\n",
    "   - Techniques like **Batch Normalization** normalize the inputs to each layer, ensuring they have a consistent mean and variance.\n",
    "   - This reduces the shift in the input distribution, keeping layer outputs more stable.\n",
    "\n",
    "2. **Smooths Loss Changes:**\n",
    "   - With stable outputs, the loss function changes more gradually during training.\n",
    "   - Gradients are smaller and more consistent, preventing drastic weight updates.\n",
    "\n",
    "3. **Improves Gradient Flow:**\n",
    "   - Normalization ensures that gradients are neither too small (vanishing) nor too large (exploding), facilitating smoother updates.\n",
    "\n",
    "4. **Speeds Up Training:**\n",
    "   - By reducing oscillations and stabilizing the training process, normalization allows the network to converge faster.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Example with Batch Normalization**\n",
    "\n",
    "Consider the forward pass of a layer:\n",
    "\\[\n",
    "z = Wx + b\n",
    "\\]\n",
    "\n",
    "- Without normalization:\n",
    "  - If \\(x\\) (input) changes significantly during training, \\(z\\) (output) will also change drastically.\n",
    "  - This affects the next layer's input, creating a cascading effect of instability.\n",
    "\n",
    "- With **Batch Normalization**:\n",
    "  - Before applying activation, normalize \\(z\\):\n",
    "    \\[\n",
    "    \\hat{z} = \\frac{z - \\mu}{\\sigma}\n",
    "    \\]\n",
    "    where \\(\\mu\\) and \\(\\sigma\\) are the batch mean and standard deviation.\n",
    "  - This ensures that the distribution of \\(z\\) remains consistent, reducing the impact of covariate shift.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Summary**\n",
    "- **Problem:** Covariate shift causes instability in training due to drastic changes in loss and gradients, leading to oscillations and slow convergence.\n",
    "- **Solution:** Normalization techniques (e.g., Batch Norm) stabilize input distributions, smooth loss changes, and ensure consistent gradient updates, accelerating training and improving model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## batch normlaization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "its first slolution was batch normalization intriduced in \"Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift\".\n",
    "\n",
    "\n",
    "In batch normalization, each feature is normalized across batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not necessarily! While normalization techniques like **Batch Normalization** (BN) adjust the distribution of activations to have a mean of 0 and variance of 1 during the intermediate normalization step, the final output of the layer might not strictly maintain these properties. Here's why:\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Steps in Batch Normalization**\n",
    "1. **Normalize the Input:**\n",
    "   - For each feature in a batch, the activations are normalized:\n",
    "     \\[\n",
    "     \\hat{x} = \\frac{x - \\mu}{\\sigma}\n",
    "     \\]\n",
    "     where:\n",
    "     - \\(x\\): Activation value.\n",
    "     - \\(\\mu\\): Mean of the batch.\n",
    "     - \\(\\sigma\\): Standard deviation of the batch.\n",
    "   - This step ensures the normalized output has a mean of 0 and variance of 1.\n",
    "\n",
    "2. **Scale and Shift (Learnable Parameters):**\n",
    "   - After normalization, BN applies a learnable linear transformation:\n",
    "     \\[\n",
    "     y = \\gamma \\hat{x} + \\beta\n",
    "     \\]\n",
    "     where:\n",
    "     - \\(\\gamma\\): Scaling parameter.\n",
    "     - \\(\\beta\\): Shifting parameter.\n",
    "   - These parameters allow the network to recover any desired distribution, not just mean = 0 and variance = 1.\n",
    "\n",
    "---\n",
    "\n",
    "### **Why Allow Scaling and Shifting?**\n",
    "1. **Preserve Model Expressiveness:**\n",
    "   - Strictly normalizing to mean = 0 and variance = 1 can limit the model's ability to learn diverse patterns.\n",
    "   - The learnable parameters \\(\\gamma\\) and \\(\\beta\\) allow the model to adapt to different distributions if needed.\n",
    "\n",
    "2. **Flexibility for Downstream Layers:**\n",
    "   - Downstream layers might require inputs with specific distributions. By adjusting \\(\\gamma\\) and \\(\\beta\\), the network can meet these requirements.\n",
    "\n",
    "---\n",
    "\n",
    "### **Final Output Distribution**\n",
    "- **Intermediate Distribution (After Normalization):**  \n",
    "  \\[\n",
    "  \\hat{x} \\sim \\text{mean = 0, variance = 1}\n",
    "  \\]\n",
    "- **Final Distribution (After Scaling and Shifting):**  \n",
    "  \\[\n",
    "  y \\sim \\text{mean = }\\beta, \\, \\text{variance = }\\gamma^2\n",
    "  \\]\n",
    "  The mean and variance depend on the learned parameters \\(\\beta\\) and \\(\\gamma\\).\n",
    "\n",
    "---\n",
    "\n",
    "### **Summary**\n",
    "The **intermediate distribution** during Batch Normalization will have a mean of 0 and variance of 1. However, the **final distribution** can have any mean and variance, as determined by the learnable parameters \\(\\gamma\\) (scaling) and \\(\\beta\\) (shifting). This flexibility is essential for enabling the model to learn effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **distribution of activations** refers to the statistical properties of the outputs (activations) of a layer in a neural network. Specifically, it describes how the values of the activations are spread across a range, often characterized by metrics like **mean**, **variance**, **standard deviation**, or the shape of the distribution (e.g., Gaussian, uniform).\n",
    "\n",
    "---\n",
    "\n",
    "### **Breaking It Down**\n",
    "\n",
    "1. **Activations:**\n",
    "   - When an input is passed through a neural network layer (e.g., a fully connected layer or a convolutional layer), the output values for each neuron are called **activations**.\n",
    "   - For example, in a single layer, if the input is \\(x\\), and the layer applies a linear transformation followed by a non-linearity (\\(f\\)), the activation would be:\n",
    "     \\[\n",
    "     a = f(Wx + b)\n",
    "     \\]\n",
    "     where \\(W\\) and \\(b\\) are the layer's weights and biases.\n",
    "\n",
    "2. **Distribution:**\n",
    "   - For a batch of data, the activations from a layer will form a set of values.\n",
    "   - The **distribution** of these activations describes the range and frequency of the values, such as whether they are centered around zero, spread out widely, or clustered in a specific range.\n",
    "\n",
    "---\n",
    "\n",
    "### **Why Is the Distribution of Activations Important?**\n",
    "\n",
    "1. **Impact on Training Stability:**\n",
    "   - If the activations have a very large variance (spread out too much), it can lead to exploding gradients.\n",
    "   - If the activations are too small or close to zero, it can cause vanishing gradients.\n",
    "   - These issues can slow down training or make the model fail to converge.\n",
    "\n",
    "2. **Covariate Shift:**\n",
    "   - As the model trains, the distribution of activations in one layer can change due to updates in the weights of previous layers. This causes a mismatch in what subsequent layers expect, leading to slower training.\n",
    "\n",
    "3. **Normalization Helps:**\n",
    "   - Techniques like **Batch Normalization** normalize the activations to have a more consistent distribution (e.g., mean ≈ 0, variance ≈ 1) during training. This makes training more stable and efficient.\n",
    "\n",
    "---\n",
    "\n",
    "### **Example of Activation Distribution**\n",
    "Suppose a neural network layer outputs activations for a batch of data. Here are two possible distributions:\n",
    "1. **Without Normalization:**\n",
    "   - Mean: 50\n",
    "   - Variance: 200\n",
    "   - The values might range widely (e.g., from 0 to 100).\n",
    "\n",
    "2. **With Normalization:**\n",
    "   - Mean: 0\n",
    "   - Variance: 1\n",
    "   - The values are centered around zero and have a more controlled spread.\n",
    "\n",
    "---\n",
    "\n",
    "### **Visualizing Distribution of Activations**\n",
    "A histogram or density plot can represent the distribution:\n",
    "- The x-axis shows the range of activation values.\n",
    "- The y-axis shows the frequency of activations in that range.\n",
    "\n",
    "---\n",
    "\n",
    "### **Summary**\n",
    "The **distribution of activations** is the statistical representation of the outputs from a neural network layer. Controlling this distribution (e.g., using normalization) is critical for stable and efficient training. It helps prevent issues like vanishing or exploding gradients and ensures that each layer receives inputs with a predictable range and scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, in **Batch Normalization (BN)**, each feature is normalized **independently across the batch**. Here's a detailed explanation:\n",
    "\n",
    "---\n",
    "\n",
    "### **What Does \"Each Feature\" Mean?**\n",
    "- Suppose the input to a layer has a shape of \\([B, C, H, W]\\), where:\n",
    "  - \\(B\\): Batch size.\n",
    "  - \\(C\\): Number of channels/features.\n",
    "  - \\(H, W\\): Height and width (for image data).\n",
    "\n",
    "- BN normalizes each feature channel (\\(C\\)) across the batch (\\(B\\)) and spatial dimensions (\\(H \\times W\\), if applicable). \n",
    "\n",
    "---\n",
    "\n",
    "### **Steps in Batch Normalization**\n",
    "\n",
    "1. **Compute Mean and Variance Across Batch:**\n",
    "   For a given feature channel \\(c\\), compute:\n",
    "   \\[\n",
    "   \\mu_c = \\frac{1}{B \\cdot H \\cdot W} \\sum_{b=1}^B \\sum_{h=1}^H \\sum_{w=1}^W x_{bchw}\n",
    "   \\]\n",
    "   \\[\n",
    "   \\sigma_c^2 = \\frac{1}{B \\cdot H \\cdot W} \\sum_{b=1}^B \\sum_{h=1}^H \\sum_{w=1}^W (x_{bchw} - \\mu_c)^2\n",
    "   \\]\n",
    "   This computes the mean (\\(\\mu_c\\)) and variance (\\(\\sigma_c^2\\)) of the feature \\(c\\) across the batch and spatial dimensions.\n",
    "\n",
    "2. **Normalize the Feature:**\n",
    "   Each activation in the batch for the feature channel \\(c\\) is normalized as:\n",
    "   \\[\n",
    "   \\hat{x}_{bchw} = \\frac{x_{bchw} - \\mu_c}{\\sqrt{\\sigma_c^2 + \\epsilon}}\n",
    "   \\]\n",
    "   where:\n",
    "   - \\(\\epsilon\\): A small constant to prevent division by zero.\n",
    "\n",
    "3. **Scale and Shift:**\n",
    "   After normalization, a learnable scaling (\\(\\gamma_c\\)) and shifting (\\(\\beta_c\\)) parameter are applied to allow the network to recover flexibility:\n",
    "   \\[\n",
    "   y_{bchw} = \\gamma_c \\hat{x}_{bchw} + \\beta_c\n",
    "   \\]\n",
    "\n",
    "---\n",
    "\n",
    "### **Why Normalize Across Batch?**\n",
    "- The distribution of inputs to a layer can vary significantly across different batches during training, leading to **covariate shift**.\n",
    "- Normalizing across the batch ensures that the input to each layer has a consistent distribution, which helps stabilize training.\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Notes**\n",
    "1. **Per Feature Channel:**\n",
    "   - Mean (\\(\\mu_c\\)) and variance (\\(\\sigma_c^2\\)) are computed **independently** for each feature channel \\(c\\).\n",
    "\n",
    "2. **Across Batch and Spatial Dimensions:**\n",
    "   - BN aggregates statistics across all samples in the batch and optionally across spatial dimensions (if working with images).\n",
    "\n",
    "3. **Output Shape:**\n",
    "   - The output shape remains the same as the input \\([B, C, H, W]\\), but the values are normalized and adjusted by \\(\\gamma\\) and \\(\\beta\\).\n",
    "\n",
    "---\n",
    "\n",
    "### **Summary**\n",
    "In Batch Normalization:\n",
    "- Each feature (channel) is normalized independently across the batch and spatial dimensions.\n",
    "- This reduces covariate shift and ensures more stable and faster training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "after batch normalization, activation swill be in gasussian distribution of mean 0 and var 1\n",
    "resulting in stable training\n",
    "\n",
    "\n",
    "THe problem:\n",
    "As each mean and std dev is calculated along batch dimension, so qw need large batch to have good results. i.e. if a batch A two classes, its mean will be differnet than the batch B who contains 2 other classes.. i.e. smal batch wont be able to cover diversity of data.. it is important for each batch to cover whole data otherwise covariance shift stays becaise mean is changing alot; hence larger batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are absolutely correct in identifying the challenge with **Batch Normalization** (BN) and its reliance on batch statistics like mean and variance. Let's break it down systematically:\n",
    "\n",
    "---\n",
    "\n",
    "### **What Batch Normalization Does**\n",
    "1. **Normalization**: It normalizes the activations across the batch dimension:\n",
    "   \\[\n",
    "   \\hat{x} = \\frac{x - \\mu_B}{\\sigma_B}\n",
    "   \\]\n",
    "   where:\n",
    "   - \\( \\mu_B \\): Mean of the batch activations.\n",
    "   - \\( \\sigma_B \\): Standard deviation of the batch activations.\n",
    "\n",
    "2. **Re-scaling and Re-shifting**: After normalization, it applies learnable parameters \\( \\gamma \\) (scale) and \\( \\beta \\) (shift) to maintain representational power:\n",
    "   \\[\n",
    "   y = \\gamma \\hat{x} + \\beta\n",
    "   \\]\n",
    "\n",
    "---\n",
    "\n",
    "### **The Problem with Small Batches**\n",
    "- **Batch Mean and Variance Dependence**:\n",
    "  - The mean (\\( \\mu_B \\)) and variance (\\( \\sigma_B^2 \\)) are computed across the batch.\n",
    "  - If the batch size is small, these statistics can vary significantly depending on the data composition of the batch (e.g., two different classes in different batches will have different means).\n",
    "  - This causes instability in training because the normalization parameters change drastically between batches.\n",
    "\n",
    "- **Covariate Shift Persists**:\n",
    "  - If the batch statistics are inconsistent, the normalization does not effectively address covariate shift. The output distribution remains unstable.\n",
    "\n",
    "---\n",
    "\n",
    "### **Why Larger Batches Help**\n",
    "1. **Better Estimation of Statistics**:\n",
    "   - With larger batches, the mean and variance better represent the entire dataset, making them less sensitive to the specific composition of the batch.\n",
    "\n",
    "2. **Reduced Variance in Batch Statistics**:\n",
    "   - Larger batch sizes smooth out variations in the data, resulting in more stable normalization parameters.\n",
    "\n",
    "---\n",
    "\n",
    "### **What Happens with Small Batches?**\n",
    "- Small batches fail to capture the diversity of the dataset.\n",
    "- This can lead to:\n",
    "  - Poor generalization.\n",
    "  - Unstable training due to highly variable batch statistics.\n",
    "  - Reduced effectiveness of normalization, as the statistics no longer represent the overall data distribution.\n",
    "\n",
    "---\n",
    "\n",
    "### **Solutions to the Small Batch Problem**\n",
    "1. **Layer Normalization**:\n",
    "   - Instead of normalizing across the batch dimension, it normalizes across the features of each sample.\n",
    "   - Effective for tasks with small batch sizes, like NLP or small datasets.\n",
    "\n",
    "2. **Group Normalization**:\n",
    "   - Divides the features into groups and normalizes within each group.\n",
    "   - Works well with small batch sizes and tasks like object detection.\n",
    "\n",
    "3. **Instance Normalization**:\n",
    "   - Normalizes each sample independently, often used in style transfer tasks.\n",
    "\n",
    "4. **Using Larger Batches**:\n",
    "   - When possible, increase the batch size to stabilize batch statistics.\n",
    "\n",
    "5. **Batch Renormalization**:\n",
    "   - Adjusts batch normalization during training by adding constraints to stabilize mean and variance.\n",
    "\n",
    "---\n",
    "\n",
    "### **Conclusion**\n",
    "Your explanation is accurate: **batch size significantly impacts the effectiveness of Batch Normalization**. Small batches fail to generalize the statistics, leading to poor normalization and training instability. To mitigate this, techniques like Layer Norm, Group Norm, or larger batch sizes are often used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Batch Normalization (BN)** and **Layer Normalization (LN)** are both techniques to stabilize and accelerate the training of neural networks by normalizing activations. However, they differ in **how** and **where** normalization is applied, and thus, their effectiveness varies depending on the task and architecture.\n",
    "\n",
    "---\n",
    "\n",
    "### **Batch Normalization (BN)**\n",
    "\n",
    "1. **How It Works:**\n",
    "   - Normalizes the activations for each feature across the **batch dimension**.\n",
    "   - For a given mini-batch, computes the mean (\\( \\mu_B \\)) and variance (\\( \\sigma_B^2 \\)) of activations for each feature, then normalizes them:\n",
    "     \\[\n",
    "     \\hat{x}_{i} = \\frac{x_i - \\mu_B}{\\sqrt{\\sigma_B^2 + \\epsilon}}\n",
    "     \\]\n",
    "   - Applies learnable scale (\\( \\gamma \\)) and shift (\\( \\beta \\)):\n",
    "     \\[\n",
    "     y_i = \\gamma \\hat{x}_{i} + \\beta\n",
    "     \\]\n",
    "\n",
    "2. **Key Features:**\n",
    "   - **Normalization Scope**: Across the batch dimension.\n",
    "   - **Usage**: Typically used in **Convolutional Neural Networks (CNNs)** and large-batch settings.\n",
    "   - **Dependency**: Sensitive to batch size; small batches can result in unstable statistics.\n",
    "\n",
    "3. **Advantages:**\n",
    "   - Addresses **internal covariate shift** by normalizing inputs to each layer.\n",
    "   - Speeds up convergence and allows for higher learning rates.\n",
    "   - Reduces sensitivity to initialization.\n",
    "\n",
    "4. **Disadvantages:**\n",
    "   - Performance degrades with **small batch sizes** due to noisy statistics.\n",
    "   - Requires maintaining batch statistics during inference, which can be complex.\n",
    "\n",
    "5. **Where It Excels:**\n",
    "   - Vision tasks (e.g., CNNs).\n",
    "   - Architectures with large batch sizes.\n",
    "\n",
    "---\n",
    "\n",
    "### **Layer Normalization (LN)**\n",
    "\n",
    "1. **How It Works:**\n",
    "   - Normalizes the activations **within each sample** across the **feature dimension**.\n",
    "   - For each sample, computes the mean (\\( \\mu \\)) and variance (\\( \\sigma^2 \\)) of all features, then normalizes them:\n",
    "     \\[\n",
    "     \\hat{x}_{i} = \\frac{x_i - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}}\n",
    "     \\]\n",
    "   - Applies learnable scale (\\( \\gamma \\)) and shift (\\( \\beta \\)):\n",
    "     \\[\n",
    "     y_i = \\gamma \\hat{x}_{i} + \\beta\n",
    "     \\]\n",
    "\n",
    "2. **Key Features:**\n",
    "   - **Normalization Scope**: Across the feature dimension within a single sample.\n",
    "   - **Usage**: Often used in **Recurrent Neural Networks (RNNs)** and Transformers.\n",
    "   - **Dependency**: Independent of batch size, making it suitable for small-batch or single-sample settings.\n",
    "\n",
    "3. **Advantages:**\n",
    "   - Effective with small batches or single-sample inputs.\n",
    "   - No dependence on batch statistics; more stable for tasks like NLP or reinforcement learning.\n",
    "   - Reduces reliance on batch diversity.\n",
    "\n",
    "4. **Disadvantages:**\n",
    "   - May not perform as well as BN in large-batch settings, particularly for vision tasks.\n",
    "\n",
    "5. **Where It Excels:**\n",
    "   - Sequential tasks (e.g., NLP, RNNs).\n",
    "   - Transformer architectures (e.g., BERT, GPT).\n",
    "   - Small-batch or online learning scenarios.\n",
    "\n",
    "---\n",
    "\n",
    "### **Comparison:**\n",
    "\n",
    "| **Aspect**               | **Batch Normalization (BN)**                      | **Layer Normalization (LN)**                  |\n",
    "|---------------------------|---------------------------------------------------|-----------------------------------------------|\n",
    "| **Normalization Axis**    | Across batch dimension (for each feature)         | Across feature dimension (within each sample) |\n",
    "| **Batch Dependency**      | Yes                                               | No                                            |\n",
    "| **Use Case**              | CNNs, large-batch settings                        | RNNs, Transformers, small-batch settings      |\n",
    "| **Sensitivity to Batch Size** | High (requires larger batches)                  | Low                                           |\n",
    "| **Computational Cost**    | Requires maintaining batch statistics             | Slightly simpler, no batch stats needed       |\n",
    "| **Applications**          | Vision tasks, deep CNNs                          | NLP, Transformers, small or dynamic batches   |\n",
    "\n",
    "---\n",
    "\n",
    "### **When to Use Which?**\n",
    "- **Batch Normalization**:\n",
    "  - Use in **vision tasks** or other settings with **large batches**.\n",
    "  - Great for architectures like CNNs where activations are highly dependent on the batch.\n",
    "\n",
    "- **Layer Normalization**:\n",
    "  - Use in **NLP tasks**, **Transformers**, or **sequential data**.\n",
    "  - Works well when batch sizes are small or inconsistent, or when training requires handling single inputs.\n",
    "\n",
    "---\n",
    "\n",
    "### **Summary**\n",
    "Both BN and LN are powerful normalization techniques with complementary strengths. While BN is a go-to for vision tasks with large batches, LN is ideal for NLP, Transformers, and scenarios where batch sizes are small or diverse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the context of **Layer Normalization**, \"each sample\" refers to a single input example in a batch of data.\n",
    "\n",
    "Let’s break this down with an example:\n",
    "\n",
    "---\n",
    "\n",
    "### **Scenario:**\n",
    "You have a batch of data with shape \\([B, F]\\), where:\n",
    "- \\(B\\) = batch size (number of samples in the batch).\n",
    "- \\(F\\) = number of features per sample.\n",
    "\n",
    "For instance, in NLP or Transformers:\n",
    "- \\(B = 4\\) (batch size of 4 sentences).\n",
    "- \\(F = 512\\) (each sentence is represented by a 512-dimensional feature vector).\n",
    "\n",
    "---\n",
    "\n",
    "### **Layer Normalization Scope:**\n",
    "- **Each sample**: Refers to a single data point (or input example) in the batch.\n",
    "  - Example: For batch index \\(i\\), the sample is a feature vector of shape \\([1, F]\\).\n",
    "\n",
    "- **Normalization Across Feature Dimension**:\n",
    "  - For a given sample, compute the **mean** (\\(\\mu\\)) and **variance** (\\(\\sigma^2\\)) of its \\(F\\) features.\n",
    "  - Normalize the features of this sample:\n",
    "    \\[\n",
    "    \\hat{x}_{i, j} = \\frac{x_{i, j} - \\mu_i}{\\sqrt{\\sigma_i^2 + \\epsilon}}\n",
    "    \\]\n",
    "    Where:\n",
    "    - \\(x_{i, j}\\) is the \\(j\\)-th feature of the \\(i\\)-th sample.\n",
    "    - \\(\\mu_i\\) and \\(\\sigma_i^2\\) are computed over the \\(F\\) features of the \\(i\\)-th sample.\n",
    "\n",
    "---\n",
    "\n",
    "### **Example:**\n",
    "\n",
    "1. **Input Batch**:\n",
    "   A batch of 4 samples, each with 3 features:\n",
    "   \\[\n",
    "   \\text{Input: } \n",
    "   \\begin{bmatrix}\n",
    "   1.0 & 2.0 & 3.0 \\\\\n",
    "   4.0 & 5.0 & 6.0 \\\\\n",
    "   7.0 & 8.0 & 9.0 \\\\\n",
    "   10.0 & 11.0 & 12.0\n",
    "   \\end{bmatrix}\n",
    "   \\]\n",
    "\n",
    "   Shape: \\([4, 3]\\)\n",
    "\n",
    "2. **Normalize Each Sample (Row):**\n",
    "   For the first sample (\\([1.0, 2.0, 3.0]\\)):\n",
    "   - Compute mean: \\(\\mu = \\frac{1.0 + 2.0 + 3.0}{3} = 2.0\\)\n",
    "   - Compute variance: \\(\\sigma^2 = \\frac{(1.0 - 2.0)^2 + (2.0 - 2.0)^2 + (3.0 - 2.0)^2}{3} = 0.666\\)\n",
    "   - Normalize each feature:\n",
    "     \\[\n",
    "     \\hat{x}_{1, j} = \\frac{x_{1, j} - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}}\n",
    "     \\]\n",
    "\n",
    "   Repeat for all samples.\n",
    "\n",
    "3. **Output**:\n",
    "   The normalized batch has the same shape (\\([4, 3]\\)).\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Point:**\n",
    "\"Each sample\" means a **single row** (data point) in the input batch, and **Layer Normalization** computes statistics (\\(\\mu, \\sigma^2\\)) only across the **features of that row**, independent of the other rows (samples) in the batch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let me explain the key differences between batch normalization and layer normalization, which are both important normalization techniques in deep learning.\n",
    "\n",
    "Batch Normalization:\n",
    "- Normalizes across the batch dimension - looks at the same feature across different samples in a batch\n",
    "- Computes mean and variance for each feature independently across the batch\n",
    "- Highly dependent on batch size - performs differently during training vs inference due to batch statistics\n",
    "- Originally designed for CNNs but widely used in many architectures\n",
    "- Great for reducing internal covariate shift and allowing higher learning rates\n",
    "- Formula: For each feature, normalize using μ_batch and σ_batch calculated across the batch\n",
    "\n",
    "Layer Normalization:\n",
    "- Normalizes across the feature dimension - looks at all features for each sample independently\n",
    "- Computes mean and variance for each sample across all features in a layer\n",
    "- Independent of batch size - behaves consistently during training and inference\n",
    "- Originally designed for RNNs but works well in transformers and other architectures\n",
    "- Particularly effective for sequential models and attention mechanisms\n",
    "- Formula: For each sample, normalize using μ_layer and σ_layer calculated across features\n",
    "\n",
    "Key practical differences:\n",
    "1. Batch dependence: BatchNorm needs a decent batch size to work well, while LayerNorm works with any batch size including 1\n",
    "\n",
    "2. Training vs Inference: BatchNorm typically uses running statistics during inference while LayerNorm behaves identically\n",
    "\n",
    "3. Feature independence: BatchNorm assumes features are independent while LayerNorm allows for feature dependencies\n",
    "\n",
    "4. Common use cases:\n",
    "   - BatchNorm: Often preferred for CNNs and feedforward networks\n",
    "   - LayerNorm: Standard choice for transformers and RNNs\n",
    "\n",
    "Would you like me to elaborate on any of these aspects or explain specific implementation details?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Instance Normalization vs Layer Normalization**\n",
    "\n",
    "Both **Instance Normalization (IN)** and **Layer Normalization (LN)** are techniques used to normalize inputs, but they differ in **where and how** normalization is applied.\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Instance Normalization (IN)**\n",
    "\n",
    "#### **Definition**:\n",
    "- Normalizes across the **spatial dimensions** (height and width) of each channel for **each sample** in the batch.  \n",
    "- Typically used in tasks like **style transfer** and **image generation**.\n",
    "\n",
    "#### **How it Works**:\n",
    "- For an input of shape \\([B, C, H, W]\\):\n",
    "  - \\(B\\): Batch size.\n",
    "  - \\(C\\): Number of channels.\n",
    "  - \\(H\\): Height.\n",
    "  - \\(W\\): Width.\n",
    "- IN computes the **mean** (\\(\\mu\\)) and **variance** (\\(\\sigma^2\\)) for each channel of each sample (i.e., over \\(H\\) and \\(W\\)):\n",
    "  \\[\n",
    "  \\mu_{b,c} = \\frac{1}{H \\cdot W} \\sum_{h=1}^{H} \\sum_{w=1}^{W} x_{b,c,h,w}\n",
    "  \\]\n",
    "  \\[\n",
    "  \\sigma_{b,c}^2 = \\frac{1}{H \\cdot W} \\sum_{h=1}^{H} \\sum_{w=1}^{W} (x_{b,c,h,w} - \\mu_{b,c})^2\n",
    "  \\]\n",
    "- Normalize each spatial location within a channel:\n",
    "  \\[\n",
    "  \\hat{x}_{b,c,h,w} = \\frac{x_{b,c,h,w} - \\mu_{b,c}}{\\sqrt{\\sigma_{b,c}^2 + \\epsilon}}\n",
    "  \\]\n",
    "\n",
    "#### **Key Characteristics**:\n",
    "- **Per-sample, per-channel normalization**: Normalization is independent for each channel and sample.\n",
    "- **Use case**: Instance Normalization is commonly used in tasks like style transfer because it removes instance-specific contrast and illumination variations.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Layer Normalization (LN)**\n",
    "\n",
    "#### **Definition**:\n",
    "- Normalizes across the **feature dimensions** for each sample in the batch.  \n",
    "- Typically used in tasks like **transformers**, **NLP**, and **RNNs**.\n",
    "\n",
    "#### **How it Works**:\n",
    "- For an input of shape \\([B, F]\\) or \\([B, C, H, W]\\):\n",
    "  - LN computes the **mean** (\\(\\mu\\)) and **variance** (\\(\\sigma^2\\)) over the **feature dimensions** for each sample (e.g., \\(F\\), \\(C \\times H \\times W\\)):\n",
    "  \\[\n",
    "  \\mu_{b} = \\frac{1}{F} \\sum_{f=1}^{F} x_{b,f}\n",
    "  \\]\n",
    "  \\[\n",
    "  \\sigma_{b}^2 = \\frac{1}{F} \\sum_{f=1}^{F} (x_{b,f} - \\mu_{b})^2\n",
    "  \\]\n",
    "- Normalize each feature for the sample:\n",
    "  \\[\n",
    "  \\hat{x}_{b,f} = \\frac{x_{b,f} - \\mu_{b}}{\\sqrt{\\sigma_{b}^2 + \\epsilon}}\n",
    "  \\]\n",
    "\n",
    "#### **Key Characteristics**:\n",
    "- **Per-sample normalization**: Normalization happens independently for each sample but across all its features.\n",
    "- **Use case**: Layer Normalization is widely used in transformer models and RNNs because it does not depend on batch size.\n",
    "\n",
    "---\n",
    "\n",
    "### **Comparison Table**\n",
    "\n",
    "| **Aspect**              | **Instance Normalization**                                   | **Layer Normalization**                                    |\n",
    "|--------------------------|------------------------------------------------------------|-----------------------------------------------------------|\n",
    "| **Normalization Scope** | Across spatial dimensions (\\(H, W\\)) within each channel.   | Across all feature dimensions (\\(C \\times H \\times W\\)).  |\n",
    "| **Statistics Computed** | Mean and variance for each channel of each sample.          | Mean and variance for all features of each sample.        |\n",
    "| **Input Shape**          | \\([B, C, H, W]\\)                                           | \\([B, F]\\) or \\([B, C, H, W]\\).                          |\n",
    "| **Use Case**             | Style transfer, image generation.                          | NLP, transformers, RNNs.                                 |\n",
    "| **Batch Size Dependency**| Independent of batch size.                                 | Independent of batch size.                                |\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Differences**:\n",
    "1. **Dimension of Normalization**:\n",
    "   - IN focuses on spatial dimensions (\\(H, W\\)) for each channel.\n",
    "   - LN focuses on feature dimensions (\\(C \\times H \\times W\\)) for each sample.\n",
    "\n",
    "2. **Use Case**:\n",
    "   - IN is better suited for tasks where spatial contrast matters, like style transfer.\n",
    "   - LN is better suited for tasks where sequential or feature-based data is important, like NLP.\n",
    "\n",
    "---\n",
    "\n",
    "Would you like to see a visual representation or code implementation of these concepts?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Instance Normalization (IN)**: \n",
    "\n",
    "- **Normalization Scope**: Instance Normalization normalizes the spatial dimensions (\\(H\\) and \\(W\\)) of each **channel** in an input, for **each sample independently**.  \n",
    "- **Key Idea**: Normalize each channel's activations for each individual sample, ignoring the batch and other channels.\n",
    "\n",
    "---\n",
    "\n",
    "### **Normalization Formula for IN**:\n",
    "\n",
    "Given an input tensor \\(x \\in \\mathbb{R}^{B \\times C \\times H \\times W}\\):  \n",
    "- \\(B\\): Batch size  \n",
    "- \\(C\\): Number of channels  \n",
    "- \\(H, W\\): Height and width of the spatial dimensions  \n",
    "\n",
    "Instance Normalization calculates:\n",
    "\\[\n",
    "\\mu_{b,c} = \\frac{1}{H \\cdot W} \\sum_{h=1}^{H} \\sum_{w=1}^{W} x_{b,c,h,w}\n",
    "\\]\n",
    "\\[\n",
    "\\sigma_{b,c}^2 = \\frac{1}{H \\cdot W} \\sum_{h=1}^{H} \\sum_{w=1}^{W} (x_{b,c,h,w} - \\mu_{b,c})^2\n",
    "\\]\n",
    "\n",
    "Then, normalize each spatial location within a channel:\n",
    "\\[\n",
    "\\hat{x}_{b,c,h,w} = \\frac{x_{b,c,h,w} - \\mu_{b,c}}{\\sqrt{\\sigma_{b,c}^2 + \\epsilon}}\n",
    "\\]\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Characteristics**:\n",
    "\n",
    "1. **Independent Per Sample**: \n",
    "   - Each sample is normalized separately.  \n",
    "   - No dependency on the rest of the batch.  \n",
    "\n",
    "2. **Channel-Specific Normalization**:\n",
    "   - Within each sample, each channel is normalized independently across its spatial dimensions (\\(H\\), \\(W\\)).\n",
    "\n",
    "3. **Spatial Context Removal**:\n",
    "   - Normalization over \\(H\\) and \\(W\\) removes variations like contrast and brightness, making it ideal for tasks like **style transfer**.\n",
    "\n",
    "---\n",
    "\n",
    "### **Comparison with Batch and Layer Normalization**:\n",
    "\n",
    "| **Aspect**                  | **Batch Normalization (BN)**                | **Layer Normalization (LN)**                 | **Instance Normalization (IN)**               |\n",
    "|-----------------------------|---------------------------------------------|---------------------------------------------|----------------------------------------------|\n",
    "| **Normalization Scope**     | Across batch for each feature (dimension).  | Across all features for each input sample.  | Across spatial dimensions for each channel.  |\n",
    "| **Input Dependency**        | Depends on the batch.                       | Independent of the batch.                   | Independent of the batch.                    |\n",
    "| **Feature Dependency**      | Normalizes each feature across batch.       | Normalizes all features for each sample.    | Normalizes spatial dimensions of each channel. |\n",
    "| **Common Use Case**         | Training large-scale models.                | NLP, transformers, small batch sizes.       | Style transfer, image generation.            |\n",
    "\n",
    "---\n",
    "\n",
    "Would you like further clarification or code examples?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***In Layer Normalization (LN), the mean and variance are calculated across all channels of a single example, treating the entire feature map as a unit. In contrast, Instance Normalization (IN) computes a separate mean and variance for each channel independently within the same example.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\n",
    "        \n",
    "        batch - one feature across batch dimension\n",
    "\n",
    "\n",
    "        layer - all activations of single example at a time(all channels)\n",
    "        \n",
    "        \n",
    "        instance - all activations of single channel at a time(all features)  .so rgb mean  3 mean and varince calculations for each channel\n",
    "\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exactly! Here's the breakdown of the difference:\n",
    "\n",
    "---\n",
    "\n",
    "### **Layer Normalization (LN):**\n",
    "- **Scope of Mean and Variance Calculation**:  \n",
    "  - Mean and variance are computed **once for the entire feature set** (all channels) of each input sample.\n",
    "  - If the input is \\(x \\in \\mathbb{R}^{B \\times C \\times H \\times W}\\), LN treats \\(C \\times H \\times W\\) as a single unit for each sample \\(b\\).\n",
    "  \n",
    "- **Key Idea**:  \n",
    "  Normalize all features **together** for each sample, ignoring spatial or channel separations.  \n",
    "\n",
    "- **Result**:  \n",
    "  - A single mean and variance are used for normalizing all the features of a sample.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Instance Normalization (IN):**\n",
    "- **Scope of Mean and Variance Calculation**:  \n",
    "  - Mean and variance are computed **separately for each channel** of each input sample.\n",
    "  - For \\(x \\in \\mathbb{R}^{B \\times C \\times H \\times W}\\), IN calculates a mean and variance for each \\(H \\times W\\) region within each channel \\(c\\), for every sample \\(b\\).  \n",
    "\n",
    "- **Key Idea**:  \n",
    "  Normalize each channel **independently** across its spatial dimensions (\\(H, W\\)) for each sample.  \n",
    "\n",
    "- **Result**:  \n",
    "  - Each channel has its own mean and variance, leading to normalization that is more localized.\n",
    "\n",
    "---\n",
    "\n",
    "### **Comparison Table**:\n",
    "\n",
    "| **Aspect**                   | **Layer Normalization (LN)**                         | **Instance Normalization (IN)**                   |\n",
    "|------------------------------|----------------------------------------------------|-------------------------------------------------|\n",
    "| **Normalization Scope**      | Across all features of a sample (\\(C \\times H \\times W\\)). | Across spatial dimensions (\\(H, W\\)) of each channel. |\n",
    "| **Mean/Variance Per Sample** | Single mean and variance for all features.          | Separate mean and variance for each channel.    |\n",
    "| **Key Use Case**             | NLP, Transformers, small batch sizes.              | Style transfer, image generation.              |\n",
    "\n",
    "---\n",
    "\n",
    "Would you like to dive deeper into specific use cases or see a visual illustration of this concept?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### siglip encoder\n",
    "![alt text](vision_lang_model_11_vit_encoder_layers_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "siglip encoder is madeup of mulitplelayers transformation layer and architecture more and less looks like this... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](vision_lang_model_10_vit_encoder_layers.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a transformer is a sequence to sequence model that converts sequence of embeddings into contextualize embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **transformer** is a type of neural network architecture primarily used for sequence-to-sequence tasks, and its main function is to transform a sequence of input embeddings into a sequence of contextualized embeddings. Here's a breakdown of how it works:\n",
    "\n",
    "1. **Input Embeddings**:  \n",
    "   Each element of the input sequence (e.g., a word or a token) is represented as an embedding, which is a fixed-size vector capturing its semantic meaning.\n",
    "\n",
    "2. **Positional Encoding**:  \n",
    "   Since transformers do not have a built-in sense of order like recurrent models, positional encodings are added to the input embeddings to inject information about the position of each token in the sequence.\n",
    "\n",
    "3. **Self-Attention Mechanism**:  \n",
    "   The self-attention mechanism allows the model to weigh the importance of each token in the sequence relative to every other token. This is how the model captures contextual relationships.\n",
    "\n",
    "4. **Feedforward Layers**:  \n",
    "   After applying self-attention, a feedforward network refines the representation of each token independently.\n",
    "\n",
    "5. **Layer Stacking**:  \n",
    "   Multiple layers of self-attention and feedforward networks are stacked to deepen the model's ability to capture complex dependencies.\n",
    "\n",
    "6. **Contextualized Embeddings**:  \n",
    "   After processing through the transformer layers, each token's embedding becomes **contextualized**, meaning its representation is influenced by the surrounding tokens in the sequence.\n",
    "\n",
    "This process is what enables transformers to excel in tasks like machine translation, text summarization, and more, where understanding the context of each token is critical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in transformer, each token catpures info is based on all previous tokens but in vision transformer it is captures info about all other patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***SigMLP***\n",
    "adds parameters and non-linearity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In a **Transformer encoder**, **FFN** stands for **Feedforward Neural Network**. It is a fully connected network applied independently to each position in the sequence after the self-attention mechanism.\n",
    "In a **Transformer encoder**, **FFN** stands for **Feedforward Neural Network**. It is a fully connected network applied independently to each position in the sequence after the self-attention mechanism.\n",
    "\n",
    "### Structure of the FFN:\n",
    "The FFN typically consists of:\n",
    "1. **Two linear transformations**:  \n",
    "   - The first transformation projects the input to a higher-dimensional space.\n",
    "   - The second transformation projects it back to the original dimensionality.\n",
    "   \n",
    "2. **Activation Function**:  \n",
    "   A non-linear activation function (usually **ReLU**) is applied after the first linear transformation.\n",
    "\n",
    "3. **Dropout**:  \n",
    "   Dropout may be applied between layers to prevent overfitting.\n",
    "\n",
    "### Formula:\n",
    "Given an input vector \\( x \\), the FFN can be expressed as:\n",
    "\\[\n",
    "\\text{FFN}(x) = \\text{ReLU}(xW_1 + b_1)W_2 + b_2\n",
    "\\]\n",
    "- \\( W_1 \\) and \\( W_2 \\): Weight matrices for the linear transformations.\n",
    "- \\( b_1 \\) and \\( b_2 \\): Bias vectors.\n",
    "- \\( \\text{ReLU} \\): Rectified Linear Unit activation function.\n",
    "\n",
    "### Role in the Transformer Encoder:\n",
    "1. **Non-linearity**:  \n",
    "   The FFN introduces non-linear transformations to enhance the model's expressiveness.\n",
    "\n",
    "2. **Token Independence**:  \n",
    "   Unlike self-attention, which considers relationships between tokens, the FFN processes each token independently. This helps refine the token's representation after it has been contextualized by the self-attention mechanism.\n",
    "\n",
    "3. **Dimensionality Expansion**:  \n",
    "   The intermediate layer's higher dimensionality (e.g., 2048 in the original transformer paper) allows the model to learn richer features before reducing back to the original size.\n",
    "\n",
    "### Overall Flow in Transformer Encoder:\n",
    "1. Input embeddings go through the **self-attention mechanism**.\n",
    "2. The output of self-attention is passed through the **FFN** for further refinement.\n",
    "3. Residual connections and layer normalization are applied around both the self-attention and FFN blocks. \n",
    "\n",
    "This combination makes the transformer encoder highly effective at learning complex relationships in sequences.\n",
    "### Structure of the FFN:\n",
    "The FFN typically consists of:\n",
    "1. **Two linear transformations**:  \n",
    "   - The first transformation projects the input to a higher-dimensional space.\n",
    "   - The second transformation projects it back to the original dimensionality.\n",
    "   \n",
    "2. **Activation Function**:  \n",
    "   A non-linear activation function (usually **ReLU**) is applied after the first linear transformation.\n",
    "\n",
    "3. **Dropout**:  \n",
    "   Dropout may be applied between layers to prevent overfitting.\n",
    "\n",
    "### Formula:\n",
    "Given an input vector \\( x \\), the FFN can be expressed as:\n",
    "\\[\n",
    "\\text{FFN}(x) = \\text{ReLU}(xW_1 + b_1)W_2 + b_2\n",
    "\\]\n",
    "- \\( W_1 \\) and \\( W_2 \\): Weight matrices for the linear transformations.\n",
    "- \\( b_1 \\) and \\( b_2 \\): Bias vectors.\n",
    "- \\( \\text{ReLU} \\): Rectified Linear Unit activation function.\n",
    "\n",
    "### Role in the Transformer Encoder:\n",
    "1. **Non-linearity**:  \n",
    "   The FFN introduces non-linear transformations to enhance the model's expressiveness.\n",
    "\n",
    "2. **Token Independence**:  \n",
    "   Unlike self-attention, which considers relationships between tokens, the FFN processes each token independently. This helps refine the token's representation after it has been contextualized by the self-attention mechanism.\n",
    "\n",
    "3. **Dimensionality Expansion**:  \n",
    "   The intermediate layer's higher dimensionality (e.g., 2048 in the original transformer paper) allows the model to learn richer features before reducing back to the original size.\n",
    "\n",
    "### Overall Flow in Transformer Encoder:\n",
    "1. Input embeddings go through the **self-attention mechanism**.\n",
    "2. The output of self-attention is passed through the **FFN** for further refinement.\n",
    "3. Residual connections and layer normalization are applied around both the self-attention and FFN blocks. \n",
    "\n",
    "This combination makes the transformer encoder highly effective at learning complex relationships in sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In a **Transformer encoder**, **FFN** stands for **Feedforward Neural Network**. It is a fully connected network applied independently to each position in the sequence after the self-attention mechanism.\n",
    "\n",
    "### Structure of the FFN:\n",
    "The FFN typically consists of:\n",
    "1. **Two linear transformations**:  \n",
    "   - The first transformation projects the input to a higher-dimensional space.\n",
    "   - The second transformation projects it back to the original dimensionality.\n",
    "   \n",
    "2. **Activation Function**:  \n",
    "   A non-linear activation function (usually **ReLU**) is applied after the first linear transformation.\n",
    "\n",
    "3. **Dropout**:  \n",
    "   Dropout may be applied between layers to prevent overfitting.\n",
    "\n",
    "### Formula:\n",
    "Given an input vector \\( x \\), the FFN can be expressed as:\n",
    "\\[\n",
    "\\text{FFN}(x) = \\text{ReLU}(xW_1 + b_1)W_2 + b_2\n",
    "\\]\n",
    "- \\( W_1 \\) and \\( W_2 \\): Weight matrices for the linear transformations.\n",
    "- \\( b_1 \\) and \\( b_2 \\): Bias vectors.\n",
    "- \\( \\text{ReLU} \\): Rectified Linear Unit activation function.\n",
    "\n",
    "### Role in the Transformer Encoder:\n",
    "1. **Non-linearity**:  \n",
    "   The FFN introduces non-linear transformations to enhance the model's expressiveness.\n",
    "\n",
    "2. **Token Independence**:  \n",
    "   Unlike self-attention, which considers relationships between tokens, the FFN processes each token independently. This helps refine the token's representation after it has been contextualized by the self-attention mechanism.\n",
    "\n",
    "3. **Dimensionality Expansion**:  \n",
    "   The intermediate layer's higher dimensionality (e.g., 2048 in the original transformer paper) allows the model to learn richer features before reducing back to the original size.\n",
    "\n",
    "### Overall Flow in Transformer Encoder:\n",
    "1. Input embeddings go through the **self-attention mechanism**.\n",
    "2. The output of self-attention is passed through the **FFN** for further refinement.\n",
    "3. Residual connections and layer normalization are applied around both the self-attention and FFN blocks. \n",
    "\n",
    "This combination makes the transformer encoder highly effective at learning complex relationships in sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, exactly! The **Feedforward Neural Network (FFN)** in the Transformer encoder adds **non-linearity** and **trainable parameters**, which are crucial for the model's expressiveness and learning capacity. Here's how:\n",
    "\n",
    "### 1. **Non-linearity**:\n",
    "- The **ReLU activation** (or other activation functions) in the FFN introduces **non-linear transformations**, allowing the model to learn complex patterns and relationships in the data.\n",
    "- Without non-linearity, the model would only be able to learn linear transformations, which severely limits its capacity to model intricate dependencies.\n",
    "\n",
    "### 2. **Parameters**:\n",
    "The FFN adds trainable parameters through the weight matrices \\( W_1 \\) and \\( W_2 \\) and the bias vectors \\( b_1 \\) and \\( b_2 \\):\n",
    "- **First linear layer**: Expands the dimensionality (e.g., from 512 to 2048 in the original Transformer).\n",
    "- **Second linear layer**: Projects the representation back to the original dimensionality (e.g., from 2048 to 512).\n",
    "- These layers contribute a significant portion of the model's trainable parameters, especially since they operate on a per-token basis.\n",
    "\n",
    "### Why FFN Matters:\n",
    "- **Refinement of Representations**: The FFN enhances token embeddings by applying additional transformations after self-attention, helping the model learn richer, more complex features.\n",
    "- **Parameter Capacity**: By adding more parameters, the FFN increases the model's ability to capture and store information, making it more expressive.\n",
    "\n",
    "Together with self-attention, the FFN ensures that the Transformer encoder can model both contextual relationships (via self-attention) and individual token features (via FFN)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
