{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **to get total number of patches in 3x3 image is divide by size of one patch i.e. 1x1**\n",
    "\n",
    "(3/1)**2 = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, exactly! In short, if the **image size** and **patch size** are square and the patch size divides the image size evenly, you can calculate the total number of patches by:\n",
    "\n",
    "\\[\n",
    "\\text{Total patches} = \\left( \\frac{\\text{Image size}}{\\text{Patch size}} \\right)^2\n",
    "\\]\n",
    "\n",
    "For your example:\n",
    "\n",
    "\\[\n",
    "\\text{Total patches} = \\left( \\frac{3}{1} \\right)^2 = 3^2 = 9\n",
    "\\]\n",
    "\n",
    "This shortcut works because the patches are square and cover the image without overlap or remainder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a Vision Transformer (ViT), the total number of patches in an image is calculated by dividing the image's dimensions by the dimensions of a single patch.\n",
    "\n",
    "For your example:\n",
    "\n",
    "- **Image size**: \\( 3 \\times 3 \\)\n",
    "- **Patch size**: \\( 1 \\times 1 \\)\n",
    "\n",
    "To calculate the total number of patches:\n",
    "\n",
    "1. **Divide the image dimensions by the patch dimensions**:\n",
    "   \\[\n",
    "   \\text{Number of patches in width (W)} = \\frac{\\text{Image width}}{\\text{Patch width}} = \\frac{3}{1} = 3\n",
    "   \\]\n",
    "   \\[\n",
    "   \\text{Number of patches in height (H)} = \\frac{\\text{Image height}}{\\text{Patch height}} = \\frac{3}{1} = 3\n",
    "   \\]\n",
    "\n",
    "2. **Multiply the number of patches along each dimension**:\n",
    "   \\[\n",
    "   \\text{Total number of patches} = \\text{W} \\times \\text{H} = 3 \\times 3 = 9\n",
    "   \\]\n",
    "\n",
    "### Explanation\n",
    "Each \\( 1 \\times 1 \\) patch is a distinct region of the \\( 3 \\times 3 \\) image. Since the image is perfectly divisible by the patch size, there are \\( 9 \\) patches in total."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LEARNING_RATE = 1e-4\n",
    "NUM_CLASSES = 10\n",
    "PATCH_SIZE = 4\n",
    "IMG_SIZE = 28\n",
    "IN_CHANNELS = 1\n",
    "NUM_HEADS = 8\n",
    "DROPOUT = 0.001\n",
    "HIDDEN_DIM = 768  # mlp head dimension\n",
    "ADAM_WEIGHT_DECAY = 0\n",
    "ADAM_BETAS = (0.9, 0.99)\n",
    "ACTIVATION = \"gelu\"\n",
    "NUM_ENCODERS = 4\n",
    "EMBED_DIM = PATCH_SIZE * PATCH_SIZE * IN_CHANNELS  # 16   #patch's  W*H*CHEANNELs\n",
    "NUM_PATCHES = (IMG_SIZE // PATCH_SIZE) ** 2  # 49\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "![alt text](<vit _01.png>)\n",
    "\n",
    "\n",
    "\n",
    "cls taken has a positional embedding too\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cls_token.shape=torch.Size([512, 1, 16])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([512, 50, 16])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, embed_dim, patch_size, num_patches, dropout, in_channels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.patcher = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=in_channels,\n",
    "                out_channels=embed_dim,\n",
    "                kernel_size=patch_size,\n",
    "                stride=patch_size,\n",
    "            ),\n",
    "            nn.Flatten(2),\n",
    "        )\n",
    "\n",
    "        self.cls_token = nn.Parameter(\n",
    "            torch.randn(size=(1, in_channels, embed_dim)), requires_grad=True\n",
    "        )  # (batch_size, inn_channels, output_channeks)\n",
    "        self.position_embeddings = nn.Parameter(\n",
    "            torch.randn(size=(1, num_patches + 1, embed_dim)), requires_grad=True\n",
    "        )  # +1 because CLS token is also acting as a batch.. each image patch/token has a positional embeddings correspondent\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        cls_token = self.cls_token.expand(\n",
    "            x.shape[0], -1, -1\n",
    "        )  # -1 mean keep dimensoins,,dont chnage them\n",
    "        print(f\"{cls_token.shape=}\")\n",
    "        x = self.patcher(x).permute(0, 2, 1)\n",
    "        # now add left cls token to it\n",
    "        x = torch.cat([cls_token, x], dim=1)\n",
    "        x = self.position_embeddings + x\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = PatchEmbedding(\n",
    "    embed_dim=EMBED_DIM,\n",
    "    patch_size=PATCH_SIZE,\n",
    "    num_patches=NUM_PATCHES,\n",
    "    dropout=DROPOUT,\n",
    "    in_channels=IN_CHANNELS,\n",
    ").to(device)\n",
    "\n",
    "x = torch.randn(512, 1, 28, 28)  # dummy input # b, c, h, w\n",
    "\n",
    "model(x).shape  # (batch, NUM_PATCHES:49  + 1 for cls, EMBED_DIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **[CLS] token** in Vision Transformers (ViT) is a special learnable embedding introduced to aggregate information across all patches for tasks like classification. Here's how it works:\n",
    "\n",
    "### Purpose of the [CLS] Token\n",
    "- The **[CLS] token** stands for \"classification token.\"\n",
    "- It acts as a placeholder for the global representation of the image.\n",
    "- After the transformer layers process the input patches, the final state of the **[CLS] token** serves as the input to a classification head for downstream tasks (e.g., predicting image labels).\n",
    "\n",
    "---\n",
    "\n",
    "### Process in ViT\n",
    "1. **Input Patches**:\n",
    "   - The image is divided into fixed-size patches, flattened, and embedded into a sequence of vectors (patch embeddings).\n",
    "   - Positional embeddings are added to retain spatial information.\n",
    "\n",
    "2. **[CLS] Token Initialization**:\n",
    "   - A learnable vector (the [CLS] token) is prepended to the sequence of patch embeddings.\n",
    "   - The input to the transformer becomes: \\([ \\text{[CLS]}, P_1, P_2, \\dots, P_N ]\\), where \\(P_i\\) are the patch embeddings.\n",
    "\n",
    "3. **Transformer Processing**:\n",
    "   - The sequence, including the [CLS] token, is passed through multiple transformer layers.\n",
    "   - Each layer updates the representation of the [CLS] token based on interactions with the patch embeddings.\n",
    "\n",
    "4. **Output**:\n",
    "   - After the final transformer layer, the [CLS] token contains a global representation of the image.\n",
    "   - This representation is passed to a **classification head** (typically an MLP) for the final prediction.\n",
    "\n",
    "---\n",
    "\n",
    "### Why Use the [CLS] Token?\n",
    "- **Global Aggregation**: The [CLS] token aggregates information from all patches, acting as a summary of the image.\n",
    "- **Simplicity**: Using a single token for classification avoids the need for pooling operations like average or max pooling.\n",
    "- **Flexibility**: The same mechanism can be adapted for tasks other than classification by modifying the downstream head.\n",
    "\n",
    "---\n",
    "\n",
    "### Example Workflow in ViT\n",
    "1. **Input Image**: \\( 224 \\times 224 \\) image divided into \\( 16 \\times 16 \\) patches → \\( 14 \\times 14 = 196 \\) patches.\n",
    "2. **Sequence**: [CLS] + 196 patch embeddings → \\( 197 \\) tokens.\n",
    "3. **Output**: Final state of [CLS] → passed to the classification head.\n",
    "\n",
    "The [CLS] token is central to the ViT architecture, enabling efficient global understanding of the input image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, in Vision Transformers (**ViT**), the **prediction is based on the [CLS] token**. Here's how it works in detail:\n",
    "\n",
    "### Why Prediction is Based on [CLS] Token\n",
    "The **[CLS] token** is designed to serve as a global representation of the input image. During the forward pass through the transformer layers, the [CLS] token interacts with all the patch embeddings and gathers information from the entire image.\n",
    "\n",
    "After the final transformer layer:\n",
    "- The [CLS] token contains a summarized feature representation of the image.\n",
    "- This feature vector is then passed through a **classification head** (e.g., a fully connected layer or MLP) to make the final prediction.\n",
    "\n",
    "---\n",
    "\n",
    "### Step-by-Step Process for Prediction\n",
    "\n",
    "1. **Input Sequence**:\n",
    "   - The input sequence is \\([ \\text{[CLS]}, P_1, P_2, \\dots, P_N ]\\), where \\(P_i\\) are the patch embeddings, and [CLS] is the classification token.\n",
    "\n",
    "2. **Transformer Layers**:\n",
    "   - The sequence is processed by the transformer, and the [CLS] token is updated at each layer through attention with the patch embeddings.\n",
    "\n",
    "3. **Final [CLS] Token**:\n",
    "   - After the last transformer layer, the [CLS] token contains a high-level feature vector summarizing the entire image.\n",
    "\n",
    "4. **Classification Head**:\n",
    "   - The [CLS] token's final representation is passed through a classification head (e.g., a linear layer with softmax) to predict the image class.\n",
    "\n",
    "---\n",
    "\n",
    "### Example\n",
    "For a classification task:\n",
    "1. **Input Image**: \\(224 \\times 224\\) image divided into \\(16 \\times 16\\) patches → \\(14 \\times 14 = 196\\) patches.\n",
    "2. **Input Sequence**: [CLS] + 196 patch embeddings → \\(197\\) tokens.\n",
    "3. **Output of Transformer**: Final [CLS] token → \\(D\\)-dimensional vector (e.g., \\(D = 768\\)).\n",
    "4. **Classification**:\n",
    "   \\[\n",
    "   \\text{Prediction} = \\text{Softmax}(\\text{Linear}(\\text{[CLS]}))\n",
    "   \\]\n",
    "\n",
    "---\n",
    "\n",
    "### Key Benefits of Using [CLS] Token for Prediction\n",
    "- **Global Context**: It gathers information from all patches through attention.\n",
    "- **Simplicity**: Eliminates the need for additional pooling layers like average or max pooling.\n",
    "- **Flexibility**: Can be adapted for tasks like segmentation or object detection by adding task-specific heads.\n",
    "\n",
    "Thus, in ViT, the final prediction for classification is **entirely based on the [CLS] token's representation**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zohaib/anaconda3/lib/python3.12/site-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cls_token.shape=torch.Size([512, 1, 16])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([512, 10])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ViT(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_patches,\n",
    "        img_size,\n",
    "        num_classes,\n",
    "        patch_size,\n",
    "        embed_dim,\n",
    "        num_encoders,\n",
    "        num_head,\n",
    "        hidden_dim,\n",
    "        dropout,\n",
    "        activation,\n",
    "        in_channels,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embeddings_block = PatchEmbedding(\n",
    "            embed_dim=embed_dim,\n",
    "            patch_size=patch_size,\n",
    "            num_patches=num_patches,\n",
    "            dropout=dropout,\n",
    "            in_channels=in_channels,\n",
    "        )\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim,\n",
    "            nhead=num_head,\n",
    "            dropout=dropout,\n",
    "            activation=activation,\n",
    "            batch_first=True,  # batch will come first i.e. (B, C,H,W)\n",
    "            norm_first=True,  # norm before attentiona and mlp lauer\n",
    "        )\n",
    "        self.encoder_blocks = nn.TransformerEncoder(\n",
    "            encoder_layer, num_layers=num_encoders\n",
    "        )\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(normalized_shape=embed_dim),\n",
    "            nn.Linear(in_features=embed_dim, out_features=num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embeddings_block(x)\n",
    "        x = self.encoder_blocks(x)\n",
    "        x = self.mlp_head(x[:, 0, :])  # only take 0th token: cls token\n",
    "        # we dont classify on whole embedding,,instead we only classify on cls token and we only feed cls token to mlp_head\n",
    "        return x\n",
    "\n",
    "\n",
    "model = ViT(\n",
    "    NUM_PATCHES,\n",
    "    IMG_SIZE,\n",
    "    NUM_CLASSES,\n",
    "    PATCH_SIZE,\n",
    "    EMBED_DIM,\n",
    "    NUM_ENCODERS,\n",
    "    NUM_HEADS,\n",
    "    HIDDEN_DIM,\n",
    "    DROPOUT,\n",
    "    ACTIVATION,\n",
    "    IN_CHANNELS,\n",
    ").to(device)\n",
    "y = torch.randn(512, 1, 28, 28)\n",
    "model(y).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch3d",
   "language": "python",
   "name": "pytorch3d"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
