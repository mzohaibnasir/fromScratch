{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **to get total number of patches in 3x3 image is divide by size of one patch i.e. 1x1**\n",
    "\n",
    "(3/1)**2 = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, exactly! In short, if the **image size** and **patch size** are square and the patch size divides the image size evenly, you can calculate the total number of patches by:\n",
    "\n",
    "\\[\n",
    "\\text{Total patches} = \\left( \\frac{\\text{Image size}}{\\text{Patch size}} \\right)^2\n",
    "\\]\n",
    "\n",
    "For your example:\n",
    "\n",
    "\\[\n",
    "\\text{Total patches} = \\left( \\frac{3}{1} \\right)^2 = 3^2 = 9\n",
    "\\]\n",
    "\n",
    "This shortcut works because the patches are square and cover the image without overlap or remainder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a Vision Transformer (ViT), the total number of patches in an image is calculated by dividing the image's dimensions by the dimensions of a single patch.\n",
    "\n",
    "For your example:\n",
    "\n",
    "- **Image size**: \\( 3 \\times 3 \\)\n",
    "- **Patch size**: \\( 1 \\times 1 \\)\n",
    "\n",
    "To calculate the total number of patches:\n",
    "\n",
    "1. **Divide the image dimensions by the patch dimensions**:\n",
    "   \\[\n",
    "   \\text{Number of patches in width (W)} = \\frac{\\text{Image width}}{\\text{Patch width}} = \\frac{3}{1} = 3\n",
    "   \\]\n",
    "   \\[\n",
    "   \\text{Number of patches in height (H)} = \\frac{\\text{Image height}}{\\text{Patch height}} = \\frac{3}{1} = 3\n",
    "   \\]\n",
    "\n",
    "2. **Multiply the number of patches along each dimension**:\n",
    "   \\[\n",
    "   \\text{Total number of patches} = \\text{W} \\times \\text{H} = 3 \\times 3 = 9\n",
    "   \\]\n",
    "\n",
    "### Explanation\n",
    "Each \\( 1 \\times 1 \\) patch is a distinct region of the \\( 3 \\times 3 \\) image. Since the image is perfectly divisible by the patch size, there are \\( 9 \\) patches in total."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 1e-4\n",
    "NUM_CLASSES = 10\n",
    "PATCH_SIZE = 4\n",
    "IMG_SIZE = 28\n",
    "IN_CHANNELS = 1\n",
    "NUM_HEADS = 8\n",
    "DROPOUT = 0.001\n",
    "HIDDEN_DIM = 768  # mlp head dimension\n",
    "ADAM_WEIGHT_DECAY = 0\n",
    "ADAM_BETAS = (0.9, 0.99)\n",
    "ACTIVATION = \"gelu\"\n",
    "NUM_ENCODERS = 4\n",
    "EMBED_DIM = PATCH_SIZE * PATCH_SIZE * IN_CHANNELS  # 16   #patch's  W*H*CHEANNELs\n",
    "NUM_PATCHES = (IMG_SIZE // PATCH_SIZE) ** 2  # 49\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# device = \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "![alt text](<vit _01.png>)\n",
    "\n",
    "\n",
    "\n",
    "cls taken has a positional embedding too\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512, 50, 16])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, embed_dim, patch_size, num_patches, dropout, in_channels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.patcher = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=in_channels,\n",
    "                out_channels=embed_dim,\n",
    "                kernel_size=patch_size,\n",
    "                stride=patch_size,\n",
    "            ),\n",
    "            nn.Flatten(2),\n",
    "        )\n",
    "\n",
    "        self.cls_token = nn.Parameter(\n",
    "            torch.randn(size=(1, in_channels, embed_dim)), requires_grad=True\n",
    "        )  # (batch_size, inn_channels, output_channeks)\n",
    "        self.position_embeddings = nn.Parameter(\n",
    "            torch.randn(size=(1, num_patches + 1, embed_dim)), requires_grad=True\n",
    "        )  # +1 because CLS token is also acting as a batch.. each image patch/token has a positional embeddings correspondent\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        cls_token = self.cls_token.expand(\n",
    "            x.shape[0], -1, -1\n",
    "        )  # -1 mean keep dimensoins,,dont chnage them\n",
    "        # print(f\"{cls_token.shape=}\")\n",
    "        x = self.patcher(x).permute(0, 2, 1)\n",
    "        # now add left cls token to it\n",
    "        x = torch.cat([cls_token, x], dim=1)\n",
    "        x = self.position_embeddings + x\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = PatchEmbedding(\n",
    "    embed_dim=EMBED_DIM,\n",
    "    patch_size=PATCH_SIZE,\n",
    "    num_patches=NUM_PATCHES,\n",
    "    dropout=DROPOUT,\n",
    "    in_channels=IN_CHANNELS,\n",
    ").to(device)\n",
    "\n",
    "x = torch.randn(512, 1, 28, 28).to(device=device)  # dummy input # b, c, h, w\n",
    "\n",
    "model(x).shape  # (batch, NUM_PATCHES:49  + 1 for cls, EMBED_DIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **[CLS] token** in Vision Transformers (ViT) is a special learnable embedding introduced to aggregate information across all patches for tasks like classification. Here's how it works:\n",
    "\n",
    "### Purpose of the [CLS] Token\n",
    "- The **[CLS] token** stands for \"classification token.\"\n",
    "- It acts as a placeholder for the global representation of the image.\n",
    "- After the transformer layers process the input patches, the final state of the **[CLS] token** serves as the input to a classification head for downstream tasks (e.g., predicting image labels).\n",
    "\n",
    "---\n",
    "\n",
    "### Process in ViT\n",
    "1. **Input Patches**:\n",
    "   - The image is divided into fixed-size patches, flattened, and embedded into a sequence of vectors (patch embeddings).\n",
    "   - Positional embeddings are added to retain spatial information.\n",
    "\n",
    "2. **[CLS] Token Initialization**:\n",
    "   - A learnable vector (the [CLS] token) is prepended to the sequence of patch embeddings.\n",
    "   - The input to the transformer becomes: \\([ \\text{[CLS]}, P_1, P_2, \\dots, P_N ]\\), where \\(P_i\\) are the patch embeddings.\n",
    "\n",
    "3. **Transformer Processing**:\n",
    "   - The sequence, including the [CLS] token, is passed through multiple transformer layers.\n",
    "   - Each layer updates the representation of the [CLS] token based on interactions with the patch embeddings.\n",
    "\n",
    "4. **Output**:\n",
    "   - After the final transformer layer, the [CLS] token contains a global representation of the image.\n",
    "   - This representation is passed to a **classification head** (typically an MLP) for the final prediction.\n",
    "\n",
    "---\n",
    "\n",
    "### Why Use the [CLS] Token?\n",
    "- **Global Aggregation**: The [CLS] token aggregates information from all patches, acting as a summary of the image.\n",
    "- **Simplicity**: Using a single token for classification avoids the need for pooling operations like average or max pooling.\n",
    "- **Flexibility**: The same mechanism can be adapted for tasks other than classification by modifying the downstream head.\n",
    "\n",
    "---\n",
    "\n",
    "### Example Workflow in ViT\n",
    "1. **Input Image**: \\( 224 \\times 224 \\) image divided into \\( 16 \\times 16 \\) patches → \\( 14 \\times 14 = 196 \\) patches.\n",
    "2. **Sequence**: [CLS] + 196 patch embeddings → \\( 197 \\) tokens.\n",
    "3. **Output**: Final state of [CLS] → passed to the classification head.\n",
    "\n",
    "The [CLS] token is central to the ViT architecture, enabling efficient global understanding of the input image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, in Vision Transformers (**ViT**), the **prediction is based on the [CLS] token**. Here's how it works in detail:\n",
    "\n",
    "### Why Prediction is Based on [CLS] Token\n",
    "The **[CLS] token** is designed to serve as a global representation of the input image. During the forward pass through the transformer layers, the [CLS] token interacts with all the patch embeddings and gathers information from the entire image.\n",
    "\n",
    "After the final transformer layer:\n",
    "- The [CLS] token contains a summarized feature representation of the image.\n",
    "- This feature vector is then passed through a **classification head** (e.g., a fully connected layer or MLP) to make the final prediction.\n",
    "\n",
    "---\n",
    "\n",
    "### Step-by-Step Process for Prediction\n",
    "\n",
    "1. **Input Sequence**:\n",
    "   - The input sequence is \\([ \\text{[CLS]}, P_1, P_2, \\dots, P_N ]\\), where \\(P_i\\) are the patch embeddings, and [CLS] is the classification token.\n",
    "\n",
    "2. **Transformer Layers**:\n",
    "   - The sequence is processed by the transformer, and the [CLS] token is updated at each layer through attention with the patch embeddings.\n",
    "\n",
    "3. **Final [CLS] Token**:\n",
    "   - After the last transformer layer, the [CLS] token contains a high-level feature vector summarizing the entire image.\n",
    "\n",
    "4. **Classification Head**:\n",
    "   - The [CLS] token's final representation is passed through a classification head (e.g., a linear layer with softmax) to predict the image class.\n",
    "\n",
    "---\n",
    "\n",
    "### Example\n",
    "For a classification task:\n",
    "1. **Input Image**: \\(224 \\times 224\\) image divided into \\(16 \\times 16\\) patches → \\(14 \\times 14 = 196\\) patches.\n",
    "2. **Input Sequence**: [CLS] + 196 patch embeddings → \\(197\\) tokens.\n",
    "3. **Output of Transformer**: Final [CLS] token → \\(D\\)-dimensional vector (e.g., \\(D = 768\\)).\n",
    "4. **Classification**:\n",
    "   \\[\n",
    "   \\text{Prediction} = \\text{Softmax}(\\text{Linear}(\\text{[CLS]}))\n",
    "   \\]\n",
    "\n",
    "---\n",
    "\n",
    "### Key Benefits of Using [CLS] Token for Prediction\n",
    "- **Global Context**: It gathers information from all patches through attention.\n",
    "- **Simplicity**: Eliminates the need for additional pooling layers like average or max pooling.\n",
    "- **Flexibility**: Can be adapted for tasks like segmentation or object detection by adding task-specific heads.\n",
    "\n",
    "Thus, in ViT, the final prediction for classification is **entirely based on the [CLS] token's representation**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt-text](vit_02.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zohaib/anaconda3/lib/python3.12/site-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([512, 10])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ViT(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_patches,\n",
    "        img_size,\n",
    "        num_classes,\n",
    "        patch_size,\n",
    "        embed_dim,\n",
    "        num_encoders,\n",
    "        num_head,\n",
    "        hidden_dim,\n",
    "        dropout,\n",
    "        activation,\n",
    "        in_channels,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embeddings_block = PatchEmbedding(\n",
    "            embed_dim=embed_dim,\n",
    "            patch_size=patch_size,\n",
    "            num_patches=num_patches,\n",
    "            dropout=dropout,\n",
    "            in_channels=in_channels,\n",
    "        )\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim,\n",
    "            nhead=num_head,\n",
    "            dropout=dropout,\n",
    "            activation=activation,\n",
    "            batch_first=True,  # batch will come first i.e. (B, C,H,W)\n",
    "            norm_first=True,  # norm before attentiona and mlp lauer\n",
    "        )\n",
    "        self.encoder_blocks = nn.TransformerEncoder(\n",
    "            encoder_layer, num_layers=num_encoders\n",
    "        )\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(normalized_shape=embed_dim),\n",
    "            nn.Linear(in_features=embed_dim, out_features=num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embeddings_block(x)\n",
    "        x = self.encoder_blocks(x)\n",
    "        x = self.mlp_head(x[:, 0, :])  # only take 0th token: cls token\n",
    "        # we dont classify on whole embedding,,instead we only classify on cls token and we only feed cls token to mlp_head\n",
    "        return x\n",
    "\n",
    "\n",
    "model = ViT(\n",
    "    NUM_PATCHES,\n",
    "    IMG_SIZE,\n",
    "    NUM_CLASSES,\n",
    "    PATCH_SIZE,\n",
    "    EMBED_DIM,\n",
    "    NUM_ENCODERS,\n",
    "    NUM_HEADS,\n",
    "    HIDDEN_DIM,\n",
    "    DROPOUT,\n",
    "    ACTIVATION,\n",
    "    IN_CHANNELS,\n",
    ").to(device)\n",
    "y = torch.randn(512, 1, 28, 28).to(device)\n",
    "model(y).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "from torch import optim\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import timeit\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42\n",
    "EPOCHS = 40\n",
    "BATCH_SIZE = 512\n",
    "\n",
    "\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "torch.cuda.manual_seed(RANDOM_SEED)\n",
    "torch.cuda.manual_seed_all(RANDOM_SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code sets a consistent random seed across different libraries and configurations to ensure reproducibility in a machine learning experiment. Let’s break it down:\n",
    "\n",
    "---\n",
    "\n",
    "### 1. **Variable Definitions**\n",
    "- **`RANDOM_SEED = 42`**  \n",
    "  This is the seed value for random number generation. The choice of `42` is arbitrary and commonly used as a placeholder for deterministic experiments.\n",
    "\n",
    "- **`EPOCHS = 40` and `BATCH_SIZE = 512`**  \n",
    "  These variables define the number of training iterations (epochs) and the size of mini-batches for gradient descent, respectively. They are unrelated to the random seed but are typical settings in ML training loops.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Random Seed Initialization**\n",
    "- **`random.seed(RANDOM_SEED)`**  \n",
    "  Sets the seed for Python's built-in `random` module, which is used for generating pseudo-random numbers.\n",
    "\n",
    "- **`np.random.seed(RANDOM_SEED)`**  \n",
    "  Sets the seed for NumPy's random number generator. NumPy is often used for array manipulations and operations requiring randomness.\n",
    "\n",
    "- **`torch.manual_seed(RANDOM_SEED)`**  \n",
    "  Sets the seed for PyTorch's CPU random number generator, ensuring consistent results for operations involving randomness on the CPU.\n",
    "\n",
    "- **`torch.cuda.manual_seed(RANDOM_SEED)`**  \n",
    "  Sets the seed for PyTorch's CUDA random number generator, ensuring consistent results for GPU computations involving randomness.\n",
    "\n",
    "- **`torch.cuda.manual_seed_all(RANDOM_SEED)`**  \n",
    "  Similar to `manual_seed`, but ensures that all GPUs (if multiple are used) use the same seed for randomness.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **PyTorch Backend Configuration**\n",
    "- **`torch.backends.cudnn.deterministic = True`**  \n",
    "  Ensures deterministic behavior for operations using cuDNN (NVIDIA's library for deep neural networks). When set to `True`, cuDNN uses deterministic algorithms for reproducibility. This may slow down training slightly.\n",
    "\n",
    "- **`torch.backends.cudnn.benchmark = False`**  \n",
    "  Disables cuDNN's benchmarking feature, which typically selects the fastest algorithm for a given operation. Turning this off ensures that the same algorithm is used for consistent results, at the cost of potentially reduced performance.\n",
    "\n",
    "---\n",
    "\n",
    "### **Purpose and Impact**\n",
    "By setting all these configurations, the code ensures that:\n",
    "1. The randomness in data shuffling, weight initialization, and other stochastic operations is consistent across runs.\n",
    "2. Results from training and evaluation remain reproducible, which is critical for debugging, testing, and sharing research.\n",
    "\n",
    "However, full reproducibility might still be affected by factors like:\n",
    "- Differences in hardware or software versions.\n",
    "- Non-deterministic operations in third-party libraries. \n",
    "\n",
    "Using this setup minimizes variability as much as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"digit-recognizer/train.csv\")\n",
    "test_df = pd.read_csv(\"digit-recognizer/test.csv\")\n",
    "submission_df = pd.read_csv(\"digit-recognizer/sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel0</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel774</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  \\\n",
       "0      1       0       0       0       0       0       0       0       0   \n",
       "1      0       0       0       0       0       0       0       0       0   \n",
       "2      1       0       0       0       0       0       0       0       0   \n",
       "3      4       0       0       0       0       0       0       0       0   \n",
       "4      0       0       0       0       0       0       0       0       0   \n",
       "\n",
       "   pixel8  ...  pixel774  pixel775  pixel776  pixel777  pixel778  pixel779  \\\n",
       "0       0  ...         0         0         0         0         0         0   \n",
       "1       0  ...         0         0         0         0         0         0   \n",
       "2       0  ...         0         0         0         0         0         0   \n",
       "3       0  ...         0         0         0         0         0         0   \n",
       "4       0  ...         0         0         0         0         0         0   \n",
       "\n",
       "   pixel780  pixel781  pixel782  pixel783  \n",
       "0         0         0         0         0  \n",
       "1         0         0         0         0  \n",
       "2         0         0         0         0  \n",
       "3         0         0         0         0  \n",
       "4         0         0         0         0  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pixel0</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>pixel9</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel774</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 784 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  \\\n",
       "0       0       0       0       0       0       0       0       0       0   \n",
       "1       0       0       0       0       0       0       0       0       0   \n",
       "2       0       0       0       0       0       0       0       0       0   \n",
       "3       0       0       0       0       0       0       0       0       0   \n",
       "4       0       0       0       0       0       0       0       0       0   \n",
       "\n",
       "   pixel9  ...  pixel774  pixel775  pixel776  pixel777  pixel778  pixel779  \\\n",
       "0       0  ...         0         0         0         0         0         0   \n",
       "1       0  ...         0         0         0         0         0         0   \n",
       "2       0  ...         0         0         0         0         0         0   \n",
       "3       0  ...         0         0         0         0         0         0   \n",
       "4       0  ...         0         0         0         0         0         0   \n",
       "\n",
       "   pixel780  pixel781  pixel782  pixel783  \n",
       "0         0         0         0         0  \n",
       "1         0         0         0         0  \n",
       "2         0         0         0         0  \n",
       "3         0         0         0         0  \n",
       "4         0         0         0         0  \n",
       "\n",
       "[5 rows x 784 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ImageId</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ImageId  Label\n",
       "0        1      0\n",
       "1        2      0\n",
       "2        3      0\n",
       "3        4      0\n",
       "4        5      0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df = train_test_split(\n",
    "    train_df, test_size=0.1, random_state=RANDOM_SEED, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTTrainDataset(Dataset):\n",
    "    def __init__(self, images, labels, indices):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.indices = indices\n",
    "        self.transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.ToPILImage(),\n",
    "                transforms.RandomRotation(15),  # 15 degrees\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.5], [0.5]),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"return item at a specific index\"\"\"\n",
    "        image = self.images[idx].reshape((28, 28)).astype(np.uint8)\n",
    "        label = self.labels[idx]\n",
    "        index = self.indices[idx]\n",
    "        image = self.transform(image)\n",
    "\n",
    "        return {\"image\": image, \"label\": label, \"index\": index}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTValDataset(Dataset):\n",
    "    def __init__(self, images, labels, indices):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.indices = indices\n",
    "        self.transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.5], [0.5]),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"return item at a specific index\"\"\"\n",
    "        image = self.images[idx].reshape((28, 28)).astype(np.uint8)\n",
    "        label = self.labels[idx]\n",
    "        index = self.indices[idx]\n",
    "        image = self.transform(image)\n",
    "\n",
    "        return {\"image\": image, \"label\": label, \"index\": index}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTSubmitDataset(Dataset):\n",
    "    def __init__(self, images, indices):\n",
    "        self.images = images\n",
    "        self.indices = indices\n",
    "        self.transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.5], [0.5]),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"return item at a specific index\"\"\"\n",
    "        image = self.images[idx].reshape((28, 28)).astype(np.uint8)\n",
    "        index = self.indices[idx]\n",
    "        image = self.transform(image)\n",
    "\n",
    "        return {\"image\": image, \"index\": index}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(train_dataset)=37800\n",
      "------------------------------\n",
      "len(val_dataset)=4200\n",
      "------------------------------\n",
      "len(test_dataset)=28000\n",
      "------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "f, axarr = plt.subplots(1, 3)\n",
    "\n",
    "\n",
    "train_dataset = MNISTTrainDataset(\n",
    "    train_df.iloc[:, 1:].values.astype(np.uint8),  # first col is labels\n",
    "    train_df.iloc[:, 0].values,\n",
    "    train_df.index.values,\n",
    ")\n",
    "print(f\"{len(train_dataset)=}\")\n",
    "axarr[0].imshow(train_dataset[0][\"image\"].squeeze(), cmap=\"gray\")\n",
    "axarr[0].set_title(\"Train Image\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "\n",
    "val_dataset = MNISTValDataset(\n",
    "    val_df.iloc[:, 1:].values.astype(np.uint8),  # first col is labels\n",
    "    val_df.iloc[:, 0].values,\n",
    "    val_df.index.values,\n",
    ")\n",
    "print(f\"{len(val_dataset)=}\")\n",
    "axarr[1].imshow(val_dataset[0][\"image\"].squeeze(), cmap=\"gray\")\n",
    "axarr[1].set_title(\"Val Image\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "\n",
    "test_dataset = MNISTSubmitDataset(\n",
    "    test_df.values.astype(np.uint8),  # first col is labels\n",
    "    test_df.index.values,\n",
    ")\n",
    "\n",
    "print(f\"{len(test_dataset)=}\")\n",
    "axarr[2].imshow(test_dataset[0][\"image\"].squeeze(), cmap=\"gray\")\n",
    "axarr[2].set_title(\"Test Image\")\n",
    "print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5533"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.index[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True\n",
    ")\n",
    "\n",
    "\n",
    "val_dataloader = DataLoader(dataset=val_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "test_dataloader = DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device='cuda'\n"
     ]
    }
   ],
   "source": [
    "print(f\"{device=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device='cuda'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 74/74 [00:12<00:00,  5.94it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 23.33it/s]\n",
      "  2%|▎         | 1/40 [00:12<08:24, 12.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "train loss epoch 1: 2.3019\n",
      "valid loss epoch 1: 2.1703\n",
      "train_accuracy epoch 1: 0.1284\n",
      "val_accuracy epoch 1: 0.2057\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 74/74 [00:12<00:00,  6.05it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 23.07it/s]\n",
      "  5%|▌         | 2/40 [00:25<08:07, 12.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "train loss epoch 2: 1.8182\n",
      "valid loss epoch 2: 1.6061\n",
      "train_accuracy epoch 2: 0.3697\n",
      "val_accuracy epoch 2: 0.4931\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 74/74 [00:12<00:00,  6.01it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 23.56it/s]\n",
      "  8%|▊         | 3/40 [00:38<07:54, 12.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "train loss epoch 3: 1.5355\n",
      "valid loss epoch 3: 1.3810\n",
      "train_accuracy epoch 3: 0.5222\n",
      "val_accuracy epoch 3: 0.6048\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 74/74 [00:12<00:00,  5.99it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 23.34it/s]\n",
      " 10%|█         | 4/40 [00:51<07:41, 12.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "train loss epoch 4: 1.3956\n",
      "valid loss epoch 4: 1.2495\n",
      "train_accuracy epoch 4: 0.5910\n",
      "val_accuracy epoch 4: 0.6662\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 74/74 [00:12<00:00,  6.00it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 23.21it/s]\n",
      " 12%|█▎        | 5/40 [01:04<07:28, 12.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "train loss epoch 5: 1.2809\n",
      "valid loss epoch 5: 1.1320\n",
      "train_accuracy epoch 5: 0.6383\n",
      "val_accuracy epoch 5: 0.7110\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 74/74 [00:12<00:00,  6.05it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 23.43it/s]\n",
      " 15%|█▌        | 6/40 [01:16<07:14, 12.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "train loss epoch 6: 1.1832\n",
      "valid loss epoch 6: 1.0488\n",
      "train_accuracy epoch 6: 0.6783\n",
      "val_accuracy epoch 6: 0.7395\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 74/74 [00:12<00:00,  5.95it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 22.96it/s]\n",
      " 18%|█▊        | 7/40 [01:29<07:03, 12.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "train loss epoch 7: 1.0884\n",
      "valid loss epoch 7: 0.9663\n",
      "train_accuracy epoch 7: 0.7149\n",
      "val_accuracy epoch 7: 0.7681\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 74/74 [00:12<00:00,  5.91it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 22.38it/s]\n",
      " 20%|██        | 8/40 [01:42<06:52, 12.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "train loss epoch 8: 1.0127\n",
      "valid loss epoch 8: 0.8899\n",
      "train_accuracy epoch 8: 0.7435\n",
      "val_accuracy epoch 8: 0.7981\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 74/74 [00:12<00:00,  5.91it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 22.98it/s]\n",
      " 22%|██▎       | 9/40 [01:55<06:41, 12.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "train loss epoch 9: 0.9471\n",
      "valid loss epoch 9: 0.8107\n",
      "train_accuracy epoch 9: 0.7617\n",
      "val_accuracy epoch 9: 0.8276\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 74/74 [00:12<00:00,  6.01it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 18.13it/s]\n",
      " 25%|██▌       | 10/40 [02:08<06:28, 12.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "train loss epoch 10: 0.8795\n",
      "valid loss epoch 10: 0.7527\n",
      "train_accuracy epoch 10: 0.7827\n",
      "val_accuracy epoch 10: 0.8386\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 74/74 [00:12<00:00,  6.03it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 23.23it/s]\n",
      " 28%|██▊       | 11/40 [02:21<06:13, 12.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "train loss epoch 11: 0.8238\n",
      "valid loss epoch 11: 0.6909\n",
      "train_accuracy epoch 11: 0.7975\n",
      "val_accuracy epoch 11: 0.8488\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 74/74 [00:12<00:00,  5.99it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 22.97it/s]\n",
      " 30%|███       | 12/40 [02:34<06:00, 12.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "train loss epoch 12: 0.7708\n",
      "valid loss epoch 12: 0.6579\n",
      "train_accuracy epoch 12: 0.8127\n",
      "val_accuracy epoch 12: 0.8600\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 74/74 [00:12<00:00,  5.96it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 23.01it/s]\n",
      " 32%|███▎      | 13/40 [02:47<05:47, 12.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "train loss epoch 13: 0.7350\n",
      "valid loss epoch 13: 0.6243\n",
      "train_accuracy epoch 13: 0.8198\n",
      "val_accuracy epoch 13: 0.8602\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 74/74 [00:12<00:00,  5.97it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 21.65it/s]\n",
      " 35%|███▌      | 14/40 [03:00<05:35, 12.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "train loss epoch 14: 0.6920\n",
      "valid loss epoch 14: 0.5992\n",
      "train_accuracy epoch 14: 0.8292\n",
      "val_accuracy epoch 14: 0.8607\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 74/74 [00:12<00:00,  5.93it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 22.80it/s]\n",
      " 38%|███▊      | 15/40 [03:13<05:23, 12.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "train loss epoch 15: 0.6499\n",
      "valid loss epoch 15: 0.5410\n",
      "train_accuracy epoch 15: 0.8391\n",
      "val_accuracy epoch 15: 0.8771\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 74/74 [00:12<00:00,  5.94it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 23.04it/s]\n",
      " 40%|████      | 16/40 [03:26<05:10, 12.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "train loss epoch 16: 0.6167\n",
      "valid loss epoch 16: 0.5237\n",
      "train_accuracy epoch 16: 0.8446\n",
      "val_accuracy epoch 16: 0.8843\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 74/74 [00:12<00:00,  5.86it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 22.82it/s]\n",
      " 42%|████▎     | 17/40 [03:39<04:58, 13.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "train loss epoch 17: 0.5814\n",
      "valid loss epoch 17: 0.4990\n",
      "train_accuracy epoch 17: 0.8560\n",
      "val_accuracy epoch 17: 0.8831\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 74/74 [00:12<00:00,  5.91it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 22.91it/s]\n",
      " 45%|████▌     | 18/40 [03:52<04:46, 13.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "train loss epoch 18: 0.5608\n",
      "valid loss epoch 18: 0.4540\n",
      "train_accuracy epoch 18: 0.8563\n",
      "val_accuracy epoch 18: 0.8914\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 74/74 [00:12<00:00,  5.95it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 21.47it/s]\n",
      " 48%|████▊     | 19/40 [04:05<04:33, 13.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "train loss epoch 19: 0.5261\n",
      "valid loss epoch 19: 0.4377\n",
      "train_accuracy epoch 19: 0.8653\n",
      "val_accuracy epoch 19: 0.8976\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 74/74 [00:12<00:00,  5.93it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 23.13it/s]\n",
      " 50%|█████     | 20/40 [04:18<04:19, 12.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "train loss epoch 20: 0.5139\n",
      "valid loss epoch 20: 0.4153\n",
      "train_accuracy epoch 20: 0.8657\n",
      "val_accuracy epoch 20: 0.8998\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 74/74 [00:12<00:00,  5.99it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 23.37it/s]\n",
      " 52%|█████▎    | 21/40 [04:31<04:06, 12.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "train loss epoch 21: 0.4817\n",
      "valid loss epoch 21: 0.3957\n",
      "train_accuracy epoch 21: 0.8722\n",
      "val_accuracy epoch 21: 0.9017\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 74/74 [00:12<00:00,  5.99it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 23.45it/s]\n",
      " 55%|█████▌    | 22/40 [04:44<03:52, 12.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "train loss epoch 22: 0.4674\n",
      "valid loss epoch 22: 0.3849\n",
      "train_accuracy epoch 22: 0.8754\n",
      "val_accuracy epoch 22: 0.9090\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 74/74 [00:12<00:00,  6.04it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 22.11it/s]\n",
      " 57%|█████▊    | 23/40 [04:56<03:38, 12.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "train loss epoch 23: 0.4442\n",
      "valid loss epoch 23: 0.3829\n",
      "train_accuracy epoch 23: 0.8795\n",
      "val_accuracy epoch 23: 0.9076\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 74/74 [00:12<00:00,  5.97it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 23.33it/s]\n",
      " 60%|██████    | 24/40 [05:09<03:26, 12.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "train loss epoch 24: 0.4214\n",
      "valid loss epoch 24: 0.3562\n",
      "train_accuracy epoch 24: 0.8870\n",
      "val_accuracy epoch 24: 0.9100\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 74/74 [00:12<00:00,  5.98it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 23.22it/s]\n",
      " 62%|██████▎   | 25/40 [05:22<03:13, 12.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "train loss epoch 25: 0.4091\n",
      "valid loss epoch 25: 0.3313\n",
      "train_accuracy epoch 25: 0.8879\n",
      "val_accuracy epoch 25: 0.9095\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 74/74 [00:12<00:00,  5.97it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 22.27it/s]\n",
      " 65%|██████▌   | 26/40 [05:35<03:00, 12.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "train loss epoch 26: 0.3887\n",
      "valid loss epoch 26: 0.3168\n",
      "train_accuracy epoch 26: 0.8935\n",
      "val_accuracy epoch 26: 0.9183\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 74/74 [00:12<00:00,  6.01it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 22.85it/s]\n",
      " 68%|██████▊   | 27/40 [05:48<02:47, 12.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "train loss epoch 27: 0.3767\n",
      "valid loss epoch 27: 0.3379\n",
      "train_accuracy epoch 27: 0.8961\n",
      "val_accuracy epoch 27: 0.9052\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 74/74 [00:12<00:00,  5.93it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 22.10it/s]\n",
      " 70%|███████   | 28/40 [06:01<02:34, 12.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "train loss epoch 28: 0.3648\n",
      "valid loss epoch 28: 0.3286\n",
      "train_accuracy epoch 28: 0.8978\n",
      "val_accuracy epoch 28: 0.9112\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 74/74 [00:12<00:00,  5.93it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 23.32it/s]\n",
      " 72%|███████▎  | 29/40 [06:14<02:22, 12.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "train loss epoch 29: 0.3504\n",
      "valid loss epoch 29: 0.3194\n",
      "train_accuracy epoch 29: 0.9019\n",
      "val_accuracy epoch 29: 0.9117\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 74/74 [00:12<00:00,  5.97it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 23.36it/s]\n",
      " 75%|███████▌  | 30/40 [06:27<02:09, 12.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "train loss epoch 30: 0.3323\n",
      "valid loss epoch 30: 0.2858\n",
      "train_accuracy epoch 30: 0.9066\n",
      "val_accuracy epoch 30: 0.9179\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 74/74 [00:12<00:00,  6.01it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 23.47it/s]\n",
      " 78%|███████▊  | 31/40 [06:39<01:55, 12.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "train loss epoch 31: 0.3340\n",
      "valid loss epoch 31: 0.2744\n",
      "train_accuracy epoch 31: 0.9052\n",
      "val_accuracy epoch 31: 0.9236\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 74/74 [00:12<00:00,  5.99it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 23.04it/s]\n",
      " 80%|████████  | 32/40 [06:52<01:42, 12.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "train loss epoch 32: 0.3176\n",
      "valid loss epoch 32: 0.2670\n",
      "train_accuracy epoch 32: 0.9089\n",
      "val_accuracy epoch 32: 0.9269\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 74/74 [00:12<00:00,  5.91it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 22.96it/s]\n",
      " 82%|████████▎ | 33/40 [07:05<01:30, 12.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "train loss epoch 33: 0.3143\n",
      "valid loss epoch 33: 0.2711\n",
      "train_accuracy epoch 33: 0.9083\n",
      "val_accuracy epoch 33: 0.9276\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 74/74 [00:12<00:00,  5.94it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 22.88it/s]\n",
      " 85%|████████▌ | 34/40 [07:18<01:17, 12.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "train loss epoch 34: 0.2990\n",
      "valid loss epoch 34: 0.2349\n",
      "train_accuracy epoch 34: 0.9124\n",
      "val_accuracy epoch 34: 0.9333\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 74/74 [00:12<00:00,  5.97it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 22.97it/s]\n",
      " 88%|████████▊ | 35/40 [07:31<01:04, 12.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "train loss epoch 35: 0.2874\n",
      "valid loss epoch 35: 0.2681\n",
      "train_accuracy epoch 35: 0.9165\n",
      "val_accuracy epoch 35: 0.9281\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 74/74 [00:12<00:00,  6.03it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 23.41it/s]\n",
      " 90%|█████████ | 36/40 [07:44<00:51, 12.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "train loss epoch 36: 0.2934\n",
      "valid loss epoch 36: 0.2486\n",
      "train_accuracy epoch 36: 0.9139\n",
      "val_accuracy epoch 36: 0.9324\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 74/74 [00:12<00:00,  5.99it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 23.35it/s]\n",
      " 92%|█████████▎| 37/40 [07:57<00:38, 12.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "train loss epoch 37: 0.2745\n",
      "valid loss epoch 37: 0.2186\n",
      "train_accuracy epoch 37: 0.9202\n",
      "val_accuracy epoch 37: 0.9360\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 74/74 [00:12<00:00,  5.99it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 22.84it/s]\n",
      " 95%|█████████▌| 38/40 [08:10<00:25, 12.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "train loss epoch 38: 0.2724\n",
      "valid loss epoch 38: 0.2322\n",
      "train_accuracy epoch 38: 0.9187\n",
      "val_accuracy epoch 38: 0.9305\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 74/74 [00:12<00:00,  5.96it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 23.51it/s]\n",
      " 98%|█████████▊| 39/40 [08:23<00:12, 12.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "train loss epoch 39: 0.2613\n",
      "valid loss epoch 39: 0.2141\n",
      "train_accuracy epoch 39: 0.9231\n",
      "val_accuracy epoch 39: 0.9374\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 74/74 [00:12<00:00,  6.03it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 22.83it/s]\n",
      "100%|██████████| 40/40 [08:35<00:00, 12.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "train loss epoch 40: 0.2553\n",
      "valid loss epoch 40: 0.2231\n",
      "train_accuracy epoch 40: 0.9229\n",
      "val_accuracy epoch 40: 0.9393\n",
      "------------------------------\n",
      "Training time:515.89s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(),\n",
    "    betas=ADAM_BETAS,\n",
    "    lr=LEARNING_RATE,\n",
    "    weight_decay=ADAM_WEIGHT_DECAY,\n",
    ")\n",
    "\n",
    "print(f\"{device=}\")\n",
    "start = timeit.default_timer()\n",
    "\n",
    "for epoch in tqdm(range(EPOCHS), position=0, leave=True):\n",
    "    \"\"\"\n",
    "    position=0: Ensures the progress bar appears at a fixed position in the console (useful when running nested loops or multiple progress bars).\n",
    "    leave=True: Ensures the progress bar remains on the console after the loop finishes. If set to False, the progress bar disappears after completion.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    train_labels = []\n",
    "    train_preds = []\n",
    "    train_running_loss = 0\n",
    "\n",
    "    for idx, img_label in enumerate(tqdm(train_dataloader, position=0, leave=True)):\n",
    "        img = img_label[\"image\"].float().to(device)\n",
    "        label = img_label[\"label\"].type(torch.uint8).to(device)\n",
    "        y_pred = model(img)\n",
    "        y_pred_label = torch.argmax(y_pred, dim=1)\n",
    "\n",
    "        train_labels.extend(\n",
    "            label.cpu().detach()\n",
    "        )  # otherwise you will ru out of menmory\n",
    "        train_preds.extend(y_pred_label.cpu().detach())\n",
    "\n",
    "        loss = criterion(y_pred, label)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_running_loss += loss.item()\n",
    "    train_loss = train_running_loss / (idx + 1)  # loss for one epoch\n",
    "\n",
    "    model.eval()\n",
    "    val_labels = []\n",
    "    val_preds = []\n",
    "    val_running_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, img_label in enumerate(tqdm(val_dataloader, position=0, leave=True)):\n",
    "            img = img_label[\"image\"].float().to(device)\n",
    "            label = img_label[\"label\"].type(torch.uint8).to(device)\n",
    "            y_pred = model(img)\n",
    "            y_pred_label = torch.argmax(y_pred, dim=1)\n",
    "\n",
    "            val_labels.extend(label.cpu().detach())\n",
    "            val_preds.extend(y_pred_label.cpu().detach())\n",
    "\n",
    "            loss = criterion(y_pred, label)\n",
    "            val_running_loss += loss.item()\n",
    "\n",
    "    val_loss = val_running_loss / (idx + 1)  # loss for one epoch\n",
    "\n",
    "    print(\"-\" * 30)\n",
    "    print(f\"train loss epoch {epoch+1}: {train_loss:.4f}\")\n",
    "    print(f\"valid loss epoch {epoch+1}: {val_loss:.4f}\")\n",
    "    print(\n",
    "        f\"train_accuracy epoch {epoch+1}: {sum(1 for x,y in zip(train_preds, train_labels) if x==y)/len(train_labels):.4f}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"val_accuracy epoch {epoch+1}: {sum(1 for x,y in zip(val_preds, val_labels) if x==y)/len(val_labels):.4f}\"\n",
    "    )\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "\n",
    "stop = timeit.default_timer()\n",
    "print(f\"Training time:{stop-start:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"torch.cuda.empty_cache() is a PyTorch function that frees up unused GPU memory in the cache, making it available for other operations. This can be helpful when you encounter GPU memory issues, but it doesn't directly reduce the memory used by your tensors or computations. It simply clears memory that PyTorch's caching allocator is holding onto for potential reuse.\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()  # to free gpu space\n",
    "\"\"\"torch.cuda.empty_cache() is a PyTorch function that frees up unused GPU memory in the cache, making it available for other operations. This can be helpful when you encounter GPU memory issues, but it doesn't directly reduce the memory used by your tensors or computations. It simply clears memory that PyTorch's caching allocator is holding onto for potential reuse.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 55/55 [00:02<00:00, 20.50it/s]\n"
     ]
    }
   ],
   "source": [
    "labels = []\n",
    "ids = []\n",
    "imgs = []\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for idx, sample in enumerate(tqdm(test_dataloader, position=0, leave=True)):\n",
    "        img = sample[\"image\"].to(device)\n",
    "        ids.extend([int(i) + 1 for i in sample[\"index\"]])\n",
    "\n",
    "        outputs = model(img)\n",
    "        imgs.extend(img.detach().cpu())\n",
    "        labels.extend([int(i) for i in torch.argmax(outputs, dim=1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "f, axarr = plt.subplots(2, 3)\n",
    "counter = 0\n",
    "for i in range(2):\n",
    "    for j in range(3):\n",
    "        axarr[i][j].imshow(imgs[counter].squeeze(), cmap=\"gray\")\n",
    "        axarr[i][j].set_title(f\"predicted {labels[counter]}\")\n",
    "        counter += 1\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch3d",
   "language": "python",
   "name": "pytorch3d"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
