{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Segmentation in computer vision is a task where the goal is to classify each pixel in an image into a specific category. Unlike classification (which assigns a label to the entire image) or object detection (which localizes objects), segmentation provides **pixel-level granularity**.\n",
    "\n",
    "There are two primary types of segmentation:\n",
    "\n",
    "1. **Semantic Segmentation**: Classifies all pixels of an image into categories but doesn't distinguish between individual objects of the same class.\n",
    "2. **Instance Segmentation**: Distinguishes between individual objects of the same class in addition to classifying pixels.\n",
    "\n",
    "Let’s break down how segmentation works step by step, including equations and details.\n",
    "\n",
    "---\n",
    "\n",
    "### **1. The Problem Setup**\n",
    "Segmentation is treated as a **dense prediction task**, where we predict a label for every pixel in the input image.\n",
    "\n",
    "#### Input:\n",
    "- Image \\( \\mathbf{I} \\) of size \\( H \\times W \\times C \\), where \\( H \\) is the height, \\( W \\) is the width, and \\( C \\) is the number of channels (e.g., 3 for RGB images).\n",
    "\n",
    "#### Output:\n",
    "- **Semantic Segmentation**: A label map \\( \\mathbf{L} \\) of size \\( H \\times W \\), where each pixel has a class label \\( l \\in \\{0, 1, \\dots, K-1\\} \\), with \\( K \\) being the number of classes.\n",
    "- **Instance Segmentation**: Similar to semantic segmentation, but with additional instance IDs for individual objects.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Neural Network Architecture**\n",
    "The core architecture for segmentation is typically a fully convolutional network (FCN). Popular variants include **U-Net**, **DeepLab**, and **Mask R-CNN**.\n",
    "\n",
    "#### Components:\n",
    "1. **Encoder**: Extracts hierarchical features from the image using convolutional layers (e.g., ResNet, VGG).\n",
    "2. **Decoder**: Upsamples these features back to the original image resolution using techniques like transposed convolutions or bilinear interpolation.\n",
    "3. **Pixel-wise Classification**: Outputs a probability distribution for each pixel across \\( K \\) classes.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Forward Pass and Predictions**\n",
    "For each pixel \\( p \\), the model predicts a vector \\( \\mathbf{y}_p \\) of probabilities for each class \\( k \\):\n",
    "\\[\n",
    "\\mathbf{y}_p = \\text{Softmax}(\\mathbf{z}_p), \\quad \\mathbf{z}_p = f(\\mathbf{I}; \\Theta)\n",
    "\\]\n",
    "Where:\n",
    "- \\( f(\\mathbf{I}; \\Theta) \\): The segmentation model with parameters \\( \\Theta \\).\n",
    "- \\( \\mathbf{z}_p \\): Logits (unnormalized scores) for pixel \\( p \\).\n",
    "- \\( \\mathbf{y}_p[k] = \\frac{e^{\\mathbf{z}_p[k]}}{\\sum_{j=1}^K e^{\\mathbf{z}_p[j]}} \\): The softmax function converts logits into probabilities.\n",
    "\n",
    "The predicted class for pixel \\( p \\) is:\n",
    "\\[\n",
    "\\hat{l}_p = \\arg\\max_k \\mathbf{y}_p[k]\n",
    "\\]\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Loss Function**\n",
    "Segmentation uses a **pixel-wise loss function** to compare predictions with ground truth labels.\n",
    "\n",
    "#### (a) **Cross-Entropy Loss**:\n",
    "For a single pixel \\( p \\):\n",
    "\\[\n",
    "\\mathcal{L}_p = - \\sum_{k=1}^K \\mathbf{y}_p^{\\text{true}}[k] \\log(\\mathbf{y}_p[k])\n",
    "\\]\n",
    "Where:\n",
    "- \\( \\mathbf{y}_p^{\\text{true}}[k] = 1 \\) if the true label of \\( p \\) is \\( k \\), and 0 otherwise.\n",
    "\n",
    "The total loss over the entire image is:\n",
    "\\[\n",
    "\\mathcal{L} = \\frac{1}{H \\times W} \\sum_{p \\in \\mathbf{I}} \\mathcal{L}_p\n",
    "\\]\n",
    "\n",
    "#### (b) **Dice Loss**:\n",
    "Dice loss measures overlap between predicted and true segmentation masks:\n",
    "\\[\n",
    "\\text{Dice Loss} = 1 - \\frac{2 \\sum_{p \\in \\mathbf{I}} \\mathbf{y}_p \\mathbf{y}_p^{\\text{true}}}{\\sum_{p \\in \\mathbf{I}} \\mathbf{y}_p^2 + \\sum_{p \\in \\mathbf{I}} (\\mathbf{y}_p^{\\text{true}})^2}\n",
    "\\]\n",
    "\n",
    "#### (c) **Combination of Losses**:\n",
    "In practice, models often combine cross-entropy and dice loss to balance pixel-wise and region-wise accuracy.\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Evaluation Metrics**\n",
    "To assess segmentation performance, we use metrics such as:\n",
    "\n",
    "1. **Pixel Accuracy**:\n",
    "\\[\n",
    "\\text{Pixel Accuracy} = \\frac{\\text{Number of correctly classified pixels}}{\\text{Total number of pixels}}\n",
    "\\]\n",
    "\n",
    "2. **Intersection over Union (IoU)**:\n",
    "For each class \\( k \\):\n",
    "\\[\n",
    "\\text{IoU}_k = \\frac{\\text{True Positive}}{\\text{True Positive} + \\text{False Positive} + \\text{False Negative}}\n",
    "\\]\n",
    "\n",
    "3. **Mean IoU (mIoU)**:\n",
    "Average IoU across all classes:\n",
    "\\[\n",
    "\\text{mIoU} = \\frac{1}{K} \\sum_{k=1}^K \\text{IoU}_k\n",
    "\\]\n",
    "\n",
    "---\n",
    "\n",
    "### **6. Example**\n",
    "Let’s go through a simplified semantic segmentation example.\n",
    "\n",
    "#### Input:\n",
    "- \\( \\mathbf{I} \\): A 3x3 grayscale image.\n",
    "- Ground truth \\( \\mathbf{L} \\):\n",
    "\\[\n",
    "\\mathbf{L} =\n",
    "\\begin{bmatrix}\n",
    "0 & 0 & 1 \\\\\n",
    "0 & 1 & 1 \\\\\n",
    "2 & 2 & 2\n",
    "\\end{bmatrix}\n",
    "\\]\n",
    "\n",
    "#### Model Output:\n",
    "Logits for each pixel (before softmax):\n",
    "\\[\n",
    "\\mathbf{z} =\n",
    "\\begin{bmatrix}\n",
    "(2, 1, 0) & (3, 0, 1) & (0, 4, 2) \\\\\n",
    "(2, 1, 0) & (1, 2, 0) & (0, 3, 1) \\\\\n",
    "(0, 1, 4) & (1, 0, 3) & (0, 0, 5)\n",
    "\\end{bmatrix}\n",
    "\\]\n",
    "\n",
    "#### Step 1: Compute probabilities with softmax.\n",
    "For pixel \\( (1, 1) \\), logits are \\( (2, 1, 0) \\):\n",
    "\\[\n",
    "\\mathbf{y} = \\text{Softmax}([2, 1, 0]) = \\left[\\frac{e^2}{e^2 + e^1 + e^0}, \\frac{e^1}{e^2 + e^1 + e^0}, \\frac{e^0}{e^2 + e^1 + e^0}\\right]\n",
    "\\]\n",
    "\n",
    "#### Step 2: Compute pixel-wise cross-entropy loss.\n",
    "If the true label for \\( (1, 1) \\) is \\( 0 \\):\n",
    "\\[\n",
    "\\mathcal{L}_{(1,1)} = -\\log(\\mathbf{y}[0])\n",
    "\\]\n",
    "\n",
    "#### Step 3: Compute overall loss.\n",
    "Average the losses across all pixels.\n",
    "\n",
    "---\n",
    "\n",
    "### **7. Advanced Topics**\n",
    "1. **Attention Mechanisms**: Enhance segmentation by focusing on important regions (e.g., DeepLab's Atrous Spatial Pyramid Pooling).\n",
    "2. **Instance Segmentation**: Adds bounding boxes or masks for individual objects (e.g., Mask R-CNN).\n",
    "3. **Transformers**: Vision transformers (e.g., SegFormer) are increasingly used for segmentation.\n",
    "\n",
    "---\n",
    "\n",
    "Let me know if you need further clarifications or examples!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dice Loss and Intersection over Union (IoU) are related but not the same. Both are metrics for evaluating segmentation performance, but their formulations differ in subtle yet important ways. Let's clarify the distinction:\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Dice Loss**\n",
    "The Dice coefficient (used in Dice Loss) is a measure of overlap between the predicted segmentation (\\( P \\)) and the ground truth segmentation (\\( G \\)).\n",
    "\n",
    "#### Dice Coefficient:\n",
    "\\[\n",
    "\\text{Dice Coefficient} = \\frac{2 \\cdot |P \\cap G|}{|P| + |G|}\n",
    "\\]\n",
    "Where:\n",
    "- \\( |P \\cap G| \\): The intersection of predicted and ground truth regions.\n",
    "- \\( |P| \\): The total area of the predicted region.\n",
    "- \\( |G| \\): The total area of the ground truth region.\n",
    "\n",
    "#### Dice Loss:\n",
    "Dice Loss is defined as:\n",
    "\\[\n",
    "\\text{Dice Loss} = 1 - \\text{Dice Coefficient}\n",
    "\\]\n",
    "In terms of pixel values:\n",
    "\\[\n",
    "\\text{Dice Coefficient} = \\frac{2 \\sum_{i} p_i g_i}{\\sum_{i} p_i^2 + \\sum_{i} g_i^2}\n",
    "\\]\n",
    "Where \\( p_i \\) and \\( g_i \\) are the predicted and ground truth values for pixel \\( i \\).\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Intersection over Union (IoU)**\n",
    "IoU (also called the Jaccard Index) measures the ratio of the intersection to the union of predicted and ground truth regions.\n",
    "\n",
    "#### IoU:\n",
    "\\[\n",
    "\\text{IoU} = \\frac{|P \\cap G|}{|P \\cup G|}\n",
    "\\]\n",
    "Where:\n",
    "- \\( |P \\cup G| = |P| + |G| - |P \\cap G| \\)\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Difference**\n",
    "The **Dice coefficient** emphasizes the **overlap** between the prediction and ground truth, giving more weight to the intersection. In contrast, **IoU** measures the intersection as a proportion of the total area covered by both prediction and ground truth.\n",
    "\n",
    "In terms of formula:\n",
    "\\[\n",
    "\\text{Dice Coefficient} = \\frac{2 \\cdot \\text{Intersection}}{\\text{Predicted Area} + \\text{Ground Truth Area}}\n",
    "\\]\n",
    "\\[\n",
    "\\text{IoU} = \\frac{\\text{Intersection}}{\\text{Predicted Area} + \\text{Ground Truth Area} - \\text{Intersection}}\n",
    "\\]\n",
    "\n",
    "---\n",
    "\n",
    "### **Relationship Between Dice and IoU**\n",
    "While Dice and IoU are related, they are not equivalent:\n",
    "\\[\n",
    "\\text{Dice Coefficient} = \\frac{2 \\cdot \\text{IoU}}{\\text{IoU} + 1}\n",
    "\\]\n",
    "\\[\n",
    "\\text{IoU} = \\frac{\\text{Dice Coefficient}}{2 - \\text{Dice Coefficient}}\n",
    "\\]\n",
    "\n",
    "---\n",
    "\n",
    "### **Why is Dice Loss Popular in Segmentation?**\n",
    "1. **Sensitivity to Class Imbalance**: Dice Loss works well for segmentation tasks with imbalanced datasets, as it directly measures overlap.\n",
    "2. **Smooth Gradient**: The Dice formulation ensures smoother gradients for optimization, especially when overlap is low.\n",
    "\n",
    "---\n",
    "\n",
    "To summarize: **Dice Loss is not simply \\( 1 - \\text{IoU} \\)**, but they are closely related and often used together for segmentation tasks. Let me know if you need a deeper dive into practical use cases or examples!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression is a supervised machine learning algorithm used for binary classification problems. It predicts the probability that a given input belongs to a particular class. Below is a full-depth explanation of logistic regression, covering its equations, loss functions, Maximum Likelihood Estimation (MLE), and sigmoid function.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. **Model Formulation**\n",
    "\n",
    "Logistic regression models the probability \\( P(y=1 | \\mathbf{x}) \\) using the **sigmoid function** applied to a linear combination of input features:\n",
    "\n",
    "\\[\n",
    "P(y=1 | \\mathbf{x}) = \\sigma(\\mathbf{w}^\\top \\mathbf{x} + b)\n",
    "\\]\n",
    "\n",
    "Where:\n",
    "- \\( \\mathbf{x} \\) is the feature vector.\n",
    "- \\( \\mathbf{w} \\) is the weight vector.\n",
    "- \\( b \\) is the bias (intercept).\n",
    "- \\( \\sigma(z) \\) is the sigmoid function, defined as:\n",
    "  \\[\n",
    "  \\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "  \\]\n",
    "\n",
    "The model's prediction \\( \\hat{y} \\) is the probability that \\( y=1 \\):\n",
    "\\[\n",
    "\\hat{y} = P(y=1 | \\mathbf{x}) = \\sigma(\\mathbf{w}^\\top \\mathbf{x} + b)\n",
    "\\]\n",
    "\n",
    "For binary classification, \\( P(y=0 | \\mathbf{x}) \\) is simply:\n",
    "\\[\n",
    "P(y=0 | \\mathbf{x}) = 1 - \\sigma(\\mathbf{w}^\\top \\mathbf{x} + b)\n",
    "\\]\n",
    "\n",
    "---\n",
    "\n",
    "## 2. **Maximum Likelihood Estimation (MLE)**\n",
    "\n",
    "Logistic regression optimizes the parameters \\( \\mathbf{w} \\) and \\( b \\) by maximizing the likelihood of the observed data.\n",
    "\n",
    "### Likelihood Function\n",
    "\n",
    "Given \\( n \\) independent training examples \\( (\\mathbf{x}_i, y_i) \\), the likelihood of the dataset is:\n",
    "\\[\n",
    "L(\\mathbf{w}, b) = \\prod_{i=1}^n P(y_i | \\mathbf{x}_i)\n",
    "\\]\n",
    "\n",
    "Using the predicted probabilities:\n",
    "\\[\n",
    "L(\\mathbf{w}, b) = \\prod_{i=1}^n \\left[ \\sigma(\\mathbf{w}^\\top \\mathbf{x}_i + b) \\right]^{y_i} \\left[ 1 - \\sigma(\\mathbf{w}^\\top \\mathbf{x}_i + b) \\right]^{1 - y_i}\n",
    "\\]\n",
    "\n",
    "### Log-Likelihood\n",
    "\n",
    "For numerical stability and easier computation, the log-likelihood is used:\n",
    "\\[\n",
    "\\ell(\\mathbf{w}, b) = \\log L(\\mathbf{w}, b) = \\sum_{i=1}^n \\left[ y_i \\log \\sigma(\\mathbf{w}^\\top \\mathbf{x}_i + b) + (1 - y_i) \\log \\left( 1 - \\sigma(\\mathbf{w}^\\top \\mathbf{x}_i + b) \\right) \\right]\n",
    "\\]\n",
    "\n",
    "---\n",
    "\n",
    "## 3. **Loss Function: Cross-Entropy**\n",
    "\n",
    "Instead of maximizing the log-likelihood, logistic regression minimizes the **negative log-likelihood**, also known as the **cross-entropy loss**:\n",
    "\n",
    "\\[\n",
    "J(\\mathbf{w}, b) = -\\frac{1}{n} \\sum_{i=1}^n \\left[ y_i \\log \\hat{y}_i + (1 - y_i) \\log (1 - \\hat{y}_i) \\right]\n",
    "\\]\n",
    "\n",
    "Where:\n",
    "- \\( \\hat{y}_i = \\sigma(\\mathbf{w}^\\top \\mathbf{x}_i + b) \\) is the predicted probability for \\( y_i=1 \\).\n",
    "\n",
    "---\n",
    "\n",
    "## 4. **Gradient Descent for Optimization**\n",
    "\n",
    "The weights \\( \\mathbf{w} \\) and bias \\( b \\) are updated iteratively using gradient descent.\n",
    "\n",
    "### Gradients\n",
    "\n",
    "The gradient of the loss function \\( J \\) with respect to \\( \\mathbf{w} \\) and \\( b \\) is:\n",
    "\n",
    "1. Gradient with respect to \\( \\mathbf{w} \\):\n",
    "   \\[\n",
    "   \\frac{\\partial J}{\\partial \\mathbf{w}} = \\frac{1}{n} \\sum_{i=1}^n \\left( \\hat{y}_i - y_i \\right) \\mathbf{x}_i\n",
    "   \\]\n",
    "\n",
    "2. Gradient with respect to \\( b \\):\n",
    "   \\[\n",
    "   \\frac{\\partial J}{\\partial b} = \\frac{1}{n} \\sum_{i=1}^n \\left( \\hat{y}_i - y_i \\right)\n",
    "   \\]\n",
    "\n",
    "### Parameter Update\n",
    "\n",
    "Using a learning rate \\( \\eta \\), the parameters are updated as:\n",
    "\\[\n",
    "\\mathbf{w} \\leftarrow \\mathbf{w} - \\eta \\frac{\\partial J}{\\partial \\mathbf{w}}\n",
    "\\]\n",
    "\\[\n",
    "b \\leftarrow b - \\eta \\frac{\\partial J}{\\partial b}\n",
    "\\]\n",
    "\n",
    "---\n",
    "\n",
    "## 5. **Interpretation of the Sigmoid Function**\n",
    "\n",
    "The sigmoid function maps any real-valued number \\( z \\) into the range \\( (0, 1) \\), making it suitable for modeling probabilities.\n",
    "\n",
    "\\[\n",
    "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "\\]\n",
    "\n",
    "Key properties:\n",
    "- \\( \\sigma(z) \\to 0 \\) as \\( z \\to -\\infty \\).\n",
    "- \\( \\sigma(z) \\to 1 \\) as \\( z \\to +\\infty \\).\n",
    "- \\( \\sigma(0) = 0.5 \\).\n",
    "\n",
    "---\n",
    "\n",
    "## 6. **Decision Boundary**\n",
    "\n",
    "Logistic regression predicts \\( y=1 \\) if the probability \\( \\hat{y} \\geq 0.5 \\), and \\( y=0 \\) otherwise. This corresponds to a decision boundary defined by:\n",
    "\n",
    "\\[\n",
    "\\mathbf{w}^\\top \\mathbf{x} + b = 0\n",
    "\\]\n",
    "\n",
    "---\n",
    "\n",
    "## 7. **Extensions**\n",
    "\n",
    "- **Regularization**: Add \\( L1 \\) (Lasso) or \\( L2 \\) (Ridge) penalties to the loss function to prevent overfitting:\n",
    "  \\[\n",
    "  J_{reg}(\\mathbf{w}, b) = J(\\mathbf{w}, b) + \\lambda \\| \\mathbf{w} \\|_p\n",
    "  \\]\n",
    "  Where \\( p=1 \\) for \\( L1 \\) and \\( p=2 \\) for \\( L2 \\).\n",
    "\n",
    "- **Multinomial Logistic Regression**: For multi-class problems, generalize logistic regression using the softmax function.\n",
    "\n",
    "---\n",
    "\n",
    "This formulation covers all critical aspects of logistic regression, including equations, loss functions, MLE, and optimization. Let me know if you'd like code examples or further elaboration on any part!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sure! Let’s go through **Logistic Regression** step-by-step, covering every concept in depth in a logical flow.\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 1: The Problem**\n",
    "Logistic regression is used for **binary classification problems**, where the target variable \\( y \\) can take one of two values: \\( y \\in \\{0, 1\\} \\).\n",
    "\n",
    "Example: Predicting whether an email is spam (\\( y=1 \\)) or not (\\( y=0 \\)).\n",
    "\n",
    "The goal is to predict the probability that the target \\( y \\) belongs to one of the classes given input features \\( \\mathbf{x} \\).\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 2: The Model**\n",
    "Logistic regression is a **linear model** that predicts the probability \\( P(y=1|\\mathbf{x}) \\) using the **sigmoid function** applied to a linear combination of input features:\n",
    "\n",
    "\\[\n",
    "P(y=1|\\mathbf{x}) = \\hat{y} = \\sigma(z), \\quad z = \\mathbf{w}^\\top \\mathbf{x} + b\n",
    "\\]\n",
    "\n",
    "Where:\n",
    "- \\( \\mathbf{x} = [x_1, x_2, \\dots, x_d] \\) is the input feature vector.\n",
    "- \\( \\mathbf{w} = [w_1, w_2, \\dots, w_d] \\) are the weights.\n",
    "- \\( b \\) is the bias term.\n",
    "- \\( z \\) is the linear combination of inputs (\\( z = \\mathbf{w}^\\top \\mathbf{x} + b \\)).\n",
    "- \\( \\sigma(z) \\) is the **sigmoid function**, defined as:\n",
    "  \\[\n",
    "  \\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "  \\]\n",
    "\n",
    "#### **Why Sigmoid?**\n",
    "The sigmoid function squashes \\( z \\) (which can range from \\( -\\infty \\) to \\( +\\infty \\)) into a range of \\( (0, 1) \\), making it suitable for modeling probabilities.\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 3: Likelihood and Log-Likelihood**\n",
    "To train the logistic regression model, we need to estimate \\( \\mathbf{w} \\) and \\( b \\) such that the predicted probabilities \\( \\hat{y} \\) match the actual labels \\( y \\).\n",
    "\n",
    "#### **Likelihood Function**\n",
    "For \\( n \\) training samples, the likelihood of observing the data is:\n",
    "\\[\n",
    "L(\\mathbf{w}, b) = \\prod_{i=1}^n P(y_i|\\mathbf{x}_i)\n",
    "\\]\n",
    "\n",
    "Since \\( P(y|\\mathbf{x}) \\) depends on whether \\( y=1 \\) or \\( y=0 \\), we can write:\n",
    "\\[\n",
    "P(y_i|\\mathbf{x}_i) = \\sigma(z_i)^{y_i} \\cdot (1 - \\sigma(z_i))^{1-y_i}, \\quad z_i = \\mathbf{w}^\\top \\mathbf{x}_i + b\n",
    "\\]\n",
    "\n",
    "Thus, the likelihood becomes:\n",
    "\\[\n",
    "L(\\mathbf{w}, b) = \\prod_{i=1}^n \\left[ \\sigma(z_i)^{y_i} \\cdot (1 - \\sigma(z_i))^{1-y_i} \\right]\n",
    "\\]\n",
    "\n",
    "#### **Log-Likelihood**\n",
    "Maximizing the likelihood directly is computationally challenging, so we take the log of the likelihood:\n",
    "\\[\n",
    "\\ell(\\mathbf{w}, b) = \\log L(\\mathbf{w}, b) = \\sum_{i=1}^n \\left[ y_i \\log \\sigma(z_i) + (1 - y_i) \\log (1 - \\sigma(z_i)) \\right]\n",
    "\\]\n",
    "\n",
    "This is the **log-likelihood function**.\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 4: Loss Function**\n",
    "Instead of maximizing the log-likelihood, we minimize the **negative log-likelihood**, also called the **cross-entropy loss**:\n",
    "\n",
    "\\[\n",
    "J(\\mathbf{w}, b) = -\\frac{1}{n} \\sum_{i=1}^n \\left[ y_i \\log \\hat{y}_i + (1 - y_i) \\log (1 - \\hat{y}_i) \\right]\n",
    "\\]\n",
    "\n",
    "Where:\n",
    "- \\( \\hat{y}_i = \\sigma(z_i) \\) is the predicted probability for \\( y_i=1 \\).\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 5: Optimization Using Gradient Descent**\n",
    "To minimize the loss function \\( J(\\mathbf{w}, b) \\), we use **gradient descent**.\n",
    "\n",
    "#### Gradients of the Loss\n",
    "1. **Gradient with respect to \\( \\mathbf{w} \\):**\n",
    "   \\[\n",
    "   \\frac{\\partial J}{\\partial \\mathbf{w}} = \\frac{1}{n} \\sum_{i=1}^n (\\hat{y}_i - y_i) \\mathbf{x}_i\n",
    "   \\]\n",
    "\n",
    "2. **Gradient with respect to \\( b \\):**\n",
    "   \\[\n",
    "   \\frac{\\partial J}{\\partial b} = \\frac{1}{n} \\sum_{i=1}^n (\\hat{y}_i - y_i)\n",
    "   \\]\n",
    "\n",
    "#### Parameter Update\n",
    "Using a learning rate \\( \\eta \\), the weights and bias are updated iteratively:\n",
    "\\[\n",
    "\\mathbf{w} \\leftarrow \\mathbf{w} - \\eta \\frac{\\partial J}{\\partial \\mathbf{w}}\n",
    "\\]\n",
    "\\[\n",
    "b \\leftarrow b - \\eta \\frac{\\partial J}{\\partial b}\n",
    "\\]\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 6: Decision Boundary**\n",
    "The decision boundary separates the two classes and is defined by:\n",
    "\\[\n",
    "\\mathbf{w}^\\top \\mathbf{x} + b = 0\n",
    "\\]\n",
    "\n",
    "Logistic regression predicts:\n",
    "- \\( y=1 \\) if \\( \\hat{y} = \\sigma(\\mathbf{w}^\\top \\mathbf{x} + b) \\geq 0.5 \\)\n",
    "- \\( y=0 \\) otherwise.\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 7: Regularization (Optional)**\n",
    "To prevent overfitting, we can add a regularization term to the loss function:\n",
    "\n",
    "1. **L2 Regularization (Ridge):**\n",
    "   \\[\n",
    "   J_{reg}(\\mathbf{w}, b) = J(\\mathbf{w}, b) + \\frac{\\lambda}{2} \\|\\mathbf{w}\\|_2^2\n",
    "   \\]\n",
    "\n",
    "2. **L1 Regularization (Lasso):**\n",
    "   \\[\n",
    "   J_{reg}(\\mathbf{w}, b) = J(\\mathbf{w}, b) + \\lambda \\|\\mathbf{w}\\|_1\n",
    "   \\]\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 8: Extensions**\n",
    "1. **Multinomial Logistic Regression**: Extends logistic regression to multi-class classification using the softmax function.\n",
    "2. **Regularized Logistic Regression**: Includes penalties to control model complexity.\n",
    "\n",
    "---\n",
    "\n",
    "### **Summary**\n",
    "1. Logistic regression models \\( P(y=1|\\mathbf{x}) \\) using \\( \\sigma(z) \\), where \\( z = \\mathbf{w}^\\top \\mathbf{x} + b \\).\n",
    "2. It uses the cross-entropy loss function to train the model.\n",
    "3. Parameters are optimized using gradient descent.\n",
    "4. The sigmoid function ensures the output is a probability.\n",
    "5. Regularization can be added to reduce overfitting.\n",
    "\n",
    "Let me know if you'd like an example implementation or more details on any step!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No worries! Let’s clarify the **logistic regression equation** and the **loss function** step by step. They serve different purposes in the model:\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Logistic Regression Equation**\n",
    "The logistic regression equation models the **relationship between the input features** and the **predicted probability** of a class.\n",
    "\n",
    "#### The Equation:\n",
    "\\[\n",
    "P(y=1|\\mathbf{x}) = \\hat{y} = \\sigma(z), \\quad z = \\mathbf{w}^\\top \\mathbf{x} + b\n",
    "\\]\n",
    "\n",
    "Where:\n",
    "- \\( z = \\mathbf{w}^\\top \\mathbf{x} + b \\): This is the **linear combination** of the input features \\( \\mathbf{x} \\), weights \\( \\mathbf{w} \\), and bias \\( b \\).\n",
    "- \\( \\sigma(z) = \\frac{1}{1 + e^{-z}} \\): This is the **sigmoid function**, which converts \\( z \\) into a probability between \\( 0 \\) and \\( 1 \\).\n",
    "\n",
    "#### What it does:\n",
    "The logistic regression equation predicts the **probability** \\( \\hat{y} \\) that \\( y=1 \\) given \\( \\mathbf{x} \\). \n",
    "\n",
    "- If \\( \\hat{y} \\geq 0.5 \\), predict \\( y=1 \\).\n",
    "- If \\( \\hat{y} < 0.5 \\), predict \\( y=0 \\).\n",
    "\n",
    "**Key Point**: The logistic equation itself does not measure how good or bad the predictions are. That’s where the loss function comes in.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Loss Function**\n",
    "The loss function quantifies the **error** between the predicted probabilities (\\( \\hat{y} \\)) and the actual labels (\\( y \\)).\n",
    "\n",
    "#### Why a Special Loss Function?\n",
    "- Logistic regression outputs probabilities, so we can’t use something like Mean Squared Error (MSE) as it doesn’t align well with probabilities.\n",
    "- Instead, we use **log-likelihood** or **cross-entropy loss** because they are better suited for probabilistic models.\n",
    "\n",
    "#### The Cross-Entropy Loss Function:\n",
    "\\[\n",
    "J(\\mathbf{w}, b) = -\\frac{1}{n} \\sum_{i=1}^n \\left[ y_i \\log \\hat{y}_i + (1 - y_i) \\log (1 - \\hat{y}_i) \\right]\n",
    "\\]\n",
    "\n",
    "Where:\n",
    "- \\( y_i \\): Actual label (\\( 0 \\) or \\( 1 \\)).\n",
    "- \\( \\hat{y}_i \\): Predicted probability from the logistic equation.\n",
    "- \\( n \\): Number of samples.\n",
    "\n",
    "#### Intuition:\n",
    "1. **If \\( y_i = 1 \\):**\n",
    "   \\[\n",
    "   \\text{Loss} = -\\log(\\hat{y}_i)\n",
    "   \\]\n",
    "   - When \\( \\hat{y}_i \\) (predicted probability) is close to 1, the loss is small.\n",
    "   - When \\( \\hat{y}_i \\) is far from 1 (close to 0), the loss is large.\n",
    "\n",
    "2. **If \\( y_i = 0 \\):**\n",
    "   \\[\n",
    "   \\text{Loss} = -\\log(1 - \\hat{y}_i)\n",
    "   \\]\n",
    "   - When \\( \\hat{y}_i \\) (predicted probability for \\( y=1 \\)) is close to 0, the loss is small.\n",
    "   - When \\( \\hat{y}_i \\) is far from 0 (close to 1), the loss is large.\n",
    "\n",
    "---\n",
    "\n",
    "### **Connecting the Two**\n",
    "1. **Logistic Equation**: \n",
    "   - Used to calculate \\( \\hat{y} \\), the predicted probability for \\( y=1 \\).\n",
    "   - Core equation: \\( \\hat{y} = \\sigma(\\mathbf{w}^\\top \\mathbf{x} + b) \\).\n",
    "\n",
    "2. **Loss Function**:\n",
    "   - Measures the error between \\( \\hat{y} \\) and the true label \\( y \\).\n",
    "   - Core equation: Cross-entropy loss \\( J(\\mathbf{w}, b) \\).\n",
    "\n",
    "---\n",
    "\n",
    "### **Example**\n",
    "Let’s go through an example for clarity.\n",
    "\n",
    "#### Inputs:\n",
    "- \\( \\mathbf{x} = [1.5, 2.0] \\), \\( \\mathbf{w} = [0.3, 0.5] \\), \\( b = -1.0 \\).\n",
    "- True label \\( y = 1 \\).\n",
    "\n",
    "#### Step 1: Compute \\( z \\):\n",
    "\\[\n",
    "z = \\mathbf{w}^\\top \\mathbf{x} + b = (0.3 \\cdot 1.5) + (0.5 \\cdot 2.0) + (-1.0) = 0.45 + 1.0 - 1.0 = 0.45\n",
    "\\]\n",
    "\n",
    "#### Step 2: Compute \\( \\hat{y} \\) using the sigmoid function:\n",
    "\\[\n",
    "\\hat{y} = \\sigma(z) = \\frac{1}{1 + e^{-0.45}} \\approx 0.61\n",
    "\\]\n",
    "\n",
    "#### Step 3: Compute the loss:\n",
    "Since \\( y = 1 \\), the loss is:\n",
    "\\[\n",
    "\\text{Loss} = -\\log(\\hat{y}) = -\\log(0.61) \\approx 0.49\n",
    "\\]\n",
    "\n",
    "If \\( y = 0 \\), the loss would be:\n",
    "\\[\n",
    "\\text{Loss} = -\\log(1 - \\hat{y}) = -\\log(1 - 0.61) = -\\log(0.39) \\approx 0.94\n",
    "\\]\n",
    "\n",
    "---\n",
    "\n",
    "### **Summary**\n",
    "- **Logistic Equation**: Computes probabilities.\n",
    "- **Loss Function**: Evaluates the quality of those probabilities.\n",
    "- **Optimization**: Minimizes the loss by adjusting \\( \\mathbf{w} \\) and \\( b \\).\n",
    "\n",
    "Let me know if you'd like further clarification!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Object detection in computer vision involves identifying and localizing objects in an image by drawing bounding boxes around them and assigning a class label. Here's a detailed, step-by-step explanation of how object detection works, including its loss functions and metrics.\n",
    "\n",
    "---\n",
    "\n",
    "## **1. Object Detection Overview**\n",
    "\n",
    "Object detection tasks can be broken into two primary components:\n",
    "- **Localization**: Identifying the precise location of the object (bounding box).\n",
    "- **Classification**: Assigning the correct class label to the object.\n",
    "\n",
    "Modern object detection models (like YOLO, Faster R-CNN, and SSD) predict:\n",
    "1. **Bounding boxes**: Represented as \\((x, y, w, h)\\), where \\(x, y\\) are the center coordinates, and \\(w, h\\) are the width and height.\n",
    "2. **Class probabilities**: The likelihood of the object belonging to each class.\n",
    "3. **Confidence score**: The likelihood that a bounding box contains an object.\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Core Components**\n",
    "\n",
    "### **2.1 Bounding Box Regression**\n",
    "The goal is to predict the coordinates of the bounding box \\((\\hat{x}, \\hat{y}, \\hat{w}, \\hat{h})\\) such that they align with the ground truth box \\((x, y, w, h)\\).\n",
    "\n",
    "#### Loss Function for Bounding Box Regression:\n",
    "One common approach is to use **Smooth L1 Loss** or **IoU-based Loss**:\n",
    "- **Smooth L1 Loss**:\n",
    "\\[\n",
    "\\mathcal{L}_{\\text{bbox}} = \n",
    "\\begin{cases} \n",
    "0.5 (\\Delta)^2 & \\text{if } |\\Delta| < 1 \\\\\n",
    "|\\Delta| - 0.5 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "\\]\n",
    "Where \\( \\Delta = \\hat{b} - b \\) is the difference between predicted and ground truth box coordinates.\n",
    "\n",
    "- **IoU Loss**:\n",
    "\\[\n",
    "\\mathcal{L}_{\\text{IoU}} = 1 - \\text{IoU}\n",
    "\\]\n",
    "Where IoU is:\n",
    "\\[\n",
    "\\text{IoU} = \\frac{|B_{\\text{pred}} \\cap B_{\\text{gt}}|}{|B_{\\text{pred}} \\cup B_{\\text{gt}}|}\n",
    "\\]\n",
    "\n",
    "---\n",
    "\n",
    "### **2.2 Classification**\n",
    "Each bounding box is assigned a class label. The goal is to minimize the error between predicted class probabilities \\(\\hat{p}_c\\) and ground truth labels \\(p_c\\).\n",
    "\n",
    "#### Loss Function for Classification:\n",
    "The standard loss used is **Cross-Entropy Loss** or **Focal Loss**:\n",
    "- **Cross-Entropy Loss**:\n",
    "\\[\n",
    "\\mathcal{L}_{\\text{cls}} = - \\sum_{c} p_c \\log(\\hat{p}_c)\n",
    "\\]\n",
    "Where \\(p_c\\) is the ground truth probability (1 for the correct class, 0 otherwise), and \\(\\hat{p}_c\\) is the predicted probability.\n",
    "\n",
    "- **Focal Loss** (for class imbalance):\n",
    "\\[\n",
    "\\mathcal{L}_{\\text{focal}} = - \\alpha (1 - \\hat{p}_c)^\\gamma p_c \\log(\\hat{p}_c)\n",
    "\\]\n",
    "Where \\(\\alpha\\) balances the importance of classes, and \\(\\gamma\\) focuses on hard-to-classify examples.\n",
    "\n",
    "---\n",
    "\n",
    "### **2.3 Objectness Score**\n",
    "To determine whether a bounding box contains an object, an **objectness score** is predicted. This score is optimized using **Binary Cross-Entropy Loss**:\n",
    "\\[\n",
    "\\mathcal{L}_{\\text{obj}} = - \\left[ p_o \\log(\\hat{p}_o) + (1 - p_o) \\log(1 - \\hat{p}_o) \\right]\n",
    "\\]\n",
    "Where:\n",
    "- \\(p_o\\): Ground truth (1 if the box contains an object, 0 otherwise).\n",
    "- \\(\\hat{p}_o\\): Predicted objectness score.\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Combined Loss Function**\n",
    "Modern detectors combine these components into a unified loss:\n",
    "\\[\n",
    "\\mathcal{L}_{\\text{total}} = \\lambda_{\\text{bbox}} \\mathcal{L}_{\\text{bbox}} + \\lambda_{\\text{cls}} \\mathcal{L}_{\\text{cls}} + \\lambda_{\\text{obj}} \\mathcal{L}_{\\text{obj}}\n",
    "\\]\n",
    "Where \\(\\lambda_{\\text{bbox}}, \\lambda_{\\text{cls}}, \\lambda_{\\text{obj}}\\) are weights to balance the contributions of each loss.\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Metrics for Object Detection**\n",
    "\n",
    "### **4.1 Intersection over Union (IoU)**\n",
    "IoU measures the overlap between the predicted bounding box \\(B_{\\text{pred}}\\) and the ground truth \\(B_{\\text{gt}}\\):\n",
    "\\[\n",
    "\\text{IoU} = \\frac{|B_{\\text{pred}} \\cap B_{\\text{gt}}|}{|B_{\\text{pred}} \\cup B_{\\text{gt}}|}\n",
    "\\]\n",
    "\n",
    "### **4.2 Mean Average Precision (mAP)**\n",
    "mAP is the primary metric for evaluating object detection models. It is calculated as the mean of the Average Precision (AP) across all classes.\n",
    "\n",
    "#### Average Precision (AP):\n",
    "AP is the area under the Precision-Recall curve:\n",
    "\\[\n",
    "\\text{Precision} = \\frac{\\text{TP}}{\\text{TP} + \\text{FP}}, \\quad \\text{Recall} = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}}\n",
    "\\]\n",
    "\n",
    "---\n",
    "\n",
    "## **5. Modern Variations**\n",
    "- **Generalized IoU (GIoU)**:\n",
    "Addresses cases where the predicted and ground truth boxes do not overlap:\n",
    "\\[\n",
    "\\text{GIoU} = \\text{IoU} - \\frac{|C - (B_{\\text{pred}} \\cup B_{\\text{gt}})|}{|C|}\n",
    "\\]\n",
    "Where \\(C\\) is the smallest enclosing box.\n",
    "\n",
    "- **Complete IoU (CIoU)**:\n",
    "Adds penalties for center distance and aspect ratio:\n",
    "\\[\n",
    "\\mathcal{L}_{\\text{CIoU}} = 1 - \\text{IoU} + \\frac{\\rho^2(\\hat{b}, b)}{c^2} + \\alpha v\n",
    "\\]\n",
    "Where:\n",
    "- \\(\\rho^2(\\hat{b}, b)\\): Distance between box centers.\n",
    "- \\(c^2\\): Diagonal length of the enclosing box.\n",
    "- \\(v\\): Aspect ratio difference.\n",
    "\n",
    "---\n",
    "\n",
    "## **6. Summary**\n",
    "Object detection combines:\n",
    "1. **Bounding Box Regression** (localization).\n",
    "2. **Classification** (class prediction).\n",
    "3. **Objectness Score** (confidence).\n",
    "\n",
    "Loss functions like Smooth L1, IoU, and Cross-Entropy optimize the model, while metrics like IoU and mAP evaluate its performance. Let me know if you'd like to dive deeper into any part!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Object tracking is the process of following an object across frames in a video sequence. Unlike object detection, which identifies objects in individual frames, tracking associates detected objects across frames to maintain consistent identities.\n",
    "\n",
    "Here’s an in-depth, step-by-step explanation of how object tracking works, including its methods, equations, and loss functions:\n",
    "\n",
    "---\n",
    "\n",
    "## **1. Object Tracking Overview**\n",
    "Object tracking typically involves two steps:\n",
    "1. **Detection**: Identify objects in each frame (e.g., bounding boxes and class labels).\n",
    "2. **Tracking**: Associate objects across frames, ensuring consistent IDs.\n",
    "\n",
    "There are two main types of tracking:\n",
    "- **Single Object Tracking (SOT)**: Focuses on tracking a single target in the video.\n",
    "- **Multi-Object Tracking (MOT)**: Tracks multiple objects simultaneously.\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Core Components**\n",
    "\n",
    "### **2.1 Detection (Initialization)**\n",
    "Tracking often begins with object detection. Detectors like YOLO, Faster R-CNN, or RetinaNet provide bounding boxes, class labels, and confidence scores. These serve as the starting point for tracking.\n",
    "\n",
    "---\n",
    "\n",
    "### **2.2 Association**\n",
    "The key challenge in tracking is associating detections across frames. This involves matching objects in the current frame with objects in the previous frame.\n",
    "\n",
    "#### Matching Strategy:\n",
    "- **IoU-based Matching**: Use Intersection over Union (IoU) to measure overlap between bounding boxes:\n",
    "\\[\n",
    "\\text{IoU} = \\frac{|B_{\\text{pred}} \\cap B_{\\text{gt}}|}{|B_{\\text{pred}} \\cup B_{\\text{gt}}|}\n",
    "\\]\n",
    "Objects with higher IoU are considered matches.\n",
    "\n",
    "- **Feature Similarity**: Compare appearance features (e.g., extracted using CNNs) between objects:\n",
    "\\[\n",
    "\\text{Similarity} = \\cos(\\theta) = \\frac{\\mathbf{f}_1 \\cdot \\mathbf{f}_2}{\\|\\mathbf{f}_1\\| \\|\\mathbf{f}_2\\|}\n",
    "\\]\n",
    "Where \\( \\mathbf{f}_1, \\mathbf{f}_2 \\) are feature vectors of the objects.\n",
    "\n",
    "- **Motion Prediction**: Use Kalman Filters or Optical Flow to predict object positions in the next frame and match detections based on proximity.\n",
    "\n",
    "---\n",
    "\n",
    "### **2.3 Temporal Tracking**\n",
    "After associating objects across frames, trackers maintain consistency using motion models.\n",
    "\n",
    "#### Kalman Filter:\n",
    "A Kalman Filter predicts the future position of an object based on its current state (position, velocity). It consists of:\n",
    "1. **Prediction Step**:\n",
    "\\[\n",
    "\\mathbf{\\hat{x}}_{t|t-1} = \\mathbf{F} \\mathbf{x}_{t-1} + \\mathbf{B} \\mathbf{u}_{t-1}\n",
    "\\]\n",
    "\\[\n",
    "\\mathbf{\\hat{P}}_{t|t-1} = \\mathbf{F} \\mathbf{P}_{t-1} \\mathbf{F}^T + \\mathbf{Q}\n",
    "\\]\n",
    "2. **Update Step**:\n",
    "\\[\n",
    "\\mathbf{K}_t = \\mathbf{\\hat{P}}_{t|t-1} \\mathbf{H}^T (\\mathbf{H} \\mathbf{\\hat{P}}_{t|t-1} \\mathbf{H}^T + \\mathbf{R})^{-1}\n",
    "\\]\n",
    "\\[\n",
    "\\mathbf{x}_t = \\mathbf{\\hat{x}}_{t|t-1} + \\mathbf{K}_t (\\mathbf{z}_t - \\mathbf{H} \\mathbf{\\hat{x}}_{t|t-1})\n",
    "\\]\n",
    "\\[\n",
    "\\mathbf{P}_t = (\\mathbf{I} - \\mathbf{K}_t \\mathbf{H}) \\mathbf{\\hat{P}}_{t|t-1}\n",
    "\\]\n",
    "Where:\n",
    "- \\( \\mathbf{x} \\): State vector (position, velocity).\n",
    "- \\( \\mathbf{P} \\): Covariance matrix.\n",
    "- \\( \\mathbf{F} \\): State transition matrix.\n",
    "- \\( \\mathbf{K} \\): Kalman gain.\n",
    "- \\( \\mathbf{Q}, \\mathbf{R} \\): Process and measurement noise.\n",
    "\n",
    "---\n",
    "\n",
    "### **2.4 Re-Identification (ReID)**\n",
    "If an object temporarily disappears (e.g., occlusion), ReID systems match it to a previously tracked object when it reappears. This involves:\n",
    "- **Feature Embedding**: Extracting discriminative features (e.g., appearance, color, texture).\n",
    "- **Similarity Matching**: Comparing features of the reappearing object with previously tracked objects.\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Loss Functions**\n",
    "\n",
    "### **3.1 Detection Loss**\n",
    "Object tracking relies on good detections, so detection loss functions (e.g., cross-entropy, IoU loss) apply.\n",
    "\n",
    "### **3.2 ReID Loss**\n",
    "To train ReID models, loss functions like Triplet Loss or Softmax Cross-Entropy Loss are used:\n",
    "- **Triplet Loss**:\n",
    "\\[\n",
    "\\mathcal{L}_{\\text{triplet}} = \\max(0, \\|\\mathbf{f}_a - \\mathbf{f}_p\\|_2^2 - \\|\\mathbf{f}_a - \\mathbf{f}_n\\|_2^2 + \\alpha)\n",
    "\\]\n",
    "Where:\n",
    "- \\( \\mathbf{f}_a \\): Anchor embedding.\n",
    "- \\( \\mathbf{f}_p \\): Positive embedding (same object).\n",
    "- \\( \\mathbf{f}_n \\): Negative embedding (different object).\n",
    "- \\( \\alpha \\): Margin.\n",
    "\n",
    "- **Cross-Entropy Loss**:\n",
    "\\[\n",
    "\\mathcal{L}_{\\text{cls}} = - \\sum_{c} p_c \\log(\\hat{p}_c)\n",
    "\\]\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Metrics for Object Tracking**\n",
    "\n",
    "### **4.1 Multi-Object Tracking Accuracy (MOTA)**\n",
    "\\[\n",
    "\\text{MOTA} = 1 - \\frac{\\sum_t (\\text{FP}_t + \\text{FN}_t + \\text{ID Switches}_t)}{\\sum_t \\text{GT}_t}\n",
    "\\]\n",
    "Where:\n",
    "- FP: False positives.\n",
    "- FN: False negatives.\n",
    "- ID Switches: Incorrect reassignments of object IDs.\n",
    "- GT: Ground truth objects.\n",
    "\n",
    "### **4.2 Multi-Object Tracking Precision (MOTP)**\n",
    "\\[\n",
    "\\text{MOTP} = \\frac{\\sum_{i, t} d_{i, t}}{\\sum_t c_t}\n",
    "\\]\n",
    "Where:\n",
    "- \\( d_{i, t} \\): Distance between predicted and ground truth objects.\n",
    "- \\( c_t \\): Number of matches at time \\( t \\).\n",
    "\n",
    "---\n",
    "\n",
    "## **5. Types of Object Tracking**\n",
    "\n",
    "### **5.1 Online Tracking**\n",
    "Tracks objects in real-time, without future frame information. Algorithms:\n",
    "- SORT (Simple Online and Realtime Tracking)\n",
    "- DeepSORT (SORT with ReID)\n",
    "\n",
    "### **5.2 Offline Tracking**\n",
    "Uses all frames for post-hoc analysis. Algorithms:\n",
    "- Tracktor\n",
    "- CRNN-based trackers.\n",
    "\n",
    "---\n",
    "\n",
    "## **6. Summary**\n",
    "Object tracking involves:\n",
    "1. **Detection**: Initializing objects using bounding boxes.\n",
    "2. **Association**: Matching objects across frames using IoU, features, or motion models.\n",
    "3. **ReID**: Handling occlusions and reappearing objects.\n",
    "4. **Optimization**: Using loss functions like Triplet Loss and Cross-Entropy.\n",
    "\n",
    "Metrics like MOTA and MOTP evaluate performance. Let me know if you want to dive into specific tracking algorithms!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
