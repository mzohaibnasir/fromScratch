{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# matrix multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset length: 10000\n",
      "Images shape: (10000, 28, 28)\n",
      "Labels shape: (10000,)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Download and load the MNIST dataset\n",
    "mnist = datasets.MNIST(root=\"./data\", train=False, download=True)\n",
    "\n",
    "# Print the length of the dataset   \n",
    "print(\"Dataset length:\", len(mnist))\n",
    "\n",
    "# Convert the entire dataset to NumPy arrays\n",
    "images_np = []\n",
    "labels_np = []\n",
    "\n",
    "for i in range(len(mnist)):\n",
    "    image, label = mnist[i]\n",
    "    # Convert the image to a NumPy array\n",
    "    image_np = transforms.ToTensor()(image).numpy()[0]\n",
    "    images_np.append(image_np)\n",
    "    labels_np.append(label)\n",
    "\n",
    "# Convert lists to NumPy arrays\n",
    "images_np = np.array(images_np)\n",
    "labels_np = np.array(labels_np)\n",
    "\n",
    "# Print the shapes of the arrays\n",
    "print(\"Images shape:\", images_np.shape)\n",
    "print(\"Labels shape:\", labels_np.shape)\n",
    "\n",
    "# Access the first 5 images and labels\n",
    "first_5_images = images_np[:5]\n",
    "first_5_labels = labels_np[:5]\n",
    "\n",
    "# Plot the first 5 images\n",
    "fig, axes = plt.subplots(1, 5, figsize=(15, 5))\n",
    "\n",
    "for i in range(5):\n",
    "    axes[i].imshow(first_5_images[i], cmap='gray')  # Display in grayscale\n",
    "    axes[i].set_title(f\"Label: {first_5_labels[i]}\")\n",
    "    axes[i].axis('off')  # Turn off axis labels\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x722b192d1810>"
      ]
     },
     "execution_count": 583,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "images_np[0].shape\n",
    "plt.imshow(images_np[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Matrix multiplication\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "weights = torch.randn(784, 10)\n",
    "bias = torch.zeros(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 784)"
      ]
     },
     "execution_count": 585,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m1 = images_np[:5].reshape(5, -1)\n",
    "m1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 784])"
      ]
     },
     "execution_count": 586,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(images_np[:5]).view(5, -1).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5, 784), torch.Size([784, 10]))"
      ]
     },
     "execution_count": 587,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m2 = weights\n",
    "\n",
    "m1.shape, m2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5, 784), (784, 10))"
      ]
     },
     "execution_count": 588,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ar, ac = m1.shape\n",
    "br, bc = m2.shape\n",
    "\n",
    "(ar, ac), (br, bc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 10])"
      ]
     },
     "execution_count": 589,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# resultant tensor\n",
    "\n",
    "t1 = torch.zeros(ar, bc)\n",
    "t1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -9.9532,  -3.8880,  -5.3230,   6.7895,  -5.3972,  -6.1437, -20.9652,\n",
       "           0.1295,  -4.8179,   1.9989],\n",
       "        [ -2.7911,  -5.1074,   2.5032,  -3.7225,  10.4510,  -5.7416,  -5.2013,\n",
       "           0.5966, -22.2556,  18.8180],\n",
       "        [  0.7113,  -3.2732,  -9.6388,   3.6244, -10.4286,  -2.6716, -12.8279,\n",
       "           0.9658,   3.3336,  -1.5013],\n",
       "        [  2.5451,  -1.3300,  -4.7280, -19.5480,  17.2274,  -2.3682, -20.7707,\n",
       "         -11.7523, -37.0048,  13.2327],\n",
       "        [ -8.1555,   0.4993,   1.6285, -11.3670,   8.5295,  -1.0342, -10.2872,\n",
       "         -13.1507,   1.6778,   4.0687]])"
      ]
     },
     "execution_count": 590,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(ar): #5 each row one at aa time\n",
    "    for j in range(bc): #10  each column one ata time\n",
    "        for k in range(ac): # 784\n",
    "            t1[i,j] += m1[i, k] * m2[k,j]\n",
    "\n",
    "t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.38 s, sys: 0 ns, total: 1.38 s\n",
      "Wall time: 1.37 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ -9.9532,  -3.8880,  -5.3230,   6.7895,  -5.3972,  -6.1437, -20.9652,\n",
       "           0.1295,  -4.8179,   1.9989],\n",
       "        [ -2.7911,  -5.1074,   2.5032,  -3.7225,  10.4510,  -5.7416,  -5.2013,\n",
       "           0.5966, -22.2556,  18.8180],\n",
       "        [  0.7113,  -3.2732,  -9.6388,   3.6244, -10.4286,  -2.6716, -12.8279,\n",
       "           0.9658,   3.3336,  -1.5013],\n",
       "        [  2.5451,  -1.3300,  -4.7280, -19.5480,  17.2274,  -2.3682, -20.7707,\n",
       "         -11.7523, -37.0048,  13.2327],\n",
       "        [ -8.1555,   0.4993,   1.6285, -11.3670,   8.5295,  -1.0342, -10.2872,\n",
       "         -13.1507,   1.6778,   4.0687]])"
      ]
     },
     "execution_count": 591,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def matmul(a,b):\n",
    "    (ar,ac), (br,bc) = a.shape, b.shape\n",
    "    c= torch.zeros(ar,bc)\n",
    "    for i in range(ar): #5 each row one at aa time\n",
    "        for j in range(bc): #10  each column one ata time\n",
    "            for k in range(ac): # 784\n",
    "                c[i,j] += a[i, k] * b[k,j]\n",
    "\n",
    "    return c\n",
    "%time matmul(m1,m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numba tak python adn turns it into machime cpde\n",
    "from numba import njit\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit\n",
    "def dot(a,b):\n",
    "    res = 0\n",
    "    for i in range(len(a)): res+=a[i]*b[i]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 120 ms, sys: 986 µs, total: 121 ms\n",
      "Wall time: 120 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "14.0"
      ]
     },
     "execution_count": 594,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time dot(np.array([1. , 2., 3.]), np.array([1. , 2., 3.]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 35 µs, sys: 1e+03 ns, total: 36 µs\n",
      "Wall time: 40.5 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "14.0"
      ]
     },
     "execution_count": 595,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time dot(np.array([1. , 2., 3.]), np.array([1. , 2., 3.]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 43 µs, sys: 2 µs, total: 45 µs\n",
      "Wall time: 52 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10607.0"
      ]
     },
     "execution_count": 596,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time dot(np.array([1. , 2., 3.,8., 99.]), np.array([1. , 2., 3.,99.,99.]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# first time it takes  to compile than it works at c's speed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now replicate that function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -9.9532,  -3.8880,  -5.3230,   6.7895,  -5.3972,  -6.1437, -20.9652,\n",
       "           0.1295,  -4.8179,   1.9989],\n",
       "        [ -2.7911,  -5.1074,   2.5032,  -3.7225,  10.4510,  -5.7416,  -5.2013,\n",
       "           0.5966, -22.2556,  18.8180],\n",
       "        [  0.7113,  -3.2732,  -9.6388,   3.6244, -10.4286,  -2.6716, -12.8279,\n",
       "           0.9658,   3.3336,  -1.5013],\n",
       "        [  2.5451,  -1.3300,  -4.7280, -19.5480,  17.2274,  -2.3682, -20.7707,\n",
       "         -11.7523, -37.0048,  13.2327],\n",
       "        [ -8.1555,   0.4993,   1.6285, -11.3670,   8.5295,  -1.0342, -10.2872,\n",
       "         -13.1507,   1.6778,   4.0687]])"
      ]
     },
     "execution_count": 597,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def matmul_njit(a,b):\n",
    "    (ar,ac), (br,bc) = a.shape, b.shape\n",
    "    c= torch.zeros(ar,bc)\n",
    "    for i in range(ar): #5 each row one at aa time\n",
    "        for j in range(bc): #10  each column one ata time\n",
    "            c[i,j] = dot(a[i, :], b[:,j])\n",
    "\n",
    "    return c\n",
    "# matmul_njit(torch.tensor(m1),torch.tensor(m2))\n",
    "matmul_njit(np.array(m1),np.array(m2))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.78 ms, sys: 995 µs, total: 2.77 ms\n",
      "Wall time: 2.04 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ -9.9532,  -3.8880,  -5.3230,   6.7895,  -5.3972,  -6.1437, -20.9652,\n",
       "           0.1295,  -4.8179,   1.9989],\n",
       "        [ -2.7911,  -5.1074,   2.5032,  -3.7225,  10.4510,  -5.7416,  -5.2013,\n",
       "           0.5966, -22.2556,  18.8180],\n",
       "        [  0.7113,  -3.2732,  -9.6388,   3.6244, -10.4286,  -2.6716, -12.8279,\n",
       "           0.9658,   3.3336,  -1.5013],\n",
       "        [  2.5451,  -1.3300,  -4.7280, -19.5480,  17.2274,  -2.3682, -20.7707,\n",
       "         -11.7523, -37.0048,  13.2327],\n",
       "        [ -8.1555,   0.4993,   1.6285, -11.3670,   8.5295,  -1.0342, -10.2872,\n",
       "         -13.1507,   1.6778,   4.0687]])"
      ]
     },
     "execution_count": 598,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time matmul_njit(np.array(m1),np.array(m2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 997 ms, sys: 1.86 ms, total: 999 ms\n",
      "Wall time: 998 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ -9.9532,  -3.8880,  -5.3230,   6.7895,  -5.3972,  -6.1437, -20.9652,\n",
       "           0.1295,  -4.8179,   1.9989],\n",
       "        [ -2.7911,  -5.1074,   2.5032,  -3.7225,  10.4510,  -5.7416,  -5.2013,\n",
       "           0.5966, -22.2556,  18.8180],\n",
       "        [  0.7113,  -3.2732,  -9.6388,   3.6244, -10.4286,  -2.6716, -12.8279,\n",
       "           0.9658,   3.3336,  -1.5013],\n",
       "        [  2.5451,  -1.3300,  -4.7280, -19.5480,  17.2274,  -2.3682, -20.7707,\n",
       "         -11.7523, -37.0048,  13.2327],\n",
       "        [ -8.1555,   0.4993,   1.6285, -11.3670,   8.5295,  -1.0342, -10.2872,\n",
       "         -13.1507,   1.6778,   4.0687]])"
      ]
     },
     "execution_count": 599,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time matmul(np.array(m1),np.array(m2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# elementwise operation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([10.,  6., -4.]), tensor([2., 8., 7.]))"
      ]
     },
     "execution_count": 600,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([10., 6., -4.])\n",
    "b = torch.tensor([2., 8, 7])\n",
    "a,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([12., 14.,  3.])"
      ]
     },
     "execution_count": 601,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6667)"
      ]
     },
     "execution_count": 602,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(a<b).float().mean()                                                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_35601/1036213963.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  m2 = torch.tensor(m2)\n"
     ]
    }
   ],
   "source": [
    "m1 = torch.tensor(m1)\n",
    "m2 = torch.tensor(m2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 784]), torch.Size([784, 10]), torch.float32, torch.float32)"
      ]
     },
     "execution_count": 604,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m1.shape, m2.shape, m1.dtype, m2.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 10])"
      ]
     },
     "execution_count": 605,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def matmul_ew(a,b):\n",
    "    (ar, ac), (br,bc) = a.shape, b.shape\n",
    "    c = torch.zeros(ar, bc)\n",
    "    for i in range(ar):\n",
    "        for j in range(bc): c[i,j] = (a[i,:] * b[:,j]).sum()\n",
    "    return c\n",
    "matmul_ew(m1,m2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %timeit -n 50 _=matmul(m1,m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.75 ms ± 34.9 µs per loop (mean ± std. dev. of 7 runs, 50 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 50 _=matmul_ew(m1,m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 10])"
      ]
     },
     "execution_count": 608,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def matmul_tdot(a,b):\n",
    "    (ar, ac), (br,bc) = a.shape, b.shape\n",
    "    c = torch.zeros(ar, bc)\n",
    "    for i in range(ar):\n",
    "        for j in range(bc): c[i,j] = torch.dot(a[i,:],  b[:,j])\n",
    "    return c\n",
    "matmul_tdot(m1,m2).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.46 ms ± 121 µs per loop (mean ± std. dev. of 7 runs, 50 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 50 _=matmul_tdot(m1,m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Broadcasting\n",
    "1. How arrays with different shapes are treated during arithmetic operations.\n",
    "2. THe smaller array is broadcast across larger array, so that they have ompatible shape.\n",
    "3. Boadcasting provides a mean of vactorizing array operations so that looping occurs in C instead of python.\n",
    "4. It does this without making needless copies of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([10.,  6., -4.])"
      ]
     },
     "execution_count": 610,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a # rank one tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ True,  True, False]), tensor([ True,  True, False]))"
      ]
     },
     "execution_count": 611,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a>0, a> torch.tensor([0,0,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`a>0` : 0 is being broadcasted here to have same dimensio as a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## broadcasting a vector to matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m = torch.randint(0,9, (3,3));m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1, 2, 3],\n",
       "         [4, 5, 6],\n",
       "         [7, 8, 9]]),\n",
       " torch.Size([3, 3]))"
      ]
     },
     "execution_count": 613,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = torch.arange(1, 10).reshape(3, 3);m, m.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([10., 20., 30.]),\n",
       " torch.Size([3]),\n",
       " torch.Size([1, 3]),\n",
       " torch.Size([3, 1]))"
      ]
     },
     "execution_count": 614,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c= torch.tensor([10, 20, 30], dtype=torch.float32);c, c.shape, c.unsqueeze(0).shape, c.unsqueeze(1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[11., 22., 33.],\n",
       "         [14., 25., 36.],\n",
       "         [17., 28., 39.]]),\n",
       " tensor([[11., 22., 33.],\n",
       "         [14., 25., 36.],\n",
       "         [17., 28., 39.]]))"
      ]
     },
     "execution_count": 615,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m+c, m+c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 616,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[11., 22., 33.],\n",
       "         [14., 25., 36.],\n",
       "         [17., 28., 39.]]),\n",
       " torch.Size([3, 3]))"
      ]
     },
     "execution_count": 616,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m+c.unsqueeze(0), (m+c.unsqueeze(0)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[11., 12., 13.],\n",
       "         [24., 25., 26.],\n",
       "         [37., 38., 39.]]),\n",
       " torch.Size([3, 3]))"
      ]
     },
     "execution_count": 617,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m+c.unsqueeze(1), (m+c.unsqueeze(1)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 618,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[11., 12., 13.],\n",
       "         [24., 25., 26.],\n",
       "         [37., 38., 39.]]),\n",
       " torch.Size([3, 3]))"
      ]
     },
     "execution_count": 618,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.unsqueeze(1)+m, (c.unsqueeze(1) +m ).shape   # m+c = c+m  ; order doesn't matter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### what actually happens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3]), torch.Size([3, 3]))"
      ]
     },
     "execution_count": 619,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.shape, m.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[10., 20., 30.],\n",
       "         [10., 20., 30.],\n",
       "         [10., 20., 30.]]),\n",
       " torch.Size([3, 3]))"
      ]
     },
     "execution_count": 620,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t= c.expand_as(m);t, t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 621,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 10.0\n",
       " 20.0\n",
       " 30.0\n",
       "[torch.storage.TypedStorage(dtype=torch.float32, device=cpu) of size 3]"
      ]
     },
     "execution_count": 621,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# t actually didn't create 3 copies\n",
    "t.storage()  # it stil contains (1,3) shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 622,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((0, 1), torch.Size([3, 3]), torch.Size([3]))"
      ]
     },
     "execution_count": 622,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.stride(), t.shape, c.shape # for every step, 0 across rows, and 1 across column;\n",
    "# stride (0,1) because expansion is acrross cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 623,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]]),\n",
       " tensor(0),\n",
       "  0\n",
       " [torch.storage.TypedStorage(dtype=torch.int64, device=cpu) of size 1],\n",
       " (0, 0))"
      ]
     },
     "execution_count": 623,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z =torch.tensor(0).expand_as(m); z, torch.tensor(0), z.storage(), z.stride()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 624,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 3]), torch.Size([1, 3]), torch.Size([1, 3]))"
      ]
     },
     "execution_count": 624,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.unsqueeze(0).shape, c[None,:].shape, c[None].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 625,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 1]), torch.Size([3, 1]))"
      ]
     },
     "execution_count": 625,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.unsqueeze(1).shape, c[:, None].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 626,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 3]), torch.Size([3, 1, 3]), torch.Size([3, 1, 1, 3]))"
      ]
     },
     "execution_count": 626,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.shape, m[:, None,:].shape, m[:, None,:][:,None,:,:].shape  # inserting dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 1, 1, 3]),\n",
       " torch.Size([3, 1, 1, 3, 1]),\n",
       " torch.Size([1, 3, 1, 1, 3]))"
      ]
     },
     "execution_count": 627,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = m[:, None,:][:,None,:,:]; x.shape, x[..., None].shape, x[None, ...].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 628,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 1]), torch.Size([3, 3]))"
      ]
     },
     "execution_count": 628,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c[:,None].shape, c[:,None].expand_as(m).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 3]), torch.Size([3, 3]))"
      ]
     },
     "execution_count": 629,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c[None].shape, c[None].expand_as(m).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[11., 12., 13.],\n",
       "         [24., 25., 26.],\n",
       "         [37., 38., 39.]]),\n",
       " tensor([[11., 12., 13.],\n",
       "         [24., 25., 26.],\n",
       "         [37., 38., 39.]]))"
      ]
     },
     "execution_count": 630,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c[:,None].expand_as(m) +m , m+c[:,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 631,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[11., 22., 33.],\n",
       "         [14., 25., 36.],\n",
       "         [17., 28., 39.]]),\n",
       " tensor([[11., 22., 33.],\n",
       "         [14., 25., 36.],\n",
       "         [17., 28., 39.]]))"
      ]
     },
     "execution_count": 631,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c[None].expand_as(m)+m, m+c[None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 632,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([10., 20., 30.])"
      ]
     },
     "execution_count": 632,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Broadcasting Rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 633,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[10., 20., 30.]]),\n",
       " tensor([[10.],\n",
       "         [20.],\n",
       "         [30.]]))"
      ]
     },
     "execution_count": 633,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c[None, :], c[:,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 634,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 3]), torch.Size([3, 1]))"
      ]
     },
     "execution_count": 634,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c[None, :].shape, c[:,None].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 635,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[100., 200., 300.],\n",
       "        [200., 400., 600.],\n",
       "        [300., 600., 900.]])"
      ]
     },
     "execution_count": 635,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c[None,:] * c[:,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 636,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 3]), torch.Size([3, 1]), torch.Size([3, 3]))"
      ]
     },
     "execution_count": 636,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c[None,:].shape, c[:,None].shape, (c[None,:] * c[:,None]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1]), torch.Size([3, 3]))"
      ]
     },
     "execution_count": 637,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.matmul(c[None:], c[:,None]).shape, (c[None,:] * c[:,None]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 638,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 3]), torch.Size([3, 1]), torch.Size([3, 3]))"
      ]
     },
     "execution_count": 638,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c[None,:].shape, c[:,None].shape, (c[None,:] * c[:,None]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 639,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[10., 20., 30.]]),\n",
       " tensor([[10.],\n",
       "         [20.],\n",
       "         [30.]]),\n",
       " tensor([[100., 200., 300.],\n",
       "         [200., 400., 600.],\n",
       "         [300., 600., 900.]]))"
      ]
     },
     "execution_count": 639,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c[None,:], c[:,None], c[None,:] * c[:,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 640,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 70.,  80.,  90.],\n",
       "        [140., 160., 180.],\n",
       "        [210., 240., 270.]])"
      ]
     },
     "execution_count": 640,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a= torch.tensor([7,8,9])\n",
    "\n",
    "a[None,:]*c[:,None]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 641,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 3]), torch.Size([3, 1]))"
      ]
     },
     "execution_count": 641,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[None,:].shape,c[:,None].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 642,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 70.,  80.,  90.],\n",
       "        [140., 160., 180.],\n",
       "        [210., 240., 270.]])"
      ]
     },
     "execution_count": 642,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[None,:]*c[:,None]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 643,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[7, 8, 9],\n",
       "         [7, 8, 9],\n",
       "         [7, 8, 9]]),\n",
       " tensor([[10., 10., 10.],\n",
       "         [20., 20., 20.],\n",
       "         [30., 30., 30.]]),\n",
       " tensor([[ 70.,  80.,  90.],\n",
       "         [140., 160., 180.],\n",
       "         [210., 240., 270.]]))"
      ]
     },
     "execution_count": 643,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(a[None,:]).expand_as(m) , (c[:,None]).expand_as(m), (a[None,:]).expand_as(m) * (c[:,None]).expand_as(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 644,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 70., 140., 210.],\n",
       "        [ 80., 160., 240.],\n",
       "        [ 90., 180., 270.]])"
      ]
     },
     "execution_count": 644,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[:, None]*c[None,:]   # c[None,:] will be multiplied to every element of a[:,None] one by one\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 645,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[7, 7, 7],\n",
       "         [8, 8, 8],\n",
       "         [9, 9, 9]]),\n",
       " tensor([[10., 20., 30.],\n",
       "         [10., 20., 30.],\n",
       "         [10., 20., 30.]]),\n",
       " tensor([[ 70., 140., 210.],\n",
       "         [ 80., 160., 240.],\n",
       "         [ 90., 180., 270.]]))"
      ]
     },
     "execution_count": 645,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this actually happens\n",
    "(a[:, None]).expand_as(m) , (c[None,:]).expand_as(m), (a[:, None]).expand_as(m) * (c[None,:]).expand_as(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 646,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False,  True,  True],\n",
       "        [False, False,  True],\n",
       "        [False, False, False]])"
      ]
     },
     "execution_count": 646,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c[None,:] >c[:,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 647,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[7, 7, 7],\n",
       "         [8, 8, 8],\n",
       "         [9, 9, 9]]),\n",
       " tensor([[10., 20., 30.],\n",
       "         [10., 20., 30.],\n",
       "         [10., 20., 30.]]),\n",
       " tensor([[False, False, False],\n",
       "         [False, False, False],\n",
       "         [False, False, False]]))"
      ]
     },
     "execution_count": 647,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(a[:, None]).expand_as(m) , (c[None,:]).expand_as(m), (a[:, None]).expand_as(m) >(c[None,:]).expand_as(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### when operating on two arrays/tensors, numpy/pytorch  compares their shapes element-wise. It starts with the trailing dimensions, work its way forward. Two dimensions are compatible when\n",
    "1. they are equal\n",
    "2. one of them is 1, in which case that dimension is broadcasted to make it the same size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# matmul with broadcasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 648,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 784]), torch.Size([784, 10]))"
      ]
     },
     "execution_count": 648,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m1.shape, m2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 649,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([784])"
      ]
     },
     "execution_count": 649,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digit = m1[0]\n",
    "digit.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 650,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([784, 1]), torch.Size([784, 10]))"
      ]
     },
     "execution_count": 650,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digit[:, None].shape, m2.shape  # maltipying digit with each 784 vector in m2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 651,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([784, 10])"
      ]
     },
     "execution_count": 651,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digit[:,None].expand_as(m2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 652,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([784, 10])"
      ]
     },
     "execution_count": 652,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(digit[:, None]*m2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 653,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "203 µs ± 12 µs per loop (mean ± std. dev. of 7 runs, 50 loops each)\n"
     ]
    }
   ],
   "source": [
    "def matmul_broadcasted(a,b):\n",
    "    (ar, ac), (br,bc) = a.shape, b.shape\n",
    "\n",
    "    c = torch.zeros(ar, bc)\n",
    "    for i in range(ar):\n",
    "        # print(f\"\\na:{a[i].shape} ||||| a[i, :, None]:{a[1, :, None].shape} ||||| b[i]:{b.shape}\")\n",
    "\n",
    "        # c[i,j] = (a[i,:] * b[:,j]).sum() # previous version\n",
    "        c[i] = (a[i, :, None] *b).sum(dim=0) # broadcast version\n",
    "    return c\n",
    "%timeit -n 50 _=matmul_broadcasted(m1,m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, that's a great summary of how broadcasting works in both NumPy and PyTorch! When performing operations on two tensors (or arrays), their shapes are compared element-wise starting from the last dimension (the rightmost one) and moving left. Here's how broadcasting works in detail:\n",
    "\n",
    "### Broadcasting Rules\n",
    "\n",
    "When comparing dimensions of two tensors for broadcasting, the following rules apply:\n",
    "\n",
    "1. **Equal Dimensions**:\n",
    "   - If the dimensions of both tensors are equal, they are compatible and can be directly used in the operation.\n",
    "   \n",
    "   Example:\n",
    "   ```python\n",
    "   Tensor A: shape (3, 4)\n",
    "   Tensor B: shape (3, 4)\n",
    "   Result: Compatible (element-wise operation proceeds)\n",
    "   ```\n",
    "\n",
    "2. **One Dimension is 1**:\n",
    "   - If one of the dimensions is 1, that dimension can be broadcasted (expanded) to match the size of the other dimension.\n",
    "   \n",
    "   Example:\n",
    "   ```python\n",
    "   Tensor A: shape (3, 4)\n",
    "   Tensor B: shape (1, 4)  # Can be broadcasted to (3, 4)\n",
    "   Result: Compatible (element-wise operation proceeds)\n",
    "   ```\n",
    "\n",
    "3. **Trailing Dimensions**:\n",
    "   - The comparison starts with the last dimension (rightmost) and moves to the left. Dimensions are compared one by one according to the rules above.\n",
    "\n",
    "### Examples to Illustrate Broadcasting\n",
    "\n",
    "#### Example 1: Compatible Shapes\n",
    "```python\n",
    "import torch\n",
    "\n",
    "a = torch.tensor([[1, 2, 3], [4, 5, 6]])  # Shape: (2, 3)\n",
    "b = torch.tensor([[1], [2]])  # Shape: (2, 1)\n",
    "\n",
    "# Broadcasting b to match the shape of a:\n",
    "# b becomes: [[1, 1, 1], [2, 2, 2]]\n",
    "\n",
    "result = a * b\n",
    "print(result)\n",
    "```\n",
    "\n",
    "**Output**:\n",
    "```\n",
    "tensor([[ 1,  2,  3],\n",
    "        [ 8, 10, 12]])\n",
    "```\n",
    "\n",
    "- `a` has shape `(2, 3)`.\n",
    "- `b` has shape `(2, 1)` and is broadcasted to shape `(2, 3)`.\n",
    "\n",
    "#### Example 2: Incompatible Shapes\n",
    "```python\n",
    "import torch\n",
    "\n",
    "a = torch.tensor([1, 2, 3])  # Shape: (3,)\n",
    "b = torch.tensor([[1, 2], [3, 4], [5, 6]])  # Shape: (3, 2)\n",
    "\n",
    "# These shapes are incompatible because their dimensions can't be broadcasted together.\n",
    "# The second dimension of a is missing, and it can't be broadcasted to match b.\n",
    "\n",
    "# The following would raise an error:\n",
    "# result = a * b\n",
    "```\n",
    "\n",
    "This will raise an error because:\n",
    "\n",
    "- `a` has shape `(3,)`.\n",
    "- `b` has shape `(3, 2)`.\n",
    "- They are incompatible because the second dimension of `a` is missing, and broadcasting rules cannot apply.\n",
    "\n",
    "#### Example 3: Scalar Broadcasting\n",
    "```python\n",
    "import torch\n",
    "\n",
    "a = torch.tensor([1, 2, 3])  # Shape: (3,)\n",
    "b = torch.tensor(2)  # Shape: ()\n",
    "\n",
    "# Broadcasting b to the shape of a:\n",
    "# b becomes: [2, 2, 2]\n",
    "\n",
    "result = a * b\n",
    "print(result)\n",
    "```\n",
    "\n",
    "**Output**:\n",
    "```\n",
    "tensor([2, 4, 6])\n",
    "```\n",
    "\n",
    "- `b` is a scalar (shape `()`), so it gets broadcasted to match the shape of `a` (shape `(3,)`).\n",
    "\n",
    "### Summary\n",
    "\n",
    "- **Broadcasting** allows operations on tensors of different shapes by \"expanding\" one or both tensors' dimensions to make their shapes compatible.\n",
    "- The comparison starts from the trailing dimensions (rightmost) and works its way left.\n",
    "- If the dimensions match or one of them is 1, they are compatible for broadcasting.\n",
    "- If the dimensions do not match and neither is 1, the operation will result in an error. \n",
    "\n",
    "This feature is extremely useful in many neural network operations, allowing for efficient and flexible computation without needing to manually adjust tensor shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand broadcasting and how dimensions are compared from right to left, let’s walk through a practical example with tensors of different shapes and see how they are broadcasted to a common shape for element-wise operations.\n",
    "\n",
    "### Example Tensors\n",
    "\n",
    "Consider two tensors:\n",
    "\n",
    "1. **Tensor A**: Shape `(1, 3)`\n",
    "2. **Tensor B**: Shape `(3, 1)`\n",
    "\n",
    "```python\n",
    "import torch\n",
    "\n",
    "A = torch.tensor([[1, 2, 3]])  # Shape: (1, 3)\n",
    "B = torch.tensor([[4], [5], [6]])  # Shape: (3, 1)\n",
    "```\n",
    "\n",
    "### Tensor Shapes Before Broadcasting\n",
    "\n",
    "- **Tensor A**:\n",
    "  ```\n",
    "  [[1, 2, 3]]\n",
    "  ```\n",
    "  Shape: `(1, 3)`\n",
    "\n",
    "- **Tensor B**:\n",
    "  ```\n",
    "  [[4],\n",
    "   [5],\n",
    "   [6]]\n",
    "  ```\n",
    "  Shape: `(3, 1)`\n",
    "\n",
    "### Broadcasting Rules\n",
    "\n",
    "**Broadcasting** works by aligning tensors to a common shape using the following rules:\n",
    "\n",
    "1. **Compare Dimensions**: Start comparing the shapes of tensors from the rightmost dimension.\n",
    "2. **Dimensions Must Match or Be 1**:\n",
    "   - If dimensions are equal, they match.\n",
    "   - If one dimension is `1`, it can be broadcast to match the other dimension.\n",
    "   - If dimensions are different and neither is `1`, broadcasting is not possible.\n",
    "\n",
    "### Applying Broadcasting\n",
    "\n",
    "1. **Align Shapes**: For tensors `A` and `B`, align them for element-wise operations. The shapes are `(1, 3)` and `(3, 1)`.\n",
    "\n",
    "2. **Start Comparing from Right**:\n",
    "   - **Rightmost Dimension**: `3` (from `A`) and `1` (from `B`)\n",
    "     - The `1` from `B` can be broadcast to `3`, making the dimensions match.\n",
    "   - **Next Dimension**: `1` (from `A`) and `3` (from `B`)\n",
    "     - The `1` from `A` can be broadcast to `3`, making the dimensions match.\n",
    "\n",
    "3. **Broadcasting**:\n",
    "   - **Expand `A`** from `(1, 3)` to `(3, 3)`:\n",
    "     ```\n",
    "     [[1, 2, 3],\n",
    "      [1, 2, 3],\n",
    "      [1, 2, 3]]\n",
    "     ```\n",
    "   - **Expand `B`** from `(3, 1)` to `(3, 3)`:\n",
    "     ```\n",
    "     [[4, 4, 4],\n",
    "      [5, 5, 5],\n",
    "      [6, 6, 6]]\n",
    "     ```\n",
    "\n",
    "### Element-Wise Operation\n",
    "\n",
    "Perform element-wise multiplication on the broadcasted tensors:\n",
    "\n",
    "```python\n",
    "result = A * B\n",
    "print(result)\n",
    "```\n",
    "\n",
    "**Output**:\n",
    "```\n",
    "tensor([[ 4,  8, 12],\n",
    "        [ 5, 10, 15],\n",
    "        [ 6, 12, 18]])\n",
    "```\n",
    "\n",
    "### Summary\n",
    "\n",
    "- **Initial Shapes**: `(1, 3)` and `(3, 1)`\n",
    "- **Broadcasted Shapes**: Both tensors are expanded to `(3, 3)`.\n",
    "- **Result Shape**: The result of the element-wise multiplication is a tensor with shape `(3, 3)`.\n",
    "\n",
    "By comparing dimensions from right to left and using the rules for broadcasting, PyTorch can automatically expand tensors to make their shapes compatible for element-wise operations. This allows for efficient and intuitive computation without manually reshaping tensors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, you’re correct. In simple matrix multiplication, if you have matrices \\(A\\) and \\(B\\) with shapes \\((1, 3)\\) and \\((3, 1)\\), respectively, the result of the multiplication will indeed be a \\((1, 1)\\) matrix. Let's break this down:\n",
    "\n",
    "### Matrix Multiplication\n",
    "\n",
    "Matrix multiplication is different from element-wise multiplication. The shapes of the matrices must be compatible for matrix multiplication:\n",
    "\n",
    "1. **Matrix \\(A\\)**: Shape \\((1, 3)\\)\n",
    "2. **Matrix \\(B\\)**: Shape \\((3, 1)\\)\n",
    "\n",
    "For matrix multiplication:\n",
    "\n",
    "- **The number of columns in \\(A\\)** must match **the number of rows in \\(B\\)**. Here, \\(A\\) has 3 columns, and \\(B\\) has 3 rows, so the multiplication is valid.\n",
    "- **The resulting matrix** will have the number of rows from \\(A\\) and the number of columns from \\(B\\). Therefore, the result will have shape \\((1, 1)\\).\n",
    "\n",
    "### Calculation\n",
    "\n",
    "The matrix multiplication of \\(A\\) and \\(B\\) can be computed as follows:\n",
    "\n",
    "```python\n",
    "import torch\n",
    "\n",
    "A = torch.tensor([[1, 2, 3]])  # Shape: (1, 3)\n",
    "B = torch.tensor([[4], [5], [6]])  # Shape: (3, 1)\n",
    "\n",
    "# Matrix multiplication\n",
    "result = torch.mm(A, B)  # or A @ B in newer versions of PyTorch\n",
    "print(result)\n",
    "```\n",
    "\n",
    "**Result**:\n",
    "\n",
    "```\n",
    "tensor([[32]])  # Shape: (1, 1)\n",
    "```\n",
    "\n",
    "### Explanation\n",
    "\n",
    "- **Matrix \\(A\\)**:\n",
    "  ```\n",
    "  [[1, 2, 3]]\n",
    "  ```\n",
    "\n",
    "- **Matrix \\(B\\)**:\n",
    "  ```\n",
    "  [[4],\n",
    "   [5],\n",
    "   [6]]\n",
    "  ```\n",
    "\n",
    "To compute the result, perform the dot product of \\(A\\) and \\(B\\):\n",
    "\n",
    "\\[ \n",
    "\\text{result} = \\begin{bmatrix}\n",
    "1 \\cdot 4 + 2 \\cdot 5 + 3 \\cdot 6\n",
    "\\end{bmatrix}\n",
    "= \\begin{bmatrix}\n",
    "4 + 10 + 18\n",
    "\\end{bmatrix}\n",
    "= \\begin{bmatrix}\n",
    "32\n",
    "\\end{bmatrix}\n",
    "\\]\n",
    "\n",
    "### Summary\n",
    "\n",
    "- **Matrix Shapes**: \\(A\\) (1, 3) and \\(B\\) (3, 1)\n",
    "- **Matrix Multiplication Result**: The result is a matrix with shape (1, 1), containing the sum of the products of corresponding elements from \\(A\\) and \\(B\\)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like there might have been a misunderstanding between matrix multiplication and element-wise multiplication. Here's a clarification:\n",
    "\n",
    "### Matrix Multiplication vs. Element-Wise Multiplication\n",
    "\n",
    "1. **Matrix Multiplication**:\n",
    "   - **Shape Rules**: For two matrices \\(A\\) with shape \\((1, 3)\\) and \\(B\\) with shape \\((3, 1)\\), matrix multiplication will result in a matrix of shape \\((1, 1)\\).\n",
    "   - **Operation**: The result is computed as the dot product of \\(A\\) and \\(B\\).\n",
    "\n",
    "2. **Element-Wise Multiplication**:\n",
    "   - **Shape Rules**: For tensors with shapes \\((1, 3)\\) and \\((3, 1)\\), element-wise multiplication (using broadcasting) results in a matrix of shape \\((3, 3)\\).\n",
    "   - **Operation**: Broadcasting expands the shapes to match, and then the element-wise product is computed.\n",
    "\n",
    "### Why You Get a \\(3 \\times 3\\) Matrix\n",
    "\n",
    "In the example where you get a \\(3 \\times 3\\) matrix, you performed element-wise multiplication rather than matrix multiplication. Here’s why:\n",
    "\n",
    "#### Example Tensor Shapes\n",
    "\n",
    "```python\n",
    "import torch\n",
    "\n",
    "A = torch.tensor([[1, 2, 3]])  # Shape: (1, 3)\n",
    "B = torch.tensor([[4], [5], [6]])  # Shape: (3, 1)\n",
    "```\n",
    "\n",
    "#### Element-Wise Multiplication\n",
    "\n",
    "```python\n",
    "result = A * B\n",
    "print(result)\n",
    "```\n",
    "\n",
    "**Output**:\n",
    "\n",
    "```\n",
    "tensor([[ 4,  8, 12],\n",
    "        [ 5, 10, 15],\n",
    "        [ 6, 12, 18]])\n",
    "```\n",
    "\n",
    "**Explanation**:\n",
    "\n",
    "- **Tensor A**: Shape `(1, 3)`, which is effectively a row vector.\n",
    "- **Tensor B**: Shape `(3, 1)`, which is effectively a column vector.\n",
    "\n",
    "When you perform element-wise multiplication:\n",
    "\n",
    "1. **Broadcasting**:\n",
    "   - **Expand `A`** from `(1, 3)` to `(3, 3)`:\n",
    "     ```\n",
    "     [[1, 2, 3],\n",
    "      [1, 2, 3],\n",
    "      [1, 2, 3]]\n",
    "     ```\n",
    "\n",
    "   - **Expand `B`** from `(3, 1)` to `(3, 3)`:\n",
    "     ```\n",
    "     [[4, 4, 4],\n",
    "      [5, 5, 5],\n",
    "      [6, 6, 6]]\n",
    "     ```\n",
    "\n",
    "2. **Element-Wise Multiplication**:\n",
    "   ```\n",
    "   [[1*4, 2*4, 3*4],\n",
    "    [1*5, 2*5, 3*5],\n",
    "    [1*6, 2*6, 3*6]]\n",
    "   ```\n",
    "\n",
    "   Which results in:\n",
    "   ```\n",
    "   [[ 4,  8, 12],\n",
    "    [ 5, 10, 15],\n",
    "    [ 6, 12, 18]]\n",
    "   ```\n",
    "\n",
    "### Summary\n",
    "\n",
    "- **Matrix Multiplication**: `(1, 3) * (3, 1)` results in `(1, 1)`.\n",
    "- **Element-Wise Multiplication**: `(1, 3) * (3, 1)` with broadcasting results in `(3, 3)`.\n",
    "\n",
    "The \\(3 \\times 3\\) matrix you observed was due to element-wise multiplication with broadcasting, not matrix multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, matrix multiplication and element-wise multiplication serve different purposes and are used for different operations in neural networks. Here’s a breakdown of why both are important and how they are used:\n",
    "\n",
    "### Matrix Multiplication\n",
    "\n",
    "**Matrix Multiplication** is used to combine two matrices in a way that is fundamental for operations like:\n",
    "\n",
    "1. **Linear Transformations**:\n",
    "   - In neural networks, each layer typically performs a linear transformation of the input data using a weight matrix.\n",
    "   - For example, in a fully connected layer (or dense layer), the operation is:\n",
    "     \\[\n",
    "     \\text{output} = \\text{input} \\times \\text{weights} + \\text{bias}\n",
    "     \\]\n",
    "   - Here, \\(\\text{input}\\) is a matrix (or tensor), \\(\\text{weights}\\) is another matrix, and the result is computed using matrix multiplication.\n",
    "\n",
    "2. **Transformation of Features**:\n",
    "   - Matrix multiplication is used to transform the input features into a new space. This is crucial in neural networks for learning and representing complex patterns.\n",
    "\n",
    "**Example**:\n",
    "\n",
    "```python\n",
    "import torch\n",
    "\n",
    "# Input tensor of shape (batch_size, features)\n",
    "input_tensor = torch.tensor([[1., 2., 3.]])  # Shape: (1, 3)\n",
    "\n",
    "# Weight matrix of shape (3, 2)\n",
    "weights = torch.tensor([[0.1, 0.2],\n",
    "                        [0.3, 0.4],\n",
    "                        [0.5, 0.6]])  # Shape: (3, 2)\n",
    "\n",
    "# Matrix multiplication\n",
    "output_tensor = torch.mm(input_tensor, weights)  # Shape: (1, 2)\n",
    "print(output_tensor)\n",
    "```\n",
    "\n",
    "### Element-Wise Multiplication\n",
    "\n",
    "**Element-Wise Multiplication** is used to perform operations where each element of one matrix or tensor is multiplied by the corresponding element of another matrix or tensor. This is crucial for:\n",
    "\n",
    "1. **Activation Functions**:\n",
    "   - Element-wise operations are used in activation functions (e.g., ReLU, sigmoid) applied after linear transformations.\n",
    "\n",
    "2. **Attention Mechanisms**:\n",
    "   - In attention mechanisms, element-wise multiplication is used to weight features.\n",
    "\n",
    "3. **Normalization and Regularization**:\n",
    "   - Techniques like dropout or batch normalization use element-wise operations to modify activations.\n",
    "\n",
    "**Example**:\n",
    "\n",
    "```python\n",
    "import torch\n",
    "\n",
    "# Two tensors of the same shape\n",
    "tensor1 = torch.tensor([[1., 2., 3.]])\n",
    "tensor2 = torch.tensor([[4., 5., 6.]])\n",
    "\n",
    "# Element-wise multiplication\n",
    "result = tensor1 * tensor2\n",
    "print(result)\n",
    "```\n",
    "\n",
    "### Why Both Are Used in Neural Networks\n",
    "\n",
    "- **Matrix Multiplication** is used to perform linear transformations and combine features in a structured way that allows the network to learn complex patterns.\n",
    "- **Element-Wise Multiplication** is used for operations that need to be applied independently to each element of the tensors, such as applying activation functions, attention mechanisms, or modifying feature maps.\n",
    "\n",
    "Both types of multiplication are fundamental to different aspects of neural network operations, and they complement each other to enable the network to learn from data and make predictions effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matrix multiplication and element-wise multiplication serve different purposes in neural networks:\n",
    "\n",
    "Matrix multiplication:\n",
    "1. Used for layer operations (e.g. dense layers)\n",
    "2. Combines input features with learned weights\n",
    "3. Allows networks to learn complex mappings\n",
    "4. Essential for most neural network architectures\n",
    "\n",
    "Element-wise multiplication:\n",
    "1. Used in specific operations (e.g. gates in LSTMs)\n",
    "2. Applied in some activation functions\n",
    "3. Used in attention mechanisms\n",
    "4. Helpful for feature scaling or masking\n",
    "\n",
    "Matrix multiplication is more fundamental to neural network operations, while element-wise multiplication is used in specific architectures or techniques.\n",
    "\n",
    "Would you like me to elaborate on any specific aspect of these operations or their applications in neural networks?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 654,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(14)"
      ]
     },
     "execution_count": 654,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.dot(torch.tensor([1,2,3]), torch.tensor([1,2,3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replacing matrix multiplication with element-wise multiplication in neural networks isn't generally feasible or desirable. Here's why:\n",
    "\n",
    "1. Dimensionality: Matrix multiplication allows combining inputs of different dimensions, which is crucial for connecting layers of different sizes. Element-wise multiplication requires matching dimensions.\n",
    "\n",
    "2. Feature interaction: Matrix multiplication enables each output to be influenced by all inputs, allowing the network to learn complex relationships. Element-wise multiplication limits each output to depend only on the corresponding input.\n",
    "\n",
    "3. Expressiveness: Neural networks with matrix multiplication are universal function approximators. Replacing this with element-wise operations would severely limit the network's ability to learn and represent complex functions.\n",
    "\n",
    "4. Information flow: Matrix multiplication allows information to flow and combine across all dimensions, which is essential for the network's ability to extract and process features.\n",
    "\n",
    "5. Established theory: Much of the theory and practice of neural networks is built on the properties of matrix multiplication.\n",
    "\n",
    "However, there are some specialized cases where element-wise operations play a crucial role:\n",
    "\n",
    "- In certain types of convolutional neural networks\n",
    "- In attention mechanisms\n",
    "- In gating mechanisms (like in LSTMs)\n",
    "\n",
    "These use element-wise operations in conjunction with, not as a replacement for, matrix multiplications.\n",
    "\n",
    "Would you like me to expand on any of these points or discuss specific architectures where element-wise operations are important?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## striding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's walk through what happens when you use `torch.tensor(0).expand_as(m)` and then inspect the properties of the resulting tensor `z`.\n",
    "\n",
    " 1. `z = torch.tensor(0).expand_as(m)`\n",
    "\n",
    "This creates a tensor `z` that has the same shape as `m`, but all elements are just views of the original scalar `0`. This means that all elements in `z` reference the same memory location as the original scalar.\n",
    "\n",
    " 2. Inspecting the Tensor `z` and Its Properties\n",
    "\n",
    "Let's look at what each part of the code returns:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 655,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]]),\n",
       " tensor(0),\n",
       "  0\n",
       " [torch.storage.TypedStorage(dtype=torch.int64, device=cpu) of size 1],\n",
       " (0, 0))"
      ]
     },
     "execution_count": 655,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = torch.arange(1, 10).reshape(3, 3)  # Create a 3x3 tensor\n",
    "z = torch.tensor(0).expand_as(m)       # Expand scalar 0 to the shape of m\n",
    "\n",
    "z, torch.tensor(0), z.storage(), z.stride()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- **`z`**: This will be a `3x3` tensor filled with `0`s, but all elements are views of the same scalar.\n",
    "\n",
    "- **`torch.tensor(0)`**: This is the original scalar tensor with a single value `0`.\n",
    "\n",
    "- **`z.storage()`**: The storage is shared and will still only contain the original `0`. The storage doesn't actually contain multiple copies of the value.\n",
    "\n",
    "- **`z.stride()`**: This indicates how many steps you need to move in the storage to get to the next element along each dimension. Since all elements in `z` are views of the same scalar, the stride will show `(0, 1)`.\n",
    "\n",
    "##### Example Output\n",
    "\n",
    "```python\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "tensor([[0, 0, 0],\n",
    "        [0, 0, 0],\n",
    "        [0, 0, 0]]),  # z (expanded tensor)\n",
    "\n",
    "tensor(0),            # Original scalar tensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<torch.Storage at 0x7f8e2410a9d0>,  # Shared storage (only contains the scalar 0)\n",
    "\n",
    "(0, 1)                # Stride\n",
    "```\n",
    "\n",
    "- **`z`**: A `3x3` tensor of zeros, where all elements are views of the same scalar.\n",
    "- **`torch.tensor(0)`**: The original scalar.\n",
    "- **`z.storage()`**: The storage shows that it is still pointing to the single value `0`.\n",
    "- **`z.stride()`**: The stride `(0, 1)` indicates that moving along the rows doesn't require a step in storage (since they all reference the same value), while moving along columns requires a step of `1` in storage.\n",
    "\n",
    " Key Points\n",
    "\n",
    "- The tensor `z` doesn't create multiple copies of the value `0`. Instead, it's an expanded view.\n",
    "- The storage remains the same and contains just one `0`.\n",
    "- The stride `(0, 1)` reflects that the tensor is effectively \"flat\" and doesn't take up more memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The concept of **stride** in PyTorch (and in many other tensor manipulation libraries) refers to the number of elements to skip in memory to move to the next position along a specific dimension of a tensor.\n",
    "\n",
    "### Explanation of Stride `(0, 1)`\n",
    "\n",
    "- **`(0, 1)`**: This is a tuple representing the stride for each dimension of the tensor.\n",
    "\n",
    "#### Breaking Down the Stride `(0, 1)`:\n",
    "\n",
    "1. **Stride `0` for the First Dimension (Rows)**:\n",
    "   - A stride of `0` in the first dimension (rows) means that to move from one row to the next, no actual step is made in memory. In other words, all rows are pointing to the same memory location (the same scalar value). This happens because the tensor is an expanded view of a single scalar, not a separate copy for each element in each row.\n",
    "   - As a result, all rows are effectively the same row, and no additional memory is traversed when moving between rows.\n",
    "\n",
    "2. **Stride `1` for the Second Dimension (Columns)**:\n",
    "   - A stride of `1` in the second dimension (columns) indicates that to move from one column to the next within the same row, you move one step in memory. In a typical 2D tensor, this would mean moving to the next element within the row.\n",
    "   - However, since in this case the entire tensor is just an expanded view of a single scalar, this stride of `1` doesn't actually correspond to different memory locations, but rather just points back to the same scalar value.\n",
    "\n",
    "### Visualization\n",
    "\n",
    "Consider a tensor created like this:\n",
    "\n",
    "```python\n",
    "z = torch.tensor(0).expand(3, 3)\n",
    "```\n",
    "\n",
    "The tensor `z` might look like this:\n",
    "\n",
    "```\n",
    "tensor([[0, 0, 0],\n",
    "        [0, 0, 0],\n",
    "        [0, 0, 0]])\n",
    "```\n",
    "\n",
    "But internally, all of these `0`s are just views of the same scalar:\n",
    "\n",
    "- The stride `(0, 1)` indicates that moving between elements in the first dimension (rows) does not require moving in memory (`0` stride).\n",
    "- Moving between elements in the second dimension (columns) requires a step of `1`, but since all elements are just views of the same scalar, this stride of `1` doesn't actually change the memory location—it just points back to the same value.\n",
    "\n",
    " Why Stride `(0, 1)`?\n",
    "\n",
    "- **Efficient Memory Usage**: The tensor does not allocate separate memory for each element, leading to efficient memory usage.\n",
    "- **Shared Value**: All elements in the tensor are views of the same underlying scalar value (`0`), and hence, moving across rows or columns doesn't require additional memory operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch3d",
   "language": "python",
   "name": "pytorch3d"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
