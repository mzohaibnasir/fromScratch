{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Character-level language Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IN CE, if we are considering actual values to be 1 (as all classes are one hot encoding and only actual class value is conisered), then it sort of act like avg -ve log likelihood/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WE have implemented bigram model using both **concurrency matrix** and **NN**.\n",
    "\n",
    "\n",
    "    In bigram model context is 1 chracter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "but if we consider more context rather than just a single previous character i.e. tri-gram then this table blows up exponenetially.. \n",
    "\n",
    "so that's why we are going implement ***MULTI-LAYER PRECEPTRON*** model to predict the next character in sequence.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "---\n",
    "---\n",
    "---\n",
    "---\n",
    "## **this paper is based on word-level language model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***MULTILAYER PERCEPTRON**\n",
    "\n",
    "\n",
    "*based on paper: ![03_makemore/papers/bengio03a.pdf]\n",
    "*\n",
    "***issues to resolve:***\n",
    "1. **it is not taking into account contexts farther than 1 or 2 words**\n",
    "2. **it is not taking into account the “similarity” between words.**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "***an important contribution of***\n",
    "\n",
    "this paper is to show that training such large-scale model is expensive but feasible, scales to large\n",
    "contexts, and yields good comparative results (\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "----\n",
    "\n",
    "### ***Abstract***\n",
    "**A goal of statistical language modeling is to learn the joint probability function of sequences of\n",
    "words in a language. This is intrinsically difficult because of the curse of dimensionality: a word\n",
    "sequence on which the model will be tested is likely to be different from all the word sequences seen\n",
    "during training. Traditional but very successful approaches based on n-grams obtain generalization\n",
    "by concatenating very short overlapping sequences seen in the training set. We propose to fight the\n",
    "curse of dimensionality by learning a distributed representation for words which allows each\n",
    "training sentence to inform the model about an exponential number of semantically neighboring\n",
    "sentences. The model learns simultaneously (1) a distributed representation for each word along\n",
    "with (2) the probability function for word sequences, expressed in terms of these representations.\n",
    "Generalization is obtained because a sequence of words that has never been seen before gets high\n",
    "probability if it is made of words that are similar (in the sense of having a nearby representation) to\n",
    "words forming an already seen sentence. Training such large models (with millions of parameters)\n",
    "within a reasonable time is itself a significant challenge. We report on experiments using neural\n",
    "networks for the probability function, showing on two text corpora that the proposed approach\n",
    "significantly improves on state-of-the-art n-gram models, and that the proposed approach allows to\n",
    "take advantage of longer contexts.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This passage discusses the concept of **statistical language modeling** and how **distributed word representations** (embeddings) help address the challenges of the **curse of dimensionality** in language models. Here’s a breakdown of the main points:\n",
    "\n",
    "1. **Goal of Statistical Language Modeling**: \n",
    "   - The aim is to learn a **joint probability function** for word sequences, allowing the model to estimate the likelihood of a sequence occurring in a language. \n",
    "   - A significant challenge in this is the sheer number of possible word sequences, as a model is likely to encounter sequences during testing that it never saw during training.\n",
    "\n",
    "2. **Curse of Dimensionality**:\n",
    "   - With a vast vocabulary, the number of possible word combinations grows exponentially, making it improbable to see every sequence during training. This makes generalization to new sequences challenging.\n",
    "\n",
    "3. **Traditional Approaches with N-grams**:\n",
    "   - Traditional models, like **n-grams**, address this by using **short overlapping sequences**. For example, a bigram model looks at two-word sequences, and a trigram model at three-word sequences. \n",
    "   - These models can generalize somewhat by reusing common short sequences but are limited in their ability to handle longer contexts.\n",
    "\n",
    "4. **Distributed Representations of Words**:\n",
    "   - To overcome these limitations, the passage proposes **learning distributed representations for words**. \n",
    "   - **Distributed representations** (often referred to as **embeddings**) capture semantic similarities by placing similar words close to each other in a multi-dimensional space. \n",
    "   - By representing words this way, the model can leverage the similarity between words to generalize better to new sequences.\n",
    "   - This allows each training sentence to inform the model about numerous related sentences by sharing semantic similarities.\n",
    "\n",
    "5. **Learning Both Representations and Probabilities**:\n",
    "   - The model learns both the **word embeddings** and the **probability function for sequences** simultaneously. \n",
    "   - By leveraging word embeddings, a sequence that hasn’t been seen before can still be assigned a high probability if it consists of words similar to those in already-seen sequences.\n",
    "\n",
    "6. **Use of Neural Networks for Language Modeling**:\n",
    "   - The paper mentions using **neural networks** to model the probability function, which is computationally intensive given the large number of parameters.\n",
    "   - This approach, however, allows the model to handle **longer contexts** than n-gram models.\n",
    "\n",
    "7. **Experimental Results**:\n",
    "   - The experiments show that the neural network-based approach with distributed word representations performs better than traditional n-gram models on certain corpora.\n",
    "   - Additionally, the model can make use of **longer contexts** in sequences, enabling better understanding and prediction of word sequences in natural language.\n",
    "\n",
    "In summary, this approach proposes using neural networks and word embeddings to overcome the curse of dimensionality, allowing for effective language modeling that generalizes well to new sequences by leveraging the semantic relationships between words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **curse of dimensionality** in the context of language modeling refers to the difficulty that arises when a model must handle an enormous number of possible word sequences due to the size of the vocabulary and the variety of ways words can be combined. This challenge can be broken down as follows:\n",
    "\n",
    "1. **Exponential Growth of Possible Sequences**:\n",
    "   - In a typical language model, the goal is to estimate the probability of word sequences. If a model has a vocabulary of, say, 10,000 words, then a two-word sequence has \\(10,000^2\\) possible combinations, a three-word sequence has \\(10,000^3\\) combinations, and so on. \n",
    "   - As the length of the sequence grows, the number of possible combinations grows exponentially. This makes it nearly impossible for the model to observe every possible sequence during training. \n",
    "\n",
    "2. **Generalization Problem**:\n",
    "   - Because a language model will encounter sequences during testing that were never seen during training, it must generalize well to these unseen sequences.\n",
    "   - Traditional approaches, like **n-gram models**, attempt to generalize by breaking sequences down into shorter, overlapping parts (e.g., using bigrams or trigrams). However, these models are limited in scope: they only capture local patterns and can't generalize well to longer sequences because they don't take enough context into account.\n",
    "  \n",
    "3. **Data Sparsity**:\n",
    "   - In high-dimensional spaces, data becomes sparse. For example, even with large datasets, most possible sequences of words will not appear in the data. \n",
    "   - As a result, traditional methods struggle with generalization, as they don't have enough data to accurately estimate the probability of sequences that were not in the training set.\n",
    "\n",
    "4. **Solution via Distributed Word Representations**:\n",
    "   - By using **distributed representations** (embeddings), each word is represented as a vector in a lower-dimensional space. This space captures semantic similarities between words, placing similar words closer together.\n",
    "   - When words have similar representations, sentences that contain these similar words can inform the model about each other, even if they aren’t exactly the same. For instance, the model can understand that \"cat\" and \"dog\" might play similar roles in sentences, which helps in estimating the probability of sequences that haven’t been explicitly seen before.\n",
    "   - This approach allows the model to overcome the curse of dimensionality by reducing the effective dimensionality of the data. Instead of treating each word as a unique entity, the model understands words in terms of their relationships with each other, enabling it to generalize to new combinations.\n",
    "\n",
    "5. **Efficient Learning with Neural Networks**:\n",
    "   - By using **neural networks** to learn both the word embeddings and the probability function for sequences, the model can effectively capture complex relationships between words and improve its understanding of sequences.\n",
    "   - This structure allows the model to make predictions based on longer contexts, something traditional n-gram models struggle with. Neural networks, particularly with distributed representations, allow the model to leverage similarities in the data to generalize more effectively.\n",
    "\n",
    "In essence, the curse of dimensionality makes it challenging to train language models on all possible word sequences. Distributed representations provide a solution by reducing the effective dimensionality and allowing the model to generalize to new sequences based on learned semantic relationships between words. This approach enables a more scalable, context-sensitive language model capable of handling the complexities of natural language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "The **Hamming distance** is a metric for comparing two strings of equal length. It measures the number of positions at which the corresponding symbols (characters or bits) are different. Essentially, it tells us how many substitutions or changes are needed to transform one string into the other.\n",
    "\n",
    "### Definition\n",
    "For two strings \\( a \\) and \\( b \\) of equal length, the Hamming distance \\( d \\) is defined as:\n",
    "\n",
    "\\[\n",
    "d(a, b) = \\sum_{i=1}^{n} \\mathbf{1}(a_i \\neq b_i)\n",
    "\\]\n",
    "\n",
    "where \\( \\mathbf{1}(a_i \\neq b_i) \\) is an indicator function that equals 1 when the characters at position \\( i \\) are different and 0 when they are the same.\n",
    "\n",
    "### Example\n",
    "Consider the following two binary strings of length 7:\n",
    "- \\( a = 1011101 \\)\n",
    "- \\( b = 1001001 \\)\n",
    "\n",
    "To calculate the Hamming distance:\n",
    "1. Compare each bit:\n",
    "   - Position 1: \\( 1 \\) vs \\( 1 \\) → match (0 changes)\n",
    "   - Position 2: \\( 0 \\) vs \\( 0 \\) → match (0 changes)\n",
    "   - Position 3: \\( 1 \\) vs \\( 0 \\) → mismatch (1 change)\n",
    "   - Position 4: \\( 1 \\) vs \\( 1 \\) → match (0 changes)\n",
    "   - Position 5: \\( 1 \\) vs \\( 0 \\) → mismatch (1 change)\n",
    "   - Position 6: \\( 0 \\) vs \\( 0 \\) → match (0 changes)\n",
    "   - Position 7: \\( 1 \\) vs \\( 1 \\) → match (0 changes)\n",
    "\n",
    "2. Count the mismatches: 2 positions differ.\n",
    "\n",
    "So, the **Hamming distance** between \\( a \\) and \\( b \\) is \\( 2 \\).\n",
    "\n",
    "### Applications\n",
    "The Hamming distance is widely used in:\n",
    "- **Error detection and correction**: In coding theory, it helps detect and correct errors by quantifying how different a received message is from an expected codeword.\n",
    "- **Information theory**: It measures the difference between two binary sequences or data points.\n",
    "- **Genetics**: It helps determine genetic similarity or differences between DNA sequences by counting the number of nucleotide differences.\n",
    "- **Network routing**: It is used in certain network protocols to find the shortest path between nodes based on hop counts or similarity of binary addresses.\n",
    "\n",
    "### Important Note\n",
    "The Hamming distance is only applicable to strings or sequences of **equal length**. If the sequences are of different lengths, you would need to use other distance metrics, such as the Levenshtein distance (or edit distance), which also considers insertions and deletions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **curse of dimensionality** in language modeling and similar tasks is a major challenge because the number of potential configurations for a set of discrete random variables (like words) grows exponentially with the number of variables involved. When we try to model the joint distribution of words in a sentence, for instance, we face a combinatorial explosion in the number of possible sequences, making it impractical to account for each one individually.\n",
    "\n",
    "Here’s a breakdown of the issue in this context:\n",
    "\n",
    "1. **Exponential Growth of Possible Sequences**: \n",
    "   - Suppose we want to model the joint distribution of 10 consecutive words, each chosen from a vocabulary of size \\( V = 100,000 \\).\n",
    "   - The number of potential 10-word sequences is \\( V^{10} = 100,000^{10} \\), or roughly \\( 10^{50} \\).\n",
    "   - This immense number means there are \\( 10^{50} - 1 \\) possible free parameters if we were to model each sequence separately, which is computationally infeasible.\n",
    "\n",
    "2. **Difficulty with Discrete Variables**:\n",
    "   - Unlike continuous variables, where we can often assume some level of smoothness or continuity, discrete variables (like individual words) do not offer this convenience.\n",
    "   - For discrete spaces, small changes can drastically alter the outcome or meaning. For example, changing a single word in a sentence can significantly change its overall meaning or context.\n",
    "   - Thus, without additional structure, the function we aim to model lacks smoothness, making it hard to generalize from one observed sequence to another.\n",
    "\n",
    "3. **Hamming Distance and Sparsity**:\n",
    "   - In a high-dimensional discrete space, any two random sequences are likely to differ in many positions. For example, with 10-word sequences, two random sequences will probably differ in most of those words, making their **Hamming distance** (the number of differing positions) high.\n",
    "   - This sparsity means that, in such a large space, most observed sequences are almost maximally far apart, leading to very limited overlap and hence fewer opportunities for the model to generalize from one sequence to another.\n",
    "  \n",
    "4. **Generalization Challenges**:\n",
    "   - In continuous spaces, methods like neural networks and Gaussian mixture models can generalize by exploiting smoothness and local patterns. However, for discrete spaces, there's no such intrinsic smoothness, making it challenging to generalize based on observed data alone.\n",
    "   - To mitigate this, approaches like learning distributed representations (e.g., word embeddings) are used to capture semantic similarities, enabling sequences that share similar words (even if not identical) to influence each other.\n",
    "\n",
    "In summary, the curse of dimensionality in language modeling is especially problematic due to the vast number of possible word sequences and the lack of smoothness in discrete spaces. This makes generalization difficult, as small changes can lead to large variations in meaning, and most sequences are far apart in terms of Hamming distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A useful way to visualize how different learning algorithms generalize, inspired from the view of\n",
    "non-parametric density estimation, is to think of how probability mass that is initially concentrated\n",
    "on the training points (e.g., training sentences) is distributed in a larger volume, usually in some form\n",
    "of neighborhood around the training points. In high dimensions, it is crucial to distribute probability\n",
    "mass where it matters rather than uniformly in all directions around each training point. We will\n",
    "show in this paper that the way in which the approach proposed here generalizes is fundamentally\n",
    "different from the way in which previous state-of-the-art statistical language modeling approaches\n",
    "are generalizing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This passage describes a conceptual approach to understanding how different learning algorithms achieve **generalization**, especially within the context of high-dimensional spaces like language models. Here, generalization can be thought of as the way **probability mass**, which initially centers around specific training points (like training sentences), is spread out or extended to cover unseen instances during testing.\n",
    "\n",
    "1. **Generalization and Probability Mass**:\n",
    "   - When a model learns from specific data points, it concentrates probability mass on those observed points.\n",
    "   - For the model to perform well on new, unseen data, it must distribute this probability mass beyond the exact training points, covering a region that includes similar but unobserved sequences. This means the model is essentially extending what it has learned to a \"neighborhood\" around the training points.\n",
    "\n",
    "2. **Challenges in High-Dimensional Spaces**:\n",
    "   - In high dimensions, the problem of generalization becomes more difficult. If probability mass is spread uniformly around training points, it will cover areas that are irrelevant or nonsensical (for example, nonsensical word sequences in language modeling).\n",
    "   - Effective generalization requires concentrating the probability mass in specific, meaningful directions around each training point, rather than spreading it equally in all directions. This way, the model gives higher probabilities to new data points that are likely to be semantically related to the training data.\n",
    "\n",
    "3. **Difference in Generalization Strategies**:\n",
    "   - Traditional language models, like n-gram models, generalize by covering short overlapping sequences, often relying on seen sequences or sequences with minor variations.\n",
    "   - The passage suggests that the approach in this paper distributes probability mass differently, perhaps by learning distributed representations (e.g., word embeddings). This allows the model to place higher probabilities on sentences with words similar to those in the training set, thereby capturing broader semantic relationships rather than only capturing exact or short overlapping sequences.\n",
    "\n",
    "In summary, this approach to generalization emphasizes placing probability mass more intelligently around training points in high-dimensional spaces. By doing so, it ensures the model captures the most relevant unseen instances, focusing on semantically meaningful areas instead of indiscriminately covering all directions. This strategy differentiates it from traditional models that might lack this nuance, leading to better handling of unseen data and more robust generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A statistical model of language can be represented by the conditional probability of the next\n",
    "word given all the previous ones, since\n",
    "    \n",
    "    Pˆ(wT 1 ) = T∏t=1  Pˆ(wtjwt1−1);\n",
    "\n",
    "where wt is the t-th word, and writing sub-sequence wij = (wi;wi+1;··· ;w j−1;w j). Such statistical language models have already been found useful in many technological applications involving\n",
    "natural language, such as speech recognition, language translation, and information retrieval. Improvements in statistical language models could thus have a significant impact on such applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The passage describes a statistical model of language that focuses on predicting the next word in a sequence based on all preceding words. This model is foundational for various applications in natural language processing (NLP). Let's break down the key concepts and implications:\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "1. **Conditional Probability**:\n",
    "   - The model expresses the probability of a sequence of words \\( w_1, w_2, \\ldots, w_T \\) (where \\( T \\) is the total number of words) as the product of conditional probabilities. Specifically, it calculates the probability of each word given the words that preceded it:\n",
    "     \\[\n",
    "     \\hat{P}(w_1^T) = \\prod_{t=1}^{T} \\hat{P}(w_t \\mid w_{1}^{t-1})\n",
    "     \\]\n",
    "   - Here, \\( w_t \\) is the \\( t \\)-th word, and \\( w_{1}^{t-1} \\) denotes the sequence of words from the first word up to the one before \\( w_t \\).\n",
    "\n",
    "2. **Sequential Prediction**:\n",
    "   - The model builds on the idea that the prediction of each word is influenced by the context of all previously seen words. This is crucial for understanding the meaning and grammatical structure of sentences.\n",
    "\n",
    "3. **Sub-sequence Notation**:\n",
    "   - The notation \\( w_{i,j} = (w_i, w_{i+1}, \\ldots, w_{j-1}, w_j) \\) represents a contiguous subsequence of words, which is important for understanding the context over which probabilities are conditioned.\n",
    "\n",
    "### Applications in Natural Language Processing\n",
    "\n",
    "The ability to predict the next word based on previous words is critical for many NLP applications, including:\n",
    "\n",
    "- **Speech Recognition**: Understanding spoken language and converting it into text relies heavily on accurately predicting and modeling the next words in a sequence. The more accurate the model, the better it can handle variations in speech.\n",
    "\n",
    "- **Language Translation**: Translating sentences from one language to another requires not just a word-for-word translation but also an understanding of context and structure, which statistical models can provide.\n",
    "\n",
    "- **Information Retrieval**: When searching for information, understanding the likely sequence of words based on user queries can improve the relevance and accuracy of the results returned.\n",
    "\n",
    "### Impact of Improvements\n",
    "\n",
    "Enhancing statistical language models can lead to significant advancements in these applications:\n",
    "- **Better Accuracy**: More accurate models can lead to fewer errors in tasks like speech recognition, where misinterpreting a single word can change the meaning of an entire sentence.\n",
    "- **Enhanced User Experience**: Improvements can lead to smoother interactions with technology, whether through more natural speech-to-text conversions or more relevant search results.\n",
    "- **Broader Functionality**: Better models can enable more sophisticated NLP applications, such as generating text or responding to queries in a more human-like manner.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "The statistical model of language outlined in the passage forms the backbone of many NLP applications by leveraging the conditional probabilities of word sequences. As improvements in these models occur, they can significantly impact various technologies that rely on understanding and processing natural language. This makes the development of effective language models a critical area of research and application in the field of artificial intelligence and machine learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Fighting the Curse of Dimensionality with Distributed Representations\n",
    "In a nutshell, the idea of the proposed approach can be summarized as follows:\n",
    "1. associate with each word in the vocabulary a distributed word feature vector (a realvalued vector in Rm),\n",
    "2. express the joint probability function of word sequences in terms of the feature vectors\n",
    "of these words in the sequence, and\n",
    "3. learn simultaneously the word feature vectors and the parameters of that probability\n",
    "function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Certainly! Here's a Markdown version of your passage that summarizes the idea of fighting the curse of dimensionality with distributed representations. This version is structured for clarity and includes proper LaTeX syntax where applicable.\n",
    "\n",
    "### Fighting the Curse of Dimensionality with Distributed Representations\n",
    "\n",
    "In a nutshell, the idea of the proposed approach can be summarized as follows:\n",
    "\n",
    "1. **Distributed Word Feature Vectors**:\n",
    "   - Associate each word in the vocabulary with a distributed word feature vector, represented as a real-valued vector in \\(\\mathbb{R}^m\\).\n",
    "  \n",
    "2. **Joint Probability Function**:\n",
    "   - Express the joint probability function of word sequences in terms of the feature vectors of the words in the sequence.\n",
    "\n",
    "3. **Simultaneous Learning**:\n",
    "   - Learn simultaneously the word feature vectors and the parameters of the probability function.\n",
    "\n",
    "### Explanation of Key Concepts\n",
    "\n",
    "1. **Distributed Word Feature Vectors**:\n",
    "   - These vectors capture semantic meanings and relationships between words, allowing the model to understand the context better. For example, similar words will have feature vectors that are closer together in the vector space.\n",
    "\n",
    "2. **Joint Probability Function**:\n",
    "   - By expressing the probability of a sequence of words through their associated feature vectors, the model can generalize better to unseen sequences. This is particularly important in overcoming the curse of dimensionality, where traditional models struggle to estimate probabilities in high-dimensional spaces.\n",
    "\n",
    "3. **Simultaneous Learning**:\n",
    "   - This approach not only refines the feature vectors but also optimizes the probability estimation function, leading to a more coherent and effective language model.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "This method provides a powerful framework for language modeling by leveraging distributed representations, which can effectively combat the challenges posed by the curse of dimensionality. By simultaneously learning the representations and the probability function, the model enhances its ability to generalize from the training data to new, unseen data.\n",
    "\n",
    "---\n",
    "\n",
    "Feel free to modify any part of it to better fit your needs or let me know if you need further adjustments!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Certainly! Here’s a concise explanation of how the squared norm of the parameters is penalized in the context of ridge regression, along with a brief overview of its significance:\n",
    "\n",
    "### Ridge Regression and Parameter Penalization\n",
    "\n",
    "In **ridge regression**, a common technique used in linear regression, the objective is to minimize the following cost function:\n",
    "\n",
    "\\[\n",
    "J(\\theta) = \\text{RSS}(\\theta) + \\lambda \\|\\theta\\|^2_2\n",
    "\\]\n",
    "\n",
    "Where:\n",
    "- \\(\\text{RSS}(\\theta)\\) is the residual sum of squares, given by:\n",
    "\n",
    "\\[\n",
    "\\text{RSS}(\\theta) = \\sum_{i=1}^{n} (y_i - X_i \\theta)^2\n",
    "\\]\n",
    "\n",
    "- \\(\\|\\theta\\|^2_2\\) is the squared Euclidean norm of the parameter vector \\(\\theta\\), which is calculated as:\n",
    "\n",
    "\\[\n",
    "\\|\\theta\\|^2_2 = \\sum_{j=1}^{p} \\theta_j^2\n",
    "\\]\n",
    "\n",
    "- \\(\\lambda\\) is a regularization parameter that controls the strength of the penalty.\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "1. **Penalization**:\n",
    "   - The term \\(\\lambda \\|\\theta\\|^2_2\\) penalizes large coefficients in the parameter vector \\(\\theta\\). This discourages overfitting by shrinking the coefficients, which can lead to a model that generalizes better to unseen data.\n",
    "\n",
    "2. **Trade-off**:\n",
    "   - The parameter \\(\\lambda\\) represents a trade-off between fitting the training data well (minimizing the RSS) and keeping the model complexity low (minimizing the squared norm of the parameters). A larger \\(\\lambda\\) increases the penalty on the size of the coefficients, leading to more shrinkage.\n",
    "\n",
    "3. **Bias-Variance Trade-off**:\n",
    "   - Ridge regression introduces a small amount of bias in exchange for a reduction in variance. This can lead to improved prediction accuracy, especially in scenarios with multicollinearity or when the number of predictors exceeds the number of observations.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Ridge regression effectively manages the complexity of the model through the penalization of the squared norm of the parameters, promoting a more stable and generalizable solution in the presence of noisy data or when dealing with high-dimensional feature spaces. This method is particularly useful in machine learning applications where overfitting is a concern. \n",
    "\n",
    "---\n",
    "\n",
    "Feel free to ask for any additional details or specific modifications!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The passage describes how a statistical language model can generalize knowledge from one sentence to various other sentences based on the similarities between words and their representations. Here’s a breakdown of the key ideas presented:\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "1. **Generalization of Probability Mass**:\n",
    "   - The model allows the transfer of probability mass from one sentence to semantically similar sentences. For example, if the training data includes \"The cat is walking in the bedroom,\" the model can also assign higher probabilities to sentences like \"A dog was running in a room\" and \"The cat is running in a room\" due to the similarities in word usage and structure.\n",
    "\n",
    "2. **Word Representations**:\n",
    "   - The model uses **distributed representations** (feature vectors) for words. Words that are semantically similar will have similar feature vectors. This allows the model to capture relationships between words and their contexts effectively.\n",
    "\n",
    "3. **Smoothness of the Probability Function**:\n",
    "   - The probability function used in the model is smooth, meaning that small changes in the feature vectors lead to small changes in the computed probabilities. This characteristic is crucial because it enables the model to generalize well. For instance, if a sentence is seen during training, the model can infer probabilities for other sentences with slight variations based on their similar word vectors.\n",
    "\n",
    "4. **Combinatorial Neighbors**:\n",
    "   - The model not only increases the probability of the original sentence but also enhances the probabilities of a combinatorial number of “neighbors” in the sentence space. These neighbors are other sentences that may differ slightly in word choice or structure but retain similar meanings or contexts. Thus, the presence of one sentence in the training set informs the model about a wide range of potential sentences, increasing its robustness.\n",
    "\n",
    "### Implications\n",
    "\n",
    "- This approach allows the model to handle variations in natural language effectively. By leveraging the relationships between words and their features, the model can predict the likelihood of various sentences even if they haven't been explicitly encountered during training.\n",
    "- It significantly improves the model's ability to deal with the **curse of dimensionality**, as the connections established through similar feature vectors allow for efficient probability estimation across a vast space of possible sentences.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "The ability of the proposed model to generalize from specific examples to a broader context enhances its effectiveness in natural language processing tasks. By utilizing smooth probability functions and distributed representations, it captures the nuances of language, leading to better performance in applications such as machine translation, speech recognition, and information retrieval. \n",
    "\n",
    "---\n",
    "\n",
    "If you need further clarification on specific aspects or have additional questions, feel free to ask!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, perplexity is directly related to the average of the negative log-likelihood (NLL) for a sequence of words in language modeling. Here’s how they connect:\n",
    "\n",
    "### Average Negative Log-Likelihood (NLL)\n",
    "The average NLL measures how well a language model predicts a sequence. It is calculated as follows:\n",
    "\\[\n",
    "\\text{Average NLL} = -\\frac{1}{L} \\sum_{i=1}^{L} \\log p(x_i | x_{1:i-1})\n",
    "\\]\n",
    "where:\n",
    "- \\( L \\) is the length of the sequence.\n",
    "- \\( p(x_i | x_{1:i-1}) \\) is the conditional probability of token \\( x_i \\) given the previous tokens.\n",
    "\n",
    "### Perplexity\n",
    "Perplexity is the exponentiated average NLL:\n",
    "\\[\n",
    "\\text{Perplexity} = \\exp\\left(\\frac{1}{L} \\sum_{i=1}^{L} -\\log p(x_i | x_{1:i-1})\\right)\n",
    "\\]\n",
    "Simplifying this, we see that:\n",
    "\\[\n",
    "\\text{Perplexity} = \\exp(\\text{Average NLL})\n",
    "\\]\n",
    "\n",
    "### Interpretation\n",
    "So, while the average NLL provides a log-scaled measure of the model’s performance, perplexity takes this value and exponentiates it to make it easier to interpret in terms of probabilities. Perplexity can be thought of as the “effective number of choices” the model has per token in a sequence. Lower perplexity indicates better performance, as it means the model is more confident in its predictions.\n",
    "\n",
    "In summary:\n",
    "- **Perplexity** is indeed the exponentiated form of the average NLL.\n",
    "- Both metrics evaluate the model’s predictive performance, with perplexity being a more intuitive measure in many language modeling contexts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's delve deeper into the intuition behind **average negative log-likelihood (NLL)** and **perplexity**, and explore why both are important metrics in language modeling.\n",
    "\n",
    "### 1. Average Negative Log-Likelihood (NLL)\n",
    "\n",
    "#### Definition\n",
    "The **average NLL** of a sequence of words \\(x_1, x_2, \\dots, x_L\\) given by a language model is:\n",
    "\\[\n",
    "\\text{Average NLL} = -\\frac{1}{L} \\sum_{i=1}^{L} \\log p(x_i | x_{1:i-1})\n",
    "\\]\n",
    "Here:\n",
    "- \\( L \\) is the length of the sequence.\n",
    "- \\( p(x_i | x_{1:i-1}) \\) is the probability that the language model assigns to word \\( x_i \\) given all the previous words \\( x_{1:i-1} \\).\n",
    "\n",
    "#### Intuition\n",
    "The negative log-likelihood essentially measures the **surprise** of a language model when it encounters the actual next word in the sequence. If the model assigns a **high probability** to the true next word, the **log probability is less negative**, making the overall NLL lower. This suggests the model's predictions align well with reality.\n",
    "\n",
    "Breaking down why the NLL is used:\n",
    "- **Log Transformation**: Logarithms convert probabilities into a cumulative sum, and because \\(\\log\\) is monotonic, the relative order of likelihoods is preserved. The log function also makes larger probabilities closer to zero and penalizes low probabilities heavily (as they become very negative), which forces the model to maximize probability for actual words.\n",
    "- **Negative Sign**: Since probabilities are between 0 and 1, \\(\\log(p)\\) values are between \\(-\\infty\\) and 0. Taking the negative makes NLL positive and more interpretable as a **penalty** or **cost** function.\n",
    "- **Averaging**: Dividing by \\( L \\) gives us a measure of **average cost per word**, providing a scale-independent way to compare models over sequences of varying lengths.\n",
    "\n",
    "Lower NLL suggests that the language model is **better at predicting** the next word in the sequence.\n",
    "\n",
    "#### Relationship with Cross-Entropy\n",
    "NLL is effectively equivalent to **cross-entropy loss** in classification problems. It measures the **distance** between the model's predicted distribution and the true distribution. The average NLL tells us how close the model’s predicted probabilities are to the actual words in the data.\n",
    "\n",
    "### 2. Perplexity\n",
    "\n",
    "#### Definition\n",
    "Perplexity is defined as the **exponentiated average NLL**:\n",
    "\\[\n",
    "\\text{Perplexity} = \\exp\\left(\\frac{1}{L} \\sum_{i=1}^{L} -\\log p(x_i | x_{1:i-1})\\right)\n",
    "\\]\n",
    "Simplifying, we get:\n",
    "\\[\n",
    "\\text{Perplexity} = \\exp(\\text{Average NLL})\n",
    "\\]\n",
    "\n",
    "#### Intuition\n",
    "While NLL gives us a direct measure of the model’s **surprise** in log-probability space, perplexity translates this into a more **intuitive metric** that reflects the model's **effective branching factor** per word in the sequence. \n",
    "\n",
    "- **Branching Factor**: Perplexity can be thought of as the **average number of choices** the model has per word. For example, if a model has a perplexity of 10, it means the model, on average, is effectively choosing between 10 likely words at each step. The lower the perplexity, the fewer choices the model needs, indicating higher confidence in its predictions.\n",
    "  \n",
    "- **Smooth Transitions**: The exponential transformation smooths the NLL, making perplexity easier to interpret for comparing model performance, especially in natural language processing. Since probabilities decay multiplicatively in a sequence, perplexity reflects the **difficulty of the task** on a scale that is easier to compare across datasets and models.\n",
    "  \n",
    "- **Relationship to Uncertainty**: A high perplexity means the model is uncertain and spread out across many possible words, while a low perplexity suggests that the model has higher certainty, focusing on fewer possible words. For example:\n",
    "  - **Perplexity of 1** would mean perfect certainty, where the model always predicts the correct word with probability 1.\n",
    "  - **Perplexity of \\(V\\)** (where \\(V\\) is the vocabulary size) would suggest a completely uniform model, where each word is equally likely.\n",
    "\n",
    "#### Why Perplexity Matters\n",
    "Perplexity provides an intuitive way to compare different language models on a given dataset or task. It allows us to understand how **confident** a model is in its predictions. Since language modeling often involves choosing among thousands or tens of thousands of words, the exponential measure of perplexity provides a **clearer view of the model’s quality**. \n",
    "\n",
    "### Comparison and Usage\n",
    "- **NLL** is a more direct measure of model quality, especially in terms of **optimization and training** since it directly relates to the loss function minimized during training.\n",
    "- **Perplexity**, however, provides an intuitive measure that is often more interpretable for understanding **model performance** in real-world contexts. For example, it gives a sense of the model’s uncertainty and the “effective choices” it has per word in a sequence.\n",
    "\n",
    "In summary:\n",
    "- Both metrics are essential for understanding model performance in language tasks.\n",
    "- NLL is useful for directly evaluating and improving the model during training.\n",
    "- Perplexity gives an intuitive measure of how well the model generalizes and predicts, especially valuable when comparing models or assessing their real-world application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **large perplexity** means that the language model is generally more **uncertain** or **less confident** in its predictions. Specifically, it indicates that, on average, the model is considering a **wider range of possible words** for each position in the sequence. \n",
    "\n",
    "### Key Implications of Large Perplexity:\n",
    "1. **Higher Uncertainty**: A large perplexity implies that the model has to consider more potential next words at each step, which shows that it has lower confidence in its predictions.\n",
    "  \n",
    "2. **More Branching**: Since perplexity reflects the effective “branching factor” of choices per word, a high perplexity suggests that each word in the sequence could branch out into many possibilities. This is less ideal, as it indicates the model does not narrow down the choices effectively.\n",
    "\n",
    "3. **Poorer Performance**: In practical terms, high perplexity typically points to a model that is not well-tuned or has not learned the underlying patterns of the language in the dataset very well. It suggests that the model struggles to capture meaningful patterns and dependencies in the data, making it less accurate at predicting the next word.\n",
    "\n",
    "### Examples of High Perplexity in Context:\n",
    "- In language modeling, if a model trained on English has a high perplexity, it may mean that the model is not adequately capturing grammatical structures or word relationships, leading to less coherent predictions.\n",
    "- In applications like speech recognition, high perplexity implies that the model may produce more errors because it is too uncertain about which words are likely to come next, potentially generating confusing or inaccurate outputs.\n",
    "\n",
    "Generally, the goal is to reduce perplexity to reflect improved model performance, meaning the model can predict the next word with higher confidence and fewer possible choices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, that's correct! Perplexity is indeed the exponentiation of the average negative log-likelihood (NLL), and it provides a more interpretable measure, especially when dealing with probability scales.\n",
    "\n",
    "### Why Convert NLL to Perplexity?\n",
    "- **Interpretability**: Perplexity is more intuitive in many cases, as it can be interpreted as the \"branching factor\" or the effective number of choices available per token. For instance, a perplexity of 10 suggests that, on average, each word in the sequence could be one of 10 equally likely options.\n",
    "- **Raw Scale**: Since log-likelihood can be negative or difficult to interpret directly, exponentiating it back (i.e., using perplexity) translates this log scale into a raw value. This is useful for comparing models or understanding language model performance on a more natural scale.\n",
    "\n",
    "### How Perplexity Relates to NLL:\n",
    "The relationship between average NLL and perplexity is given by:\n",
    "\\[\n",
    "\\text{Perplexity} = \\exp\\left(\\text{Average Negative Log-Likelihood}\\right)\n",
    "\\]\n",
    "\n",
    "### Example:\n",
    "If the average NLL for a language model is `2.3`, the perplexity is calculated as:\n",
    "\\[\n",
    "\\text{Perplexity} = \\exp(2.3) \\approx 10\n",
    "\\]\n",
    "This indicates that, on average, the model views each word as having about 10 possible choices based on the context provided by the preceding words.\n",
    "\n",
    "So yes, converting from log scale (NLL) to raw values (perplexity) is helpful for making practical sense of language model quality in real-world terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The probability function is expressed as a product of conditional probabilities of the next word given the previous ones, (e.g. using a multilayer neural network to predict the next word given the previous ones, in the experiments). This function has parameters that can be iteratively tuned in order to maximize the log-likelihood of the training data or a regularized criterion, e.g. by adding a weight decay penalty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The logits in a language model represent unnormalized scores for each possible next word. When these logits are passed through a softmax function, they become probabilities that sum to 1. The resulting probabilities then represent the model’s beliefs about the likelihood of each possible next word in the sequence.\n",
    "\n",
    "### Logits and Perplexity\n",
    "Here’s how logits tie into the branching factor concept in perplexity:\n",
    "\n",
    "1. **Logits as Scores**:\n",
    "   - Each logit represents a raw score for each word in the vocabulary. Higher logits mean that the model is more confident in that particular word as the next one in the sequence.\n",
    "   - The softmax function converts these logits into probabilities, allowing us to interpret them as the likelihoods of each word occurring next.\n",
    "\n",
    "2. **Probabilities and Perplexity**:\n",
    "   - Once the logits are transformed into probabilities, we use these probabilities to calculate the average negative log-likelihood over the sequence. This NLL quantifies the model's uncertainty (or certainty) in predicting the sequence.\n",
    "   - The average NLL, when exponentiated, gives us the perplexity, which reflects the model's \"branching factor\" or the number of possible choices it considers at each step.\n",
    "\n",
    "3. **Branching Factor**:\n",
    "   - A higher perplexity implies a higher branching factor, indicating that the model is less certain and considers more words as likely candidates.\n",
    "   - Conversely, a lower perplexity indicates a lower branching factor, meaning the model is more confident and considers fewer words as probable next words.\n",
    "\n",
    "Therefore, **logits contribute to the branching factor by determining the shape of the probability distribution over the vocabulary**. If the logits result in a sharper distribution (with a few high probabilities), the branching factor will be lower. If they result in a flatter distribution (more spread out probabilities), the branching factor will be higher, leading to a larger perplexity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This neural probabilistic language model (NPLM) approach describes a model for predicting the probability of a sequence of words by leveraging both word embeddings and a neural network. Here’s a breakdown of the architecture and intuition:\n",
    "\n",
    "### Model Architecture\n",
    "\n",
    "1. **Word Embeddings (Feature Vectors)**:\n",
    "   - Each word \\( i \\) in the vocabulary \\( V \\) has an associated feature vector \\( C(i) \\) in a \\( |V| \\times m \\) matrix \\( C \\). This feature vector is a dense, learned representation of the word in an \\( m \\)-dimensional space.\n",
    "   - The word feature vector \\( C(i) \\) is essentially an embedding of the word \\( i \\) and is part of the model’s parameters.\n",
    "   - When processing a sequence \\( (w_{t-n+1}, \\dots, w_{t-1}) \\), each word’s feature vector is concatenated to form an input vector \\( x \\).\n",
    "\n",
    "2. **Neural Network Function \\( g \\)**:\n",
    "   - The function \\( g \\) is implemented as a neural network with parameters \\( \\omega \\).\n",
    "   - The neural network computes the probability \\( P(\\hat{w_t} \\mid w_{t-1}, \\dots, w_{t-n+1}) \\) of the next word \\( w_t \\) given the previous words \\( (w_{t-1}, \\dots, w_{t-n+1}) \\).\n",
    "   - This function \\( g \\) can be implemented using various architectures:\n",
    "     - **Feed-forward neural network**: where the input is processed in layers, leading to a prediction for the next word.\n",
    "     - **Recurrent neural network**: which introduces shared weights across time steps, making it more efficient for handling sequential data.\n",
    "   - In this model, \\( g \\) takes the concatenated embeddings from \\( C \\) as input and produces the next word probability.\n",
    "\n",
    "3. **Hidden Layers**:\n",
    "   - The architecture includes two hidden layers:\n",
    "     1. The first layer is the word embedding layer \\( C \\) itself.\n",
    "     2. The second hidden layer is a standard hidden layer with a nonlinear activation function (hyperbolic tangent, \\( \\tanh \\)), allowing the model to learn more complex relationships.\n",
    "   - The final output layer is a **softmax layer**, which transforms the network's output logits into probabilities for each word in the vocabulary.\n",
    "\n",
    "4. **Output Calculation**:\n",
    "   - For each word \\( w_t \\), we calculate a logit score \\( y_i \\) for each possible next word \\( i \\):\n",
    "     \\[\n",
    "     y = b + W x + U \\tanh(d + H x)\n",
    "     \\]\n",
    "     where:\n",
    "     - \\( W \\) is the weight matrix for any direct connections,\n",
    "     - \\( U \\) is the weight matrix for connections through the \\( \\tanh \\) activation,\n",
    "     - \\( b \\) and \\( d \\) are bias terms,\n",
    "     - \\( H \\) is the weight matrix for the hidden layer,\n",
    "     - \\( x \\) is the concatenated feature vector from the \\( n \\)-previous words.\n",
    "   - These logits \\( y_i \\) are then passed through a softmax function:\n",
    "     \\[\n",
    "     P(\\hat{w_t} \\mid w_{t-1}, \\dots, w_{t-n+1}) = \\frac{e^{y_{w_t}}}{\\sum_i e^{y_i}}\n",
    "     \\]\n",
    "     This provides a probability distribution over the vocabulary.\n",
    "\n",
    "### Objective and Training\n",
    "- **Objective Function**: The goal is to maximize the log-likelihood of the training data, which is the average log probability of the observed sequences:\n",
    "  \\[\n",
    "  L = \\frac{1}{T} \\sum_t \\log P(\\hat{w_t} \\mid w_{t-1}, \\dots, w_{t-n+1}; \\theta) + R(\\theta)\n",
    "  \\]\n",
    "  Here, \\( R(\\theta) \\) is a regularization term (such as weight decay) to penalize large weights, helping prevent overfitting.\n",
    "  \n",
    "- **Parameter Efficiency**: \n",
    "  - This model scales linearly with \\( |V| \\), the vocabulary size, because each word has a single embedding vector in \\( C \\). It also scales linearly with \\( n \\), the order of the model, meaning it can efficiently handle larger vocabularies and contexts.\n",
    "  - Recurrent or time-delay networks could reduce this scaling even further, making the model more efficient for longer contexts.\n",
    "\n",
    "### Intuition of the Architecture\n",
    "- **Learning Word Similarities**: Words that often appear in similar contexts tend to have similar feature vectors, allowing the model to generalize across sequences of words. For instance, after learning that \"cat\" and \"dog\" appear in similar contexts, the model can predict sequences involving either word more confidently.\n",
    "  \n",
    "- **Predictive Capability and Generalization**:\n",
    "  - The neural network \\( g \\), by mapping the context of previous words to the next word probabilities, can capture complex patterns in language.\n",
    "  - By adjusting the weights in the \\( C \\) matrix and the neural network’s parameters \\( \\omega \\), the model learns to assign higher probabilities to more likely word sequences, leveraging both local context and global patterns in the training data.\n",
    "\n",
    "In summary, this neural probabilistic language model is an early example of how embeddings and neural networks can jointly model word sequences, capturing nuanced relationships and enabling better generalization across unseen data. The softmax layer guarantees valid probabilities, and the model is designed for efficient scaling with both vocabulary size and sequence length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to cope with curse of dimenionality and computational expense, this paper proposed methodologies to counter it. \n",
    "this paper is based on word-level model but we are working with charater level model.\n",
    "\n",
    "\n",
    "\n",
    "1. Fighting curse of dimeneionality with Distributed Representations:\n",
    "    this paper suggests to associate with each word a m-dimensional(m=30) feature vector. where each vector is just a point in vector space. \n",
    "    In begining these words are initializen, then these embeddings willl be tuned using backpropagation. so during course of traininig, these points/vectors are going to basically move around in the space.so that similar words might end up closer to eachother, converseky word that mean different may end up somewhere else.\n",
    "\n",
    "    if th model has never encounter sentence like **A dog was running in a room** that mean this sentence is **out of distribution**. but it has seen sentence like **the dog was walking in the room**  and maybe your network has learned that 'a' and 'the are interchageable with each other, so it embeddings for 'a' and embeddings for 'the' and it pt them nearby eachother in the space.   similarly, model might now that 'cats' and 'dogs' are animals and they co occur in lots of similar contexts. so even though you haven't seen 'walking' or 'running', you can through the embedding space, transfer knowledge and you can generalize ton new scenerioes,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## so paramaters of n in fig 1 is:\n",
    "1. weights + biases of output layer\n",
    "2. weights + biases of hidden layer\n",
    "3. and embedding look up table c (m sizwd vector representation of each word)\n",
    "\n",
    "\n",
    "all of this is optimized unsing backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%matlplotlib` not found.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matlplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma',\n",
       " 'olivia',\n",
       " 'ava',\n",
       " 'isabella',\n",
       " 'sophia',\n",
       " 'charlotte',\n",
       " 'mia',\n",
       " 'amelia',\n",
       " 'harper',\n",
       " 'evelyn']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = open('names.txt', 'r').read().splitlines()\n",
    "words[:10]\n",
    "                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32033"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i, s in enumerate(chars)}\n",
    "stoi['.'] =0\n",
    "itos = {i:s for s, i in stoi.items()}\n",
    "print(itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w='emma'; context=[0, 0, 0]\n",
      "\tX=[[0, 0, 0]]\n",
      "\tY=[5]\n",
      "\t ... ----> e\n",
      "\tX=[[0, 0, 0], [0, 0, 5]]\n",
      "\tY=[5, 13]\n",
      "\t ..e ----> m\n",
      "\tX=[[0, 0, 0], [0, 0, 5], [0, 5, 13]]\n",
      "\tY=[5, 13, 13]\n",
      "\t .em ----> m\n",
      "\tX=[[0, 0, 0], [0, 0, 5], [0, 5, 13], [5, 13, 13]]\n",
      "\tY=[5, 13, 13, 1]\n",
      "\t emm ----> a\n",
      "\tX=[[0, 0, 0], [0, 0, 5], [0, 5, 13], [5, 13, 13], [13, 13, 1]]\n",
      "\tY=[5, 13, 13, 1, 0]\n",
      "\t mma ----> .\n",
      "w='olivia'; context=[0, 0, 0]\n",
      "\tX=[[0, 0, 0], [0, 0, 5], [0, 5, 13], [5, 13, 13], [13, 13, 1], [0, 0, 0]]\n",
      "\tY=[5, 13, 13, 1, 0, 15]\n",
      "\t ... ----> o\n",
      "\tX=[[0, 0, 0], [0, 0, 5], [0, 5, 13], [5, 13, 13], [13, 13, 1], [0, 0, 0], [0, 0, 15]]\n",
      "\tY=[5, 13, 13, 1, 0, 15, 12]\n",
      "\t ..o ----> l\n",
      "\tX=[[0, 0, 0], [0, 0, 5], [0, 5, 13], [5, 13, 13], [13, 13, 1], [0, 0, 0], [0, 0, 15], [0, 15, 12]]\n",
      "\tY=[5, 13, 13, 1, 0, 15, 12, 9]\n",
      "\t .ol ----> i\n",
      "\tX=[[0, 0, 0], [0, 0, 5], [0, 5, 13], [5, 13, 13], [13, 13, 1], [0, 0, 0], [0, 0, 15], [0, 15, 12], [15, 12, 9]]\n",
      "\tY=[5, 13, 13, 1, 0, 15, 12, 9, 22]\n",
      "\t oli ----> v\n",
      "\tX=[[0, 0, 0], [0, 0, 5], [0, 5, 13], [5, 13, 13], [13, 13, 1], [0, 0, 0], [0, 0, 15], [0, 15, 12], [15, 12, 9], [12, 9, 22]]\n",
      "\tY=[5, 13, 13, 1, 0, 15, 12, 9, 22, 9]\n",
      "\t liv ----> i\n",
      "\tX=[[0, 0, 0], [0, 0, 5], [0, 5, 13], [5, 13, 13], [13, 13, 1], [0, 0, 0], [0, 0, 15], [0, 15, 12], [15, 12, 9], [12, 9, 22], [9, 22, 9]]\n",
      "\tY=[5, 13, 13, 1, 0, 15, 12, 9, 22, 9, 1]\n",
      "\t ivi ----> a\n",
      "\tX=[[0, 0, 0], [0, 0, 5], [0, 5, 13], [5, 13, 13], [13, 13, 1], [0, 0, 0], [0, 0, 15], [0, 15, 12], [15, 12, 9], [12, 9, 22], [9, 22, 9], [22, 9, 1]]\n",
      "\tY=[5, 13, 13, 1, 0, 15, 12, 9, 22, 9, 1, 0]\n",
      "\t via ----> .\n",
      "w='ava'; context=[0, 0, 0]\n",
      "\tX=[[0, 0, 0], [0, 0, 5], [0, 5, 13], [5, 13, 13], [13, 13, 1], [0, 0, 0], [0, 0, 15], [0, 15, 12], [15, 12, 9], [12, 9, 22], [9, 22, 9], [22, 9, 1], [0, 0, 0]]\n",
      "\tY=[5, 13, 13, 1, 0, 15, 12, 9, 22, 9, 1, 0, 1]\n",
      "\t ... ----> a\n",
      "\tX=[[0, 0, 0], [0, 0, 5], [0, 5, 13], [5, 13, 13], [13, 13, 1], [0, 0, 0], [0, 0, 15], [0, 15, 12], [15, 12, 9], [12, 9, 22], [9, 22, 9], [22, 9, 1], [0, 0, 0], [0, 0, 1]]\n",
      "\tY=[5, 13, 13, 1, 0, 15, 12, 9, 22, 9, 1, 0, 1, 22]\n",
      "\t ..a ----> v\n",
      "\tX=[[0, 0, 0], [0, 0, 5], [0, 5, 13], [5, 13, 13], [13, 13, 1], [0, 0, 0], [0, 0, 15], [0, 15, 12], [15, 12, 9], [12, 9, 22], [9, 22, 9], [22, 9, 1], [0, 0, 0], [0, 0, 1], [0, 1, 22]]\n",
      "\tY=[5, 13, 13, 1, 0, 15, 12, 9, 22, 9, 1, 0, 1, 22, 1]\n",
      "\t .av ----> a\n",
      "\tX=[[0, 0, 0], [0, 0, 5], [0, 5, 13], [5, 13, 13], [13, 13, 1], [0, 0, 0], [0, 0, 15], [0, 15, 12], [15, 12, 9], [12, 9, 22], [9, 22, 9], [22, 9, 1], [0, 0, 0], [0, 0, 1], [0, 1, 22], [1, 22, 1]]\n",
      "\tY=[5, 13, 13, 1, 0, 15, 12, 9, 22, 9, 1, 0, 1, 22, 1, 0]\n",
      "\t ava ----> .\n",
      "w='isabella'; context=[0, 0, 0]\n",
      "\tX=[[0, 0, 0], [0, 0, 5], [0, 5, 13], [5, 13, 13], [13, 13, 1], [0, 0, 0], [0, 0, 15], [0, 15, 12], [15, 12, 9], [12, 9, 22], [9, 22, 9], [22, 9, 1], [0, 0, 0], [0, 0, 1], [0, 1, 22], [1, 22, 1], [0, 0, 0]]\n",
      "\tY=[5, 13, 13, 1, 0, 15, 12, 9, 22, 9, 1, 0, 1, 22, 1, 0, 9]\n",
      "\t ... ----> i\n",
      "\tX=[[0, 0, 0], [0, 0, 5], [0, 5, 13], [5, 13, 13], [13, 13, 1], [0, 0, 0], [0, 0, 15], [0, 15, 12], [15, 12, 9], [12, 9, 22], [9, 22, 9], [22, 9, 1], [0, 0, 0], [0, 0, 1], [0, 1, 22], [1, 22, 1], [0, 0, 0], [0, 0, 9]]\n",
      "\tY=[5, 13, 13, 1, 0, 15, 12, 9, 22, 9, 1, 0, 1, 22, 1, 0, 9, 19]\n",
      "\t ..i ----> s\n",
      "\tX=[[0, 0, 0], [0, 0, 5], [0, 5, 13], [5, 13, 13], [13, 13, 1], [0, 0, 0], [0, 0, 15], [0, 15, 12], [15, 12, 9], [12, 9, 22], [9, 22, 9], [22, 9, 1], [0, 0, 0], [0, 0, 1], [0, 1, 22], [1, 22, 1], [0, 0, 0], [0, 0, 9], [0, 9, 19]]\n",
      "\tY=[5, 13, 13, 1, 0, 15, 12, 9, 22, 9, 1, 0, 1, 22, 1, 0, 9, 19, 1]\n",
      "\t .is ----> a\n",
      "\tX=[[0, 0, 0], [0, 0, 5], [0, 5, 13], [5, 13, 13], [13, 13, 1], [0, 0, 0], [0, 0, 15], [0, 15, 12], [15, 12, 9], [12, 9, 22], [9, 22, 9], [22, 9, 1], [0, 0, 0], [0, 0, 1], [0, 1, 22], [1, 22, 1], [0, 0, 0], [0, 0, 9], [0, 9, 19], [9, 19, 1]]\n",
      "\tY=[5, 13, 13, 1, 0, 15, 12, 9, 22, 9, 1, 0, 1, 22, 1, 0, 9, 19, 1, 2]\n",
      "\t isa ----> b\n",
      "\tX=[[0, 0, 0], [0, 0, 5], [0, 5, 13], [5, 13, 13], [13, 13, 1], [0, 0, 0], [0, 0, 15], [0, 15, 12], [15, 12, 9], [12, 9, 22], [9, 22, 9], [22, 9, 1], [0, 0, 0], [0, 0, 1], [0, 1, 22], [1, 22, 1], [0, 0, 0], [0, 0, 9], [0, 9, 19], [9, 19, 1], [19, 1, 2]]\n",
      "\tY=[5, 13, 13, 1, 0, 15, 12, 9, 22, 9, 1, 0, 1, 22, 1, 0, 9, 19, 1, 2, 5]\n",
      "\t sab ----> e\n",
      "\tX=[[0, 0, 0], [0, 0, 5], [0, 5, 13], [5, 13, 13], [13, 13, 1], [0, 0, 0], [0, 0, 15], [0, 15, 12], [15, 12, 9], [12, 9, 22], [9, 22, 9], [22, 9, 1], [0, 0, 0], [0, 0, 1], [0, 1, 22], [1, 22, 1], [0, 0, 0], [0, 0, 9], [0, 9, 19], [9, 19, 1], [19, 1, 2], [1, 2, 5]]\n",
      "\tY=[5, 13, 13, 1, 0, 15, 12, 9, 22, 9, 1, 0, 1, 22, 1, 0, 9, 19, 1, 2, 5, 12]\n",
      "\t abe ----> l\n",
      "\tX=[[0, 0, 0], [0, 0, 5], [0, 5, 13], [5, 13, 13], [13, 13, 1], [0, 0, 0], [0, 0, 15], [0, 15, 12], [15, 12, 9], [12, 9, 22], [9, 22, 9], [22, 9, 1], [0, 0, 0], [0, 0, 1], [0, 1, 22], [1, 22, 1], [0, 0, 0], [0, 0, 9], [0, 9, 19], [9, 19, 1], [19, 1, 2], [1, 2, 5], [2, 5, 12]]\n",
      "\tY=[5, 13, 13, 1, 0, 15, 12, 9, 22, 9, 1, 0, 1, 22, 1, 0, 9, 19, 1, 2, 5, 12, 12]\n",
      "\t bel ----> l\n",
      "\tX=[[0, 0, 0], [0, 0, 5], [0, 5, 13], [5, 13, 13], [13, 13, 1], [0, 0, 0], [0, 0, 15], [0, 15, 12], [15, 12, 9], [12, 9, 22], [9, 22, 9], [22, 9, 1], [0, 0, 0], [0, 0, 1], [0, 1, 22], [1, 22, 1], [0, 0, 0], [0, 0, 9], [0, 9, 19], [9, 19, 1], [19, 1, 2], [1, 2, 5], [2, 5, 12], [5, 12, 12]]\n",
      "\tY=[5, 13, 13, 1, 0, 15, 12, 9, 22, 9, 1, 0, 1, 22, 1, 0, 9, 19, 1, 2, 5, 12, 12, 1]\n",
      "\t ell ----> a\n",
      "\tX=[[0, 0, 0], [0, 0, 5], [0, 5, 13], [5, 13, 13], [13, 13, 1], [0, 0, 0], [0, 0, 15], [0, 15, 12], [15, 12, 9], [12, 9, 22], [9, 22, 9], [22, 9, 1], [0, 0, 0], [0, 0, 1], [0, 1, 22], [1, 22, 1], [0, 0, 0], [0, 0, 9], [0, 9, 19], [9, 19, 1], [19, 1, 2], [1, 2, 5], [2, 5, 12], [5, 12, 12], [12, 12, 1]]\n",
      "\tY=[5, 13, 13, 1, 0, 15, 12, 9, 22, 9, 1, 0, 1, 22, 1, 0, 9, 19, 1, 2, 5, 12, 12, 1, 0]\n",
      "\t lla ----> .\n",
      "w='sophia'; context=[0, 0, 0]\n",
      "\tX=[[0, 0, 0], [0, 0, 5], [0, 5, 13], [5, 13, 13], [13, 13, 1], [0, 0, 0], [0, 0, 15], [0, 15, 12], [15, 12, 9], [12, 9, 22], [9, 22, 9], [22, 9, 1], [0, 0, 0], [0, 0, 1], [0, 1, 22], [1, 22, 1], [0, 0, 0], [0, 0, 9], [0, 9, 19], [9, 19, 1], [19, 1, 2], [1, 2, 5], [2, 5, 12], [5, 12, 12], [12, 12, 1], [0, 0, 0]]\n",
      "\tY=[5, 13, 13, 1, 0, 15, 12, 9, 22, 9, 1, 0, 1, 22, 1, 0, 9, 19, 1, 2, 5, 12, 12, 1, 0, 19]\n",
      "\t ... ----> s\n",
      "\tX=[[0, 0, 0], [0, 0, 5], [0, 5, 13], [5, 13, 13], [13, 13, 1], [0, 0, 0], [0, 0, 15], [0, 15, 12], [15, 12, 9], [12, 9, 22], [9, 22, 9], [22, 9, 1], [0, 0, 0], [0, 0, 1], [0, 1, 22], [1, 22, 1], [0, 0, 0], [0, 0, 9], [0, 9, 19], [9, 19, 1], [19, 1, 2], [1, 2, 5], [2, 5, 12], [5, 12, 12], [12, 12, 1], [0, 0, 0], [0, 0, 19]]\n",
      "\tY=[5, 13, 13, 1, 0, 15, 12, 9, 22, 9, 1, 0, 1, 22, 1, 0, 9, 19, 1, 2, 5, 12, 12, 1, 0, 19, 15]\n",
      "\t ..s ----> o\n",
      "\tX=[[0, 0, 0], [0, 0, 5], [0, 5, 13], [5, 13, 13], [13, 13, 1], [0, 0, 0], [0, 0, 15], [0, 15, 12], [15, 12, 9], [12, 9, 22], [9, 22, 9], [22, 9, 1], [0, 0, 0], [0, 0, 1], [0, 1, 22], [1, 22, 1], [0, 0, 0], [0, 0, 9], [0, 9, 19], [9, 19, 1], [19, 1, 2], [1, 2, 5], [2, 5, 12], [5, 12, 12], [12, 12, 1], [0, 0, 0], [0, 0, 19], [0, 19, 15]]\n",
      "\tY=[5, 13, 13, 1, 0, 15, 12, 9, 22, 9, 1, 0, 1, 22, 1, 0, 9, 19, 1, 2, 5, 12, 12, 1, 0, 19, 15, 16]\n",
      "\t .so ----> p\n",
      "\tX=[[0, 0, 0], [0, 0, 5], [0, 5, 13], [5, 13, 13], [13, 13, 1], [0, 0, 0], [0, 0, 15], [0, 15, 12], [15, 12, 9], [12, 9, 22], [9, 22, 9], [22, 9, 1], [0, 0, 0], [0, 0, 1], [0, 1, 22], [1, 22, 1], [0, 0, 0], [0, 0, 9], [0, 9, 19], [9, 19, 1], [19, 1, 2], [1, 2, 5], [2, 5, 12], [5, 12, 12], [12, 12, 1], [0, 0, 0], [0, 0, 19], [0, 19, 15], [19, 15, 16]]\n",
      "\tY=[5, 13, 13, 1, 0, 15, 12, 9, 22, 9, 1, 0, 1, 22, 1, 0, 9, 19, 1, 2, 5, 12, 12, 1, 0, 19, 15, 16, 8]\n",
      "\t sop ----> h\n",
      "\tX=[[0, 0, 0], [0, 0, 5], [0, 5, 13], [5, 13, 13], [13, 13, 1], [0, 0, 0], [0, 0, 15], [0, 15, 12], [15, 12, 9], [12, 9, 22], [9, 22, 9], [22, 9, 1], [0, 0, 0], [0, 0, 1], [0, 1, 22], [1, 22, 1], [0, 0, 0], [0, 0, 9], [0, 9, 19], [9, 19, 1], [19, 1, 2], [1, 2, 5], [2, 5, 12], [5, 12, 12], [12, 12, 1], [0, 0, 0], [0, 0, 19], [0, 19, 15], [19, 15, 16], [15, 16, 8]]\n",
      "\tY=[5, 13, 13, 1, 0, 15, 12, 9, 22, 9, 1, 0, 1, 22, 1, 0, 9, 19, 1, 2, 5, 12, 12, 1, 0, 19, 15, 16, 8, 9]\n",
      "\t oph ----> i\n",
      "\tX=[[0, 0, 0], [0, 0, 5], [0, 5, 13], [5, 13, 13], [13, 13, 1], [0, 0, 0], [0, 0, 15], [0, 15, 12], [15, 12, 9], [12, 9, 22], [9, 22, 9], [22, 9, 1], [0, 0, 0], [0, 0, 1], [0, 1, 22], [1, 22, 1], [0, 0, 0], [0, 0, 9], [0, 9, 19], [9, 19, 1], [19, 1, 2], [1, 2, 5], [2, 5, 12], [5, 12, 12], [12, 12, 1], [0, 0, 0], [0, 0, 19], [0, 19, 15], [19, 15, 16], [15, 16, 8], [16, 8, 9]]\n",
      "\tY=[5, 13, 13, 1, 0, 15, 12, 9, 22, 9, 1, 0, 1, 22, 1, 0, 9, 19, 1, 2, 5, 12, 12, 1, 0, 19, 15, 16, 8, 9, 1]\n",
      "\t phi ----> a\n",
      "\tX=[[0, 0, 0], [0, 0, 5], [0, 5, 13], [5, 13, 13], [13, 13, 1], [0, 0, 0], [0, 0, 15], [0, 15, 12], [15, 12, 9], [12, 9, 22], [9, 22, 9], [22, 9, 1], [0, 0, 0], [0, 0, 1], [0, 1, 22], [1, 22, 1], [0, 0, 0], [0, 0, 9], [0, 9, 19], [9, 19, 1], [19, 1, 2], [1, 2, 5], [2, 5, 12], [5, 12, 12], [12, 12, 1], [0, 0, 0], [0, 0, 19], [0, 19, 15], [19, 15, 16], [15, 16, 8], [16, 8, 9], [8, 9, 1]]\n",
      "\tY=[5, 13, 13, 1, 0, 15, 12, 9, 22, 9, 1, 0, 1, 22, 1, 0, 9, 19, 1, 2, 5, 12, 12, 1, 0, 19, 15, 16, 8, 9, 1, 0]\n",
      "\t hia ----> .\n"
     ]
    }
   ],
   "source": [
    "# build the dataset\n",
    "\n",
    "block_size = 3\n",
    "X, Y =[], []\n",
    "for w in words[:5]:\n",
    "    # print(w)\n",
    "    context = [0] * block_size\n",
    "    print(f\"{w=}; {context=}\")\n",
    "    for ch in w + '.':\n",
    "        ix = stoi[ch]\n",
    "        X.append(context)\n",
    "        Y.append(ix)\n",
    "        print(f\"\\t{X=}\\n\\t{Y=}\")\n",
    "        print(\"\\t\", ''.join(itos[i] for i in context), '---->', itos[ix])\n",
    "        context = context[1:]+[ix]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
