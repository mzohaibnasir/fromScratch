{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Singular Value Decomposition\n",
    "covers matrix decompsition and how can a matrix be ranked deficient. and how we can produce smaller matrid that captures most of the information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **What is a Rank-Deficient Matrix?**  \n",
    "A rank-deficient matrix is a matrix where the rank (the number of linearly independent rows or columns) is less than its full possible rank. For an \\( m \\times n \\) matrix, the rank is at most \\( \\min(m, n) \\). If the rank is less than this, some rows or columns are linearly dependent, meaning they can be expressed as linear combinations of others.\n",
    "\n",
    "---\n",
    "\n",
    "### **Why is \\( W \\) Rank Deficient?**  \n",
    "In the context of pre-trained models:\n",
    "- **High Dimensionality but Low Complexity**: The weight matrix \\( W \\) often has very large dimensions (e.g., 1000 × 1000), but the information it encodes is structured and concentrated in a much smaller subspace.  \n",
    "- **Intrinsic Low Rank**: Empirical studies show that the task-specific updates or features learned by \\( W \\) during training are inherently low-dimensional, meaning only a small number of linearly independent components are sufficient to capture most of the useful information.  \n",
    "\n",
    "Thus, \\( W \\) is rank deficient because many of its rows or columns are redundant or dependent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Singular Value Decomposition (SVD) Explanation**\n",
    "\n",
    "SVD is a factorization of a matrix \\( W \\) into three components:  \n",
    "\\[ W = U \\cdot S \\cdot V^T \\]\n",
    "\n",
    "Where:\n",
    "- \\( W \\) is the original matrix.\n",
    "- \\( U \\) is a matrix containing the left singular vectors of \\( W \\).\n",
    "- \\( S \\) is a diagonal matrix containing the singular values of \\( W \\).\n",
    "- \\( V^T \\) (or \\( V \\) transposed) is a matrix containing the right singular vectors of \\( W \\).\n",
    "\n",
    "The dimensions of these matrices are as follows:\n",
    "- \\( U \\) has dimensions \\( m \\times m \\), where \\( m \\) is the number of rows in \\( W \\).\n",
    "- \\( S \\) is a vector of length \\( \\min(m, n) \\), where \\( n \\) is the number of columns in \\( W \\).\n",
    "- \\( V \\) has dimensions \\( n \\times n \\).\n",
    "\n",
    "However, the rank of \\( W \\) (let’s say \\( r \\)) is usually much smaller than its dimensions, so most of the singular values in \\( S \\) will be very small or close to zero. Thus, only the first \\( r \\) singular values and corresponding columns of \\( U \\) and \\( V \\) are important for a low-rank approximation.\n",
    "\n",
    "---\n",
    "\n",
    "### **Code Explanation**\n",
    "\n",
    "1. **Performing SVD on \\( W \\):**\n",
    "\n",
    "```python\n",
    "U, S, V = torch.svd(W)\n",
    "```\n",
    "- This decomposes \\( W \\) into the three matrices \\( U \\), \\( S \\), and \\( V \\).\n",
    "\n",
    "2. **Shape of Matrices:**\n",
    "\n",
    "```python\n",
    "W.shape, U.shape, S.shape, V.shape\n",
    "```\n",
    "- Displays the dimensions of \\( W \\), \\( U \\), \\( S \\), and \\( V \\).\n",
    "\n",
    "3. **Rank-r Approximation:**\n",
    "   - To get a rank-\\( r \\) approximation of \\( W \\), we keep only the first \\( r \\) singular values in \\( S \\), and the corresponding columns of \\( U \\) and rows of \\( V \\).\n",
    "   \n",
    "```python\n",
    "U_r = U[:, :W_rank]  # Picking the first r columns of U\n",
    "S_r = torch.diag(S[:W_rank])  # Diagonal matrix with the first r singular values\n",
    "V_r = V[:, :W_rank].t()  # Picking the first r columns of V and transposing it\n",
    "```\n",
    "\n",
    "   - **\\( U_r \\)**: A matrix of size \\( m \\times r \\), consisting of the first \\( r \\) columns of \\( U \\).\n",
    "   - **\\( S_r \\)**: A diagonal matrix of size \\( r \\times r \\), consisting of the first \\( r \\) singular values.\n",
    "   - **\\( V_r \\)**: A matrix of size \\( n \\times r \\), consisting of the first \\( r \\) columns of \\( V \\) (transposed).\n",
    "\n",
    "4. **Computing the low-rank approximation:**\n",
    "\n",
    "```python\n",
    "B = U_r @ S_r  # Matrix multiplication to compute B\n",
    "A = V_r  # Assigning V_r to A\n",
    "```\n",
    "\n",
    "   - **\\( B \\)**: This is the low-rank approximation of the original matrix \\( W \\), computed as \\( U_r \\cdot S_r \\).\n",
    "   - **\\( A \\)**: This is just \\( V_r \\), representing the right singular vectors.\n",
    "\n",
    "5. **Final Shapes:**\n",
    "\n",
    "```python\n",
    "print(f\"{B.shape=}\\t {A.shape=}\")\n",
    "U.shape, U_r.shape, S.shape, S_r.shape\n",
    "```\n",
    "- Displays the shapes of \\( B \\) and \\( A \\), and the shapes of the matrices \\( U \\), \\( U_r \\), \\( S \\), and \\( S_r \\).\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Insights:**\n",
    "- **Rank-r Approximation**: The decomposition and truncation to rank \\( r \\) gives a low-rank approximation of \\( W \\), reducing the number of parameters while retaining most of the significant information.\n",
    "- **Efficiency**: By keeping only the first \\( r \\) singular values and corresponding vectors, the complexity is reduced significantly, leading to computational savings and reduced memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Singular Value Decomposition (SVD) Explanation**\n",
    "\n",
    "SVD is a factorization of a matrix \\( W \\) into three components:  \n",
    "\\[ W = U \\cdot S \\cdot V^T \\]\n",
    "\n",
    "Where:\n",
    "- \\( W \\) is the original matrix.\n",
    "- \\( U \\) is a matrix containing the left singular vectors of \\( W \\).\n",
    "- \\( S \\) is a diagonal matrix containing the singular values of \\( W \\).\n",
    "- \\( V^T \\) (or \\( V \\) transposed) is a matrix containing the right singular vectors of \\( W \\).\n",
    "\n",
    "The dimensions of these matrices are as follows:\n",
    "- \\( U \\) has dimensions \\( m \\times m \\), where \\( m \\) is the number of rows in \\( W \\).\n",
    "- \\( S \\) is a vector of length \\( \\min(m, n) \\), where \\( n \\) is the number of columns in \\( W \\).\n",
    "- \\( V \\) has dimensions \\( n \\times n \\).\n",
    "\n",
    "However, the rank of \\( W \\) (let’s say \\( r \\)) is usually much smaller than its dimensions, so most of the singular values in \\( S \\) will be very small or close to zero. Thus, only the first \\( r \\) singular values and corresponding columns of \\( U \\) and \\( V \\) are important for a low-rank approximation.\n",
    "\n",
    "---\n",
    "\n",
    "### **Code Explanation**\n",
    "\n",
    "1. **Performing SVD on \\( W \\):**\n",
    "\n",
    "```python\n",
    "U, S, V = torch.svd(W)\n",
    "```\n",
    "- This decomposes \\( W \\) into the three matrices \\( U \\), \\( S \\), and \\( V \\).\n",
    "\n",
    "2. **Shape of Matrices:**\n",
    "\n",
    "```python\n",
    "W.shape, U.shape, S.shape, V.shape\n",
    "```\n",
    "- Displays the dimensions of \\( W \\), \\( U \\), \\( S \\), and \\( V \\).\n",
    "\n",
    "3. **Rank-r Approximation:**\n",
    "   - To get a rank-\\( r \\) approximation of \\( W \\), we keep only the first \\( r \\) singular values in \\( S \\), and the corresponding columns of \\( U \\) and rows of \\( V \\).\n",
    "   \n",
    "```python\n",
    "U_r = U[:, :W_rank]  # Picking the first r columns of U\n",
    "S_r = torch.diag(S[:W_rank])  # Diagonal matrix with the first r singular values\n",
    "V_r = V[:, :W_rank].t()  # Picking the first r columns of V and transposing it\n",
    "```\n",
    "\n",
    "   - **\\( U_r \\)**: A matrix of size \\( m \\times r \\), consisting of the first \\( r \\) columns of \\( U \\).\n",
    "   - **\\( S_r \\)**: A diagonal matrix of size \\( r \\times r \\), consisting of the first \\( r \\) singular values.\n",
    "   - **\\( V_r \\)**: A matrix of size \\( n \\times r \\), consisting of the first \\( r \\) columns of \\( V \\) (transposed).\n",
    "\n",
    "4. **Computing the low-rank approximation:**\n",
    "\n",
    "```python\n",
    "B = U_r @ S_r  # Matrix multiplication to compute B\n",
    "A = V_r  # Assigning V_r to A\n",
    "```\n",
    "\n",
    "   - **\\( B \\)**: This is the low-rank approximation of the original matrix \\( W \\), computed as \\( U_r \\cdot S_r \\).\n",
    "   - **\\( A \\)**: This is just \\( V_r \\), representing the right singular vectors.\n",
    "\n",
    "5. **Final Shapes:**\n",
    "\n",
    "```python\n",
    "print(f\"{B.shape=}\\t {A.shape=}\")\n",
    "U.shape, U_r.shape, S.shape, S_r.shape\n",
    "```\n",
    "- Displays the shapes of \\( B \\) and \\( A \\), and the shapes of the matrices \\( U \\), \\( U_r \\), \\( S \\), and \\( S_r \\).\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Insights:**\n",
    "- **Rank-r Approximation**: The decomposition and truncation to rank \\( r \\) gives a low-rank approximation of \\( W \\), reducing the number of parameters while retaining most of the significant information.\n",
    "- **Efficiency**: By keeping only the first \\( r \\) singular values and corresponding vectors, the complexity is reduced significantly, leading to computational savings and reduced memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an excellent and intuitive explanation of SVD! You've effectively used analogies, visuals, and code to clarify the concept, especially in the context of low-rank approximations like LoRA. Here’s a brief refinement and follow-up to further reinforce understanding:\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Highlights from Your Explanation**\n",
    "1. **Rotation, Scaling, Rotation**:  \n",
    "   The decomposition of \\( W \\) into \\( U \\), \\( S \\), and \\( V^T \\) is like breaking a complex transformation into three manageable steps:\n",
    "   - Aligning to the principal directions (\\( V^T \\)).\n",
    "   - Stretching/shrinking along those directions (\\( S \\)).\n",
    "   - Final alignment (\\( U \\)).\n",
    "\n",
    "2. **Everyday Analogies**:  \n",
    "   Your use of stretched rectangles and describing a face helps make the math more relatable.\n",
    "\n",
    "3. **Code Connection**:  \n",
    "   You bridged theory and practice with the PyTorch example, showing how to:\n",
    "   - Perform SVD.\n",
    "   - Truncate to a low rank \\( r \\).\n",
    "   - Reconstruct \\( W \\) with reduced dimensions (\\( B \\) and \\( A \\)).\n",
    "\n",
    "---\n",
    "\n",
    "### **Why SVD is Perfect for LoRA**\n",
    "LoRA leverages SVD's ability to approximate matrices:\n",
    "- **Reducing Parameters**: By focusing on the top \\( r \\) singular values, LoRA stores and updates only a fraction of the original parameters, saving memory and computation.\n",
    "- **Capturing Core Patterns**: Even with a reduced rank, the low-rank matrices \\( B \\) and \\( A \\) retain the most impactful transformations in \\( W \\), making it efficient without losing much fidelity.\n",
    "\n",
    "---\n",
    "\n",
    "### **Additional Intuitive Analogies**\n",
    "1. **Data Compression**:  \n",
    "   Think of \\( SVD \\) as compressing an audio file:\n",
    "   - \\( V^T \\): Analyzes the main frequencies.\n",
    "   - \\( S \\): Decides how loud each frequency is.\n",
    "   - \\( U \\): Combines the frequencies to recreate the sound.\n",
    "\n",
    "2. **Projecting Shadows**:  \n",
    "   Imagine a complex 3D object casting shadows:\n",
    "   - \\( V^T \\): Aligns the object to specific axes.\n",
    "   - \\( S \\): Measures the length of shadows along these axes.\n",
    "   - \\( U \\): Reorients the shadows back to recreate the object.\n",
    "\n",
    "---\n",
    "\n",
    "### **Possible Enhancements**\n",
    "- **Interactive Visuals**: Using animations or interactive tools (e.g., Python's `matplotlib`) to visualize how \\( U \\), \\( S \\), and \\( V^T \\) transform \\( W \\) could make it even more engaging.\n",
    "- **Low-Rank Reconstruction Error**: Adding an example to compute the difference between the original \\( W \\) and its low-rank approximation \\( B \\cdot A \\) would demonstrate how well the approximation works.\n",
    "\n",
    "Would you like me to create visualizations, further simplify the code, or dive deeper into the mathematics behind SVD?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let me explain SVD (Singular Value Decomposition) intuitively, using everyday analogies to build understanding.\n",
    "\n",
    "Think of SVD as taking a complex transformation (like a complicated photo filter) and breaking it down into three simpler steps that, when combined, give you the same result. \n",
    "\n",
    "\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    subgraph \"Step 1: Rotation V^T\"\n",
    "        A[Original Space] --> B[Rotate to align\\nwith main directions]\n",
    "    end\n",
    "    subgraph \"Step 2: Scaling S\"\n",
    "        B --> C[Stretch/shrink\\nalong directions]\n",
    "    end\n",
    "    subgraph \"Step 3: Rotation U\"\n",
    "        C --> D[Final rotation\\nto target shape]\n",
    "    end\n",
    "    \n",
    "    style A fill:#f9f,stroke:#333\n",
    "    style B fill:#bbf,stroke:#333\n",
    "    style C fill:#bfb,stroke:#333\n",
    "    style D fill:#fbb,stroke:#333\n",
    "\n",
    "```\n",
    "\n",
    "Let's break down how SVD works step by step:\n",
    "\n",
    "Step 1: Finding Principal Directions (V matrix)\n",
    "Imagine you're looking at a stretched and rotated rectangle. The first step is finding the main directions of this shape - like finding the longest and shortest paths through it. In linear algebra terms, we're finding the eigenvectors, which tell us the fundamental directions of transformation. Think of V as a recipe for rotating your input to align with these main directions.\n",
    "\n",
    "Step 2: Determining the Stretching Factors (S matrix)\n",
    "Once we've found these main directions, we measure how much stretching happens along each direction. These are our singular values - they tell us how much the transformation magnifies or shrinks things along each principal direction. If you were looking at our rectangle example, these values would tell you how long and how wide it is along its main axes. The S matrix contains these stretching factors along its diagonal.\n",
    "\n",
    "Step 3: Finding the Final Orientation (U matrix)\n",
    "Finally, we figure out how to rotate the stretched shape into its final position. The U matrix describes this final rotation - it's like adjusting the angle of your stretched shape to match exactly what you want.\n",
    "\n",
    "Let's see this in code, with explanations of what's happening at each step:\n",
    "\n",
    "```python\n",
    "import torch\n",
    "\n",
    "# Let's say we have a weight matrix W\n",
    "W = torch.randn(768, 768)  # Example dimensions from a transformer layer\n",
    "\n",
    "# Step 1: Perform SVD\n",
    "U, S, V = torch.svd(W)\n",
    "\n",
    "# At this point:\n",
    "# U is like a recipe for the final rotation (768 x 768)\n",
    "# S contains the stretching factors (768 values)\n",
    "# V contains the initial rotation directions (768 x 768)\n",
    "\n",
    "# For LoRA, we only want to keep the top r most important directions\n",
    "r = 2  # Example rank\n",
    "\n",
    "# Step 2: Take only the most important parts\n",
    "U_r = U[:, :r]    # Keep only first r columns (768 x 2)\n",
    "S_r = S[:r]       # Keep only first r stretching factors (2)\n",
    "V_r = V[:, :r]    # Keep only first r rotation directions (768 x 2)\n",
    "\n",
    "# Step 3: Create our final low-rank matrices\n",
    "# Combine U_r and S_r to form B\n",
    "B = U_r * S_r.unsqueeze(0)  # Shape: (768 x 2)\n",
    "# Use V_r transpose as A\n",
    "A = V_r.t()                 # Shape: (2 x 768)\n",
    "\n",
    "# Now B @ A gives us a low-rank approximation of the original matrix W\n",
    "```\n",
    "\n",
    "To understand why this is powerful, imagine you're trying to describe a face:\n",
    "1. First, you identify the main features (like the direction and position of the nose, eyes, mouth) - this is like finding V\n",
    "2. Then you note how pronounced each feature is (how big the nose is, how wide the eyes are) - these are your singular values in S\n",
    "3. Finally, you describe how these features are arranged to make the final face - this is your U matrix\n",
    "\n",
    "The beauty of SVD is that you can choose to keep only the most important features (highest singular values) and still get a good approximation of the original. In LoRA's case, this means we can capture the essential aspects of how we want to modify our model using far fewer numbers than the original matrix contained.\n",
    "\n",
    "Think about compressing a photo:\n",
    "- The original matrix W is like a high-resolution photo\n",
    "- The SVD process finds the most important patterns in this photo\n",
    "- By keeping only the top r patterns (low rank), we get a compressed version that still captures the essential features\n",
    "\n",
    "This is why SVD is so useful for LoRA - it lets us capture the important aspects of how we want to modify our model while dramatically reducing the number of parameters we need to store and update.\n",
    "\n",
    "Would you like me to elaborate on any of these concepts or provide more concrete examples of how SVD works in practice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "_ = torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.0797,  0.5545,  0.8058, -0.7140, -0.1518,  1.0773,  2.3690,  0.8486,\n",
      "         -1.1825, -3.2632],\n",
      "        [-0.3303,  0.2283,  0.4145, -0.1924, -0.0215,  0.3276,  0.7926,  0.2233,\n",
      "         -0.3422, -0.9614],\n",
      "        [-0.5256,  0.9864,  2.4447, -0.0290,  0.2305,  0.5000,  1.9831, -0.0311,\n",
      "         -0.3369, -1.1376],\n",
      "        [ 0.7900, -1.1336, -2.6746,  0.1988, -0.1982, -0.7634, -2.5763, -0.1696,\n",
      "          0.6227,  1.9294],\n",
      "        [ 0.1258,  0.1458,  0.5090,  0.1768,  0.1071, -0.1327, -0.0323, -0.2294,\n",
      "          0.2079,  0.5128],\n",
      "        [ 0.7697,  0.0050,  0.5725,  0.6870,  0.2783, -0.7818, -1.2253, -0.8533,\n",
      "          0.9765,  2.5786],\n",
      "        [ 1.4157, -0.7814, -1.2121,  0.9120,  0.1760, -1.4108, -3.1692, -1.0791,\n",
      "          1.5325,  4.2447],\n",
      "        [-0.0119,  0.6050,  1.7245,  0.2584,  0.2528, -0.0086,  0.7198, -0.3620,\n",
      "          0.1865,  0.3410],\n",
      "        [ 1.0485, -0.6394, -1.0715,  0.6485,  0.1046, -1.0427, -2.4174, -0.7615,\n",
      "          1.1147,  3.1054],\n",
      "        [ 0.9088,  0.1936,  1.2136,  0.8946,  0.4084, -0.9295, -1.2294, -1.1239,\n",
      "          1.2155,  3.1628]])\n"
     ]
    }
   ],
   "source": [
    "#  generate a rank deficient matrix W\n",
    "d, k = 10, 10\n",
    "\n",
    "# this way we can gernate a rank deficicent matrix\n",
    "\n",
    "W_rank = 2\n",
    "W = torch.randn(d, W_rank) @ torch.randn(W_rank, k)\n",
    "print(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W_rank=2\n"
     ]
    }
   ],
   "source": [
    "# evaluatin rank\n",
    "W_rank = np.linalg.matrix_rank(W)\n",
    "print(f\"{W_rank=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## perform SVD on the W matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B.shape=torch.Size([10, 2])\t A.shape=torch.Size([2, 10])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([10, 10]),\n",
       " torch.Size([10, 2]),\n",
       " torch.Size([10]),\n",
       " torch.Size([2, 2]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## perform SVD of the W matrix.. which produces three matrices U, S and V where W = UxSxV^T but dims of U,S and V are much smaller\n",
    "\n",
    "U, S, V = torch.svd(W)\n",
    "W.shape, U.shape, S.shape, V.shape\n",
    "\n",
    "# for rank-r factorization, keep only the first r singular values and corresponding columns of U and V)\n",
    "U_r = U[:, :W_rank]  # picking aonly k=2 columns\n",
    "S_r = torch.diag(S[:W_rank])\n",
    "V_r = V[:, :W_rank].t()\n",
    "\n",
    "# compute C = U_r * S_r and R = V_r\n",
    "B = U_r @ S_r\n",
    "A = V_r\n",
    "\n",
    "\n",
    "print(f\"{B.shape=}\\t {A.shape=}\")\n",
    "U.shape, U_r.shape, S.shape, S_r.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Given the same input, check the output using the original W matrice and the matrices resulting from decomposition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original y using W:\n",
      " y=tensor([ 7.2684e+00,  2.3162e+00,  7.7151e+00, -1.0446e+01, -8.1639e-03,\n",
      "        -3.7270e+00, -1.1146e+01,  2.0207e+00, -9.6258e+00, -4.1163e+00])\n",
      "\n",
      "\n",
      "y` computed using BA:\n",
      " y_prime=tensor([ 7.2684e+00,  2.3162e+00,  7.7151e+00, -1.0446e+01, -8.1638e-03,\n",
      "        -3.7270e+00, -1.1146e+01,  2.0207e+00, -9.6258e+00, -4.1163e+00])\n"
     ]
    }
   ],
   "source": [
    "# generate random bias and input\n",
    "bias = torch.randn(d)\n",
    "x = torch.randn(d)\n",
    "\n",
    "\n",
    "# compute Y + Wx + b\n",
    "y = W @ x + bias\n",
    "\n",
    "\n",
    "# compute y` =  CRx +b`\n",
    "y_prime = (B @ A) @ x + bias\n",
    "\n",
    "\n",
    "print(f\"original y using W:\\n {y=}\\n\\n\")\n",
    "print(f\"y` computed using BA:\\n {y_prime=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total parameters of W: 100\n",
      "total parameters of B and A: 40\n"
     ]
    }
   ],
   "source": [
    "print(f\"total parameters of W: {W.nelement()}\")\n",
    "print(f\"total parameters of B and A: {B.nelement()+ A.nelement()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the context of **LoRA** and its explanation, **rank** refers to the **matrix rank**, not the order or sequence.\n",
    "\n",
    "---\n",
    "\n",
    "### **What is Matrix Rank?**\n",
    "\n",
    "The rank of a matrix is the maximum number of linearly independent rows or columns in the matrix. It is a measure of the **complexity** or **dimensionality** of the space that the matrix spans.\n",
    "\n",
    "For example:\n",
    "1. A \\( 3 \\times 3 \\) matrix:\n",
    "   - If all three rows (or columns) are linearly independent, the rank is 3.\n",
    "   - If only two rows (or columns) are linearly independent, the rank is 2.\n",
    "   - If only one row (or column) is linearly independent, the rank is 1.\n",
    "\n",
    "Mathematically:\n",
    "- For a matrix \\( A \\in \\mathbb{R}^{m \\times n} \\), the rank \\( r \\) is:\n",
    "  \\[\n",
    "  r = \\text{dim}(\\text{span of rows}) = \\text{dim}(\\text{span of columns})\n",
    "  \\]\n",
    "\n",
    "---\n",
    "\n",
    "### **Rank in LoRA**\n",
    "\n",
    "In LoRA, the rank refers to the rank \\( r \\) of the **low-rank decomposition** of the task-specific update matrix \\( \\Delta W \\).\n",
    "\n",
    "\\[\n",
    "\\Delta W = A B^\\top\n",
    "\\]\n",
    "\n",
    "Here:\n",
    "- \\( A \\in \\mathbb{R}^{d_{\\text{out}} \\times r} \\)\n",
    "- \\( B \\in \\mathbb{R}^{d_{\\text{in}} \\times r} \\)\n",
    "\n",
    "The rank \\( r \\) is much smaller than \\( \\min(d_{\\text{out}}, d_{\\text{in}}) \\), meaning \\( \\Delta W \\) can be represented using far fewer independent components. This low rank indicates that the adjustment needed to fine-tune the model is inherently simple and does not require a full update of the large weight matrix \\( W \\).\n",
    "\n",
    "---\n",
    "\n",
    "### **Why is Low Rank Important?**\n",
    "\n",
    "1. **Efficiency**: \n",
    "   - A low-rank matrix requires fewer parameters to represent, reducing memory and computational costs.\n",
    "   - For example, if \\( d_{\\text{out}} = 10,000 \\), \\( d_{\\text{in}} = 10,000 \\), and \\( r = 4 \\):\n",
    "     - Full matrix: \\( 10,000 \\times 10,000 = 100 \\, \\text{million parameters} \\)\n",
    "     - Low-rank update: \\( 10,000 \\times 4 + 10,000 \\times 4 = 80,000 \\, \\text{parameters} \\)\n",
    "\n",
    "2. **Interpretability**:\n",
    "   - A low rank suggests that only a small number of key dimensions (or directions) need adjustment to adapt the model to a specific task.\n",
    "   - This highlights the structured nature of how LLMs encode knowledge.\n",
    "\n",
    "---\n",
    "\n",
    "In summary, **rank in LoRA refers to the matrix rank** of the task-specific update matrix \\( \\Delta W \\), representing the dimensionality of the adjustment space. It’s a critical factor in achieving efficient and targeted fine-tuning of large language models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch3d",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
